              T HESIS R ESEARCH P LAN




Deep learning vision approach for
semi-supervised defect detection in
     manufacturing industry



                             Author:
               Eneko I NTXAUSTI A RBAIZA

                         Supervisors:
Dr. Ekhi Z UGASTI U RIGUEN         Dr. Carlos C ERNUDA G ARCIA




            PhD Programme in Applied Engineering
            Electronics and Computing Departament
                    Faculty of Engineering
                  Mondragon Unibertsitatea

                       Mondragon
                      December 2022
                            Declaration

Hereby I declare that this document is my original authorial work, which I have
worked out on my own. All sources, references, and literature used or excerpted
during elaboration of this work are properly cited and listed in complete reference to
the due source.




                                                            Eneko Intxausti Arbaiza
                                                        Mondragon, December 2022




                                                                                     i
                                 Abstract

The globalisation of markets leads companies to opt for a product specialisation
strategy, with the aim of offering superior quality and highly reliable goods. In
addition, complex manufacture of products implies that many factors interact with
each other in the production line. This, combined with the increase in quality standards,
has resulted in quality control becoming one of the key points in the manufacturing
process.
    The popularity and high performance of deep learning approaches within different
research fields has been reflected to some degree in Optical Quality Control (OQC) for
castings, where data-driven methods have outperformed traditional computer vision
techniques. However, challenges such as difficulty in labelling images make the
solution sub-optimal, so new approaches need to be considered. In addition, methods
collected in the literature are subject to problem-specific context, complicating their
use and implementation in other environments.
    The aim of this work is to investigate current defect detection methods in order to
build a comprehensive taxonomy covering a wide range of methods for the application
in industrial environments with different characteristics and needs.
    Experiments show that deep learning-based defect detection methods are suitable
for industrial quality control processes. In particular, results show the specificity
of methods as they achieve promising results under the specific conditions of the
problem.
    In future lines, it is contemplated to broaden the spectrum of requirements and
limitations of the experiments in order to propose a methodology applicable in the
industrial environment, providing an accurate, adaptable and explainable solution to
different real use cases.

keywords:
Defect detection, Castings, Deep learning, Computer vision, Optical quality control



                                                                                      iii
                             Laburpena

Merkatuen globalizazioak produktuak espezializatzeko estrategia bat aukeratzera
eramaten ditu enpresak, kalitate eta fidagarritasun handiko produktuak eskaintzeko
helburuarekin. Gainera, produktuen fabrikazio konplexuak faktore askoren elkarrekin-
tza dakar produkzio-katean. Horrek, kalitate-estandarrak handitzearekin batera,
kalitate-kontrola fabrikazio-prozesuaren funtsezko puntuetako bat bihurtzea eragin du.
    Hainbat ikerketa-eremutan bezala, ikaskuntza sakonaren ospea eta errendimendu
handia galdaketa-piezetarako Kalitate Optikoaren Kontroleko (OQC) zenbait alderditan
islatu dira, non datuetan oinarritutako metodoek ordenagailu bidezko ikusmen-teknika
tradizionalak gainditu dituzten. Hala ere, irudiak etiketatzeko zailtasunek lortutako
soluzioa guztiz optimoa ez izatea eragiten dute, eta, beraz, ikuspegi berrien aldeko
apustua egin behar da. Gainera, bibliografian jasotako metodoak arazoaren testuinguru
espezifiko bati lotuta daude, eta horrek zaildu egiten du beste ingurune batzuetan
erabiltzea eta ezartzea.
    Lan honen helburua akatsak detektatzeko egungo metodoak ikertzea da, ezaugarri
eta behar desberdinak dituzten ingurune industrialetan aplikatzeko metodo sorta zabala
barne hartzen duen taxonomia bat eraikitzeko.
    Esperimentuek erakusten dute ikaskuntza sakonean oinarritutako akatsak detekta-
tzeko metodoak egokiak direla industria-kalitatea kontroleko prozesuetarako. Bereziki,
emaitzek metodoen espezifikotasuna erakusten dute, emaitza oparoak lortzen baitituzte
arazoaren baldintza espezifikoetan.
    Etorkizuneko ildoetan, esperimentuen baldintzen eta mugen espektroa zabaltzea
aurreikusten da, industria-inguruneari aplika dakiokeen metodologia proposatzeko,
benetako erabilera-kasu desberdinei konponbide zehatza, moldagarria eta esplikagarria
emanez.

keywords:
Defect detection, Castings, Deep learning, Computer vision, Optical quality control


                                                                                    v
                              Resumen

La globalización de los mercados lleva a las empresas a optar por una estrategia de
especialización de productos, con el objetivo de ofrecer productos de calidad superior
y altamente fiables. Además, la compleja fabricación de los productos implica la
interacción entre sí de muchos factores en la cadena de producción. Esto, unido al
aumento de los estándares de calidad, ha provocado que el control de calidad se haya
convertido en uno de los puntos clave del proceso de fabricación.
    La popularidad y el alto rendimiento de los enfoques de aprendizaje profundo
dentro de diferentes campos de investigación se han visto reflejados en cierto aspectos
en el Control de Calidad Óptica (OQC) para piezas de fundición, donde los métodos
basados en datos han superado a las técnicas tradicionales de visión por ordenador. Sin
embargo, inconvenientes como la dificultad para etiquetar las imágenes hacen que la
solución no sea óptima, por lo que es necesario apostar por nuevos enfoques. Además,
los métodos recogidos en la bibliografía están sujetos a un contexto específico del
problema, lo que dificulta su uso e implementación en otros entornos.
    El objetivo de este trabajo es investigar los métodos actuales de detección de
defectos para construir una taxonomía que abarque una amplia gama de métodos para
la aplicación en entornos industriales con diferentes características y necesidades.
    Los experimentos muestran que los métodos de detección de defectos basados en
aprendizaje profundo son adecuados para los procesos de control de calidad industrial.
En particular, los resultados muestran la especificidad de los métodos, puesto que
consiguen resultados prometedores en condiciones específicas del problema.
    En líneas futuras, se contempla ampliar el espectro de requisitos y limitaciones
de los experimentos para proponer una metodología aplicable al entorno industrial,
proporcionando una solución precisa, adaptable y explicable a diferentes casos de uso
reales.

keywords:
Defect detection, Castings, Deep learning, Computer vision, Optical quality control

                                                                                       vii
                               Contents

1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .        1
   1.1   Smart Manufacturing . . . . . . . . . . . . . . . . . . . . . . . . .        1
   1.2   Introduction to Optical Quality Control . . . . . . . . . . . . . . .        2
   1.3   Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .     2
   1.4   Organisation of the work . . . . . . . . . . . . . . . . . . . . . . .       3
2 Theoretical background . . . . . . . . . . . . . . . . . . . . . . . . . .          4
   2.1   Computer vision     . . . . . . . . . . . . . . . . . . . . . . . . . . .    4
         2.1.1   Image representation . . . . . . . . . . . . . . . . . . . . .       5
         2.1.2   Image preprocessing . . . . . . . . . . . . . . . . . . . . .        6
         2.1.3   Feature extraction    . . . . . . . . . . . . . . . . . . . . . .    9
   2.2   Deep learning in computer vision      . . . . . . . . . . . . . . . . . .   10
         2.2.1   Feature extraction backbones      . . . . . . . . . . . . . . . .   13
         2.2.2   Applications . . . . . . . . . . . . . . . . . . . . . . . . .      17
         2.2.3   Evaluation metrics . . . . . . . . . . . . . . . . . . . . . .      22
   2.3   Defect detection in industry quality control . . . . . . . . . . . . .      23
         2.3.1   Optical defect detection . . . . . . . . . . . . . . . . . . .      25
3 State of the Art     . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   27
   3.1   Methodology of Literature Review . . . . . . . . . . . . . . . . . .        27
   3.2   Optical Quality Control in castings . . . . . . . . . . . . . . . . . .     29
         3.2.1   Traditional approaches . . . . . . . . . . . . . . . . . . . .      29
         3.2.2   Machine learning-based approaches . . . . . . . . . . . . .         32
         3.2.3   Deep learning-based approaches . . . . . . . . . . . . . . .        34
   3.3   Semi-supervised learning in Defect Detection . . . . . . . . . . . .        40
         3.3.1   Benchmark datasets . . . . . . . . . . . . . . . . . . . . .        40
         3.3.2   One-class methods . . . . . . . . . . . . . . . . . . . . . .       41
   3.4   Explainable Artificial Intelligence . . . . . . . . . . . . . . . . . .     47
         3.4.1   Explainability in defect detection . . . . . . . . . . . . . .      49

                                                                                     ix
    3.5   Critical Review . . . . . . . . . . . . . . . . . . . . . . . . . . . .     51
4 Objectives and hypotheses         . . . . . . . . . . . . . . . . . . . . . . . .   54
    4.1   Hypotheses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      54
    4.2   Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    55
5 Research methodology and work plan . . . . . . . . . . . . . . . . . .              56
    5.1   Research and data methodologies . . . . . . . . . . . . . . . . . . .       56
    5.2   Work plan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .     58
    5.3   Publication plan . . . . . . . . . . . . . . . . . . . . . . . . . . . .    61
    5.4   Research internship . . . . . . . . . . . . . . . . . . . . . . . . . .     62
6 Preliminary Results . . . . . . . . . . . . . . . . . . . . . . . . . . . .         63
    6.1   Use case I: supervised learning approaches for Defect Detection in
          Castings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    63
          6.1.1   Introduction . . . . . . . . . . . . . . . . . . . . . . . . . .    63
          6.1.2   Description of the dataset . . . . . . . . . . . . . . . . . . .    65
          6.1.3   Experimental procedure . . . . . . . . . . . . . . . . . . .        66
          6.1.4   Results . . . . . . . . . . . . . . . . . . . . . . . . . . . .     70
          6.1.5   Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . .     74
    6.2   Use case II: defect detection in surface images via semi-supervised
          methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .     75
          6.2.1   Introduction . . . . . . . . . . . . . . . . . . . . . . . . . .    75
          6.2.2   Description of the dataset . . . . . . . . . . . . . . . . . . .    76
          6.2.3   Experimental procedure . . . . . . . . . . . . . . . . . . .        77
          6.2.4   Results . . . . . . . . . . . . . . . . . . . . . . . . . . . .     80
          6.2.5   Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . .     82
7 Conclusions       . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   84
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      87




x
                         List of Figures

2.1   Example of convolution operation in 2D. Image and filter mask are
      represented by left and middle matrix, respectively. Each mask value
      is multiplied by its respective image pixel and all of them are summed up.
      Output is positioned based on localization of used image part. Process is
      performed using sliding-window approach        . . . . . . . . . . . . . . .    8
2.2   Examples of filter masks [1, 2] . . . . . . . . . . . . . . . . . . . . . .     8
2.3   Topological illustration of an fully connected ANN . . . . . . . . . . .       11
2.4   Several activation functions [3]. (a) ReLU function, (b) sigmoid function
      and (c) tanh function . . . . . . . . . . . . . . . . . . . . . . . . . . .    12
2.5   Architecture of Convolutional Neural Network. This scheme is formed
      by two convolutional layers, two pooling layers, one flatten layer and two
      fully connected layers. In the case of using the CNN as feature extractor,
      fully connected layers are removed from network . . . . . . . . . . . .        14
2.6   Scheme of a confusion matrix for binary classification. T P , F P , F N and
      T N are the abbreviations of True Positive, False Positive, False Negative
      and True Negative, respectively . . . . . . . . . . . . . . . . . . . . . .    23
2.7   Diagram of most commonly employed quality control methods            . . . .   25

3.1   The method used to perform the literature review represented by a funnel
      scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .     28
3.2   Number of publications related to casting defect detection (1977-2022) [4] 28
3.3   Publishing companies relating to casting defect detection . . . . . . . .      29
3.4   Histogram of publications according to the type of article and techniques
      for casting defect detection problems. Statistics and spectral belong to
      traditional approaches discussed below . . . . . . . . . . . . . . . . . .     29
3.5   Summary of DL techniques applied to Defect Detection in castings . . .         36

5.1   CRISP-DM reference model [5] . . . . . . . . . . . . . . . . . . . . .         58

                                                                                     xi
5.2   Work plan is represented by a Gantt diagram and covers all three years of
      the thesis. It is divided in six work packages and each package contains
      several tasks. Estimated duration for each task is shown in months and is
      highlighted with the colour of its respective work package. Submissions
      are identified by a diamond . . . . . . . . . . . . . . . . . . . . . . . .     59

6.1   Workflow diagram of casting product OQC process in Fagor Ederlan . .            66
6.2   Patch-level pre-processing strategy for defective images. The process
      is similar for false defective images, except that patches obtained from
      ground-truth are classified as OK patches. . . . . . . . . . . . . . . . .      67
6.3   Proposed architecture of our baseline CNN . . . . . . . . . . . . . . .         69
6.4   Comparison of models’ performance using different dataset sizes: 10% of
      the dataset, 50% and whole dataset      . . . . . . . . . . . . . . . . . . .   71
6.5   Defect height (left) and width (right) distribution in the casting dataset      72
6.6   Representation of some images from the dataset. A non-defective product
      (left), an obvious defective product (middle) and a subtle defective
      product (right) are shown     . . . . . . . . . . . . . . . . . . . . . . . .   77
6.7   Overview of our semi-supervised approach for defect detection. This
      pipeline consists in three stages: obtaining training images by data
      augmentation, extracting features from the images by feature extractor
      backbones, and finally, reducing the dimensionality of the feature vector
      by PCA and image classification by anomaly detection methods . . . .            78
6.8   Influence of number of neighbours in LOF method . . . . . . . . . . .           81
6.9   Confusion matrix of the test set for the model with LOF and data augmentation.
      All images are correctly classified. . . . . . . . . . . . . . . . . . . . .    82




xii
                           List of Tables

3.1   Summary of the principal semi-supervised learning-based approaches
      used in defect detection . . . . . . . . . . . . . . . . . . . . . . . . . .     43
3.2   One-class methods in MVTec AD benchmark dataset. D: Density-based,
      C: Classification-based, R: Reconstruction-based. Model performance is
      evaluated using image-level AUC for detection and pixel-level AUC for
      localization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .     46

5.1   Publication plan for the thesis   . . . . . . . . . . . . . . . . . . . . . .    61

6.1   Number of 64 × 64 patches obtained in preprocessing for casting dataset
      gathered in 2021 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .       69
6.2   Comparison of the best performing models, trained on the complete image
      dataset   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    71
6.3   Detection accuracy of proposed models trained with different patch-sizes         72
6.4   Comparison of ViT and baseline CNN architectures trained with raw and
      filtered images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    74
6.5   Distribution of non-defective and defective product images in training,
      validation and test sets . . . . . . . . . . . . . . . . . . . . . . . . . .     76
6.6   Comparison of different configurations of the proposed method. In case
      of applying data augmentation, the training set is increased by a factor of
      four . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .     82




                                                                                      xiii
                              Acronyms

AE Autoencoder

AI Artificial Intelligence

ANN Artifical Neural Network

AUC Area Under the ROC Curve

CAE Convolutional Autoencoder

CNN Convolutional Neural Network

CT Computer Tomography

DL Deep Learning

DR Digital Radiography

FCN Fully Convolutional Network

FPN Feature Pyramid Network

FT Fourier Transform

GAN Generative Adversarial Network

GPU Graphics Processing Unit

I4.0 Industry 4.0

IoU Intersection over Union

KD Knowledge Distillation



                                         xv
k-NN k Nearest Neighbors

LBP Local Binary Pattern

LOF Local Outlier Factor

LoG Laplacian-of-Gaussian

ML Machine Learning

MLP Multilayer Perceptron

mAP Mean Average-Precision

NLP Natural Language Processing

NF Normalizing Flow

OCC One-class Classification

OQC Optical Quality Control

PCA Principal Component Analysis

PdM Predictive Maintenance

ReLU Rectified Linear Unit

RNN Recurrent Neural Network

ROC Receiver Operating Characteristic

SSD Single Shot Multibox Detector

SoTA State-of-the-Art

SVD Singular Value Decomposition

SVDD Support Vector Domain Descriptor

SVM Support Vector Machine

ViT Vision Transformers

XAI eXplainable Artificial Intelligence

YOLO You Only Look Once




xvi
                                                                         Chapter 1


                           Introduction

1.1     Smart Manufacturing

In the last few years, manufacturing industry has undergone a profound business
transformation to address the demands of consumers and governments, implementing
innovative technologies and environments that bridge cyber and physical worlds.
Advances such as internet of things, cloud computing and big data are driving the shift
towards an smart manufacturing [6]. Therefore, smart manufacturing has been turned
into the principal element of Industry 4.0 (I4.0) and takes the advantage of digital
technologies to collect new data and process it, providing useful information to the
manufacturing systems [7].
    The evolution of the I4.0 has carried many innovations in the fields of automation,
control and information technology related to manufacturing processes. The conversion
to an intelligent industry, the establishment of sensors, machines and cloud technologies
allow for increased data collection [8], resulting in improved quality control processes.
Applying new measures to quality control becomes necessary, as the trend towards
smaller production lines with smaller batch sizes and greater variety made previous
control techniques obsolete [9].
    In this context, the amount of Artificial Intelligence (AI) based applications has
increased significantly. Although AI has been around for 70 years, the capabilities
of computers and the cost of computing at that time limited the research capabilities.
However, over the years, advances such as Graphics Processing Unit (GPU) or parallel
computing have made possible to solve highly complex problems.
    Among all the AI techniques that have gained popularity, deep learning (DL) has
been the most successful in different fields of application. DL-based methods are
capable of modelling a complex system with the aim of optimising processes and
tasks, extracting system features with limited domain knowledge [10]. Thus, allowing
for increased performance on data analysis problems.

                                                                                       1
1. I NTRODUCTION


1.2      Introduction to Optical Quality Control
The rise in competitiveness in the current market is forcing companies to develop high
quality products. In addition, with shorter production times and more personalised
products, ensuring product quality is an extremely difficult task. For this reason,
companies are developing new technologies for intelligent quality control, which will
improve the quality of the products and the manufacturing process itself.
     The trend in recent years has been to automate the quality control process, using
processes such as Optical Quality Control (OQC). OQC focuses on detecting whether
a product contains defects by visually analysing each product. Turning OQC into
an automatic process that uses data to obtain knowledge allows to reduce human
intervention in such a critical process. On the one hand, being a repetitive process, the
human factor becomes unreliable. Fatigue may influence the correct performance of
the task, not detecting defective products, which means big losses for the company.
On the other hand, the large amount of products to be analysed in a short period of
time makes the task usually infeasible for an operator.


1.3      Motivation
In order to reduce costs and to achieve a high level of competitiveness and product
quality, many companies invest money in defect detection research. This research is
aimed at implementing reliable methods to minimise (or even eliminate) the number
of defective products reaching the customer.
     For this type of process, research must be focused on three main pillars:

■   Defect detection: applying State-of-the-Art (SoTA) techniques to detect defects in
    manufacturing products.

■   Model diagnosis and defect localisation: through different techniques including
    semi-supervised learning and explainable Artificial Intelligence (XAI).

■   Transferability of knowledge: the results must be transferable to new processes.
    Sometimes, a new production line lacks sufficient data to fit a model, so knowledge
    from other processes should be transferred to this new line.

     Research carried out to date is far from a real industrial final control process.
Although the results obtained are based on industrial product datasets, the conclusions
of these works are not presented as direct solutions for the industrial sector, but rather
as the result of an industrial dataset [11]. Transferring the results to real industrial
processes is of vital importance in these cases.

2
                                                      1.4. Organisation of the work


1.4     Organisation of the work
This work is divided in seven chapters. The current is Chapter 1, which introduces
OQC as core topic of the thesis and presents the motivation and organisation of the
work.
   Chapter 2 details background knowledge to understand both computer vision and
deep learning methods presented in the work, and it discusses different approaches
used in industrial quality control processes. Chapter 3 reviews literature concerning
defect detection and presents SoTA methods, followed by a discussion of their
shortcomings.
   Chapter 4 proposes the hypothesis of the work, as well as additional hypotheses
and objectives in order to demonstrate the main hypothesis. Chapter 5 includes
selected research methodology and presents work and publication plan.
   Chapter 6 gathers conducted work during the first year of the thesis and describes
experimental procedure and preliminary results for different use cases.
   Finally, Chapter 7 presents conclusions of the work and future research lines.




                                                                                    3
                                                                        Chapter 2


            Theoretical background

This chapter includes an overview of the theoretical background required to understand
the techniques used throughout the thesis. In Section 2.1, computer vision field is
introduced. Moreover, the different steps of the computer vision problems are outlined
and most relevant techniques are explained. Section 2.2 presents a summary of deep
learning approaches and dives into its application for computer vision problems.
Finally, Section 2.3 introduces the topic of industry quality control processes.


2.1 Computer vision
Computer vision is an interdisciplinary field of AI that focuses on leading machines
toward processing and understanding digital images and videos, mimicking human
vision system as closely as possible.
    The pipeline of a typical computer vision problem consists of several steps: image
acquisition and formation, image preprocessing and feature extraction. According to
the target of the problem, final step will follow a classification, detection or image
generative approach.
    As a first step of computer vision problems, image formation involves a projection
of the 3-dimensional world onto a 2-dimensional surface [12]. Procedure is carried
out by sensing devices, which mimic the function of human eyes. Sensing device
represents a key element for computer vision systems and it is usually a digital camera,
a X-ray imaging machine or a radar. Its purpose is to capture the objective located
in 3D real world and transform it into a 2D digital image. This process is known as
Imaging.
    The aim of this work is not computer graphics, so we will not dive into image
formation. An extensive explanation of this topic can be found in the works by Corke
et al. [12] and Szeliski [13]. The following sections explain concisely next steps in a
typical computer vision problem.

4
                                                                      2.1. Computer vision


2.1.1 Image representation
This task focuses on clearly representing images formed in above-mentioned steps.
image representation allows adapting the image obtained from a sensing device in a
way that is compatible with the computer. Moreover, a correct representation allows
subsequent image analysis to be carried out efficiently. Next definitions are necessary
to understand digital image concept.

Definition 2.1.1 (Pixel). Pixel is the smallest element in a digital image. It is
indivisible and the coherent union of numerous pixels forms an image.

Definition 2.1.2 (Digital image). A digital image is a function that maps from an
discrete spatial domain Ω formed by pixels to some colour space F :

                           I:        Ω                −→        F
                                                                                      (2.1)
                                     (x, y)         7−→        uxy
where

      Ω = {(x, y) : 1 ≤ x ≤ Ncols ∧ 1 ≤ y ≤ Nrows } ⊂ Z2 , Ncols , Nrows ≥ 1

     Additionally, gathering from Definition 2.1.2, digital images may be seen as
matrices that contain pixels information in each of its elements. Depending on the
codomain F from Function 2.1, image matrices may be represented as follows:

■   Binary image: each pixel value has only two possibilities, 0 or 1 (uxy = {0, 1}),
    white and black, respectively.

■   Continuous greyscale image: each pixel value is a real number between 0 and
    1 (uxy = [0, 1]). For convenience, in this case, 0 belongs to black colour and 1
    belongs to white colour. Rest of real numbers are grey-levels interpolated between
    black and white. X-ray images are of this type.

■   N-dimensional vector-valued image: image has N channels per pixel, so each
    pixel is represented by u = (u1 , ..., uN ) vector. Value vector may contain continuous
    values,
                                uxy = (u1xy , ..., uN            N
                                                    xy ) ∈ [0, 1] ,                   (2.2)

    or scalars,
                                            N
                        uxy = (u1xy , ..., uxy ) ∈ {0, 1, ..., 2a − 1}N               (2.3)

    where a ≥ 1 and a ∈ N. For a = 8 and N = 3, it is defined as RGB colour model,
    and each channel takes a scalar value from 0 to 255. In this case, black pixel is
    represented by u = (0, 0, 0) and white by u = (255, 255, 255) .

                                                                                         5
2. T HEORETICAL BACKGROUND


Clearly, image formed by two or more channels need the same number of matrices to
be represented. Specifically, most used digital image representations are formed by
three matrixes. Otherwise, binary or greyscale images are represented by only one
matrix.

                                                           (m,n)
Definition 2.1.3 (Image window). An image window Wp                (I) is an image of size
(m, n) formed by cropping a part of image I centred on p reference pixel.

    Even if in most of the cases entire images are analysed, it is sometimes important
to highlight only specific sections of the image. Image window is used to focus on
meaningful patches of the image in order to extract local information more accurately
by removing irrelevant parts.


2.1.2     Image preprocessing

The aim of image preprocessing is to change the original image in order to make easier
the subsequent feature extraction process. This step focuses on making corrections
in raw images, such as reducing image noise, increasing sharpness or balancing the
illumination.
    Preprocessing techniques are slightly dependent on the feature extractors [2].
It is important that applied image correction suitably combines with subsequently
extraction process in order to intelligently leverage preprocessing step [1, 2, 14].
Otherwise, utilising preprocessing enhancements that are not related to descriptors
could return worse results that using raw images [2].
    According to the area of influence, there are three different ways to process target
pixels from the image [15]. Point operators deal with an input pixel at a time and only
take into account its value for operations. Local operators utilise an arbitrary set of
pixels obtained from target pixel’s neighbourhood in order to modify its value. In this
way, local dependencies are presented in new pixels’ values. Finally, global operators
employ the entire image to correct the target pixel. In practice, local operators are the
most commonly used [16, 17].


2.1.2.1 Convolution

Filtering is the most common strategy for signal processing [18]. It aims to remove or
accentuate some frequencies, magnitudes or phases in certain parts of signal spectrum
and is widely used in several applications such as telecommunications [19], speech
processing [20] or biomedical systems [21]. As an image is anything but a two
dimensional signal, filtering might be used as a preprocessing strategy.

6
                                                                   2.1. Computer vision


    Convolution is one of the main operations in image preprocessing and it is easily
computed as a discrete spatial operator [2]. Convolution operation is summed up in a
element-wise multiplication between two matrixes and a subsequent addition.

Definition 2.1.4 (Convolution operation). Convolution of pixel p = (x, y) in image I
is defined as
                                         k
                                         X k
                                           X
                 J(p) = I ∗ W (p) =                 wij · I(x + i, y + j)             (2.4)
                                        i=−k j=−k

where W represents (2k + 1) × (2k + 1) size kernel and wij ∈ R.

    Since target pixel’s neighbourhood is required in the estimation of new pixel’s
value, convolution belongs to the group of local operators. Furthermore, by using
element-wise multiplication and addition operations, convolution is considered a linear
operation.
    Figure 2.1 shows a simple convolution between a binary image and an arbitrary
filter mask. Top-left green number is obtained following a pixel-wise multiplication
and addition between mask and equivalent part of the image. Mathematical expression
can be found in Equation 2.4. Same process is carried out for all (overlapping)
windows in the image and output is represented by a new matrix.
    Convolution is critical for filtering operations that aim to select specific frequencies
throughout the signal, removing undesired ones (low-pass, band-pass and high-pass
filters). Also, fitting the kernel mask to the image allows to blurring, sharpening, or
even detecting edges from images.
    Figure 2.2 shows some of the most well-known kernels regarding to image
preprocessing. Kernels presented in Figure 2.2a and 2.2b belongs to Sobel operator
[1] and their main performance is related to edge detection tasks. As an edge detector,
Sobel operator tries to locate changes in intensity, so it aims to estimate image
derivatives using convolutions [2]. Derivatives in vertical and horizontal directions of
the image are estimated convolving original image with those two kernels, respectively.
Combining results achieved from convolutions, intensity gradients for each pixel are
obtained.
    Several edge detectors are based on second derivative of the image, in order
to avoid noise obtained by first derivatives. Following similar process as above,
convolutions are applied to image using Laplacian kernel [2], so that estimate the
second derivative from each pixel (Figure 2.2c). Nevertheless, no edge-sections of an
image may contain high intensity contrast, principally, due to small objects. Therefore,
a Gaussian smoothing filter is employed to remove image noise before employing
Laplacian kernel. This method is known as Laplacian of Gaussian (LoG) [22].

                                                                                          7
 2. T HEORETICAL BACKGROUND


                      1   1   1    0    0
                      0   1   1    1    0           1 0 1            4 3 4
                      0   0   1    1    1    ∗      0 1 0      =     2 4 3
                      1   0   0    1    1           1 0 1            2 3 4
                      1   1   0    0    1

 1·1+1·0+1·1+0·0+1·1+1·0+0·1+0·0+1·1 = 1+1+1+1 = 4

     Figure 2.1: Example of convolution operation in 2D. Image and filter mask
     are represented by left and middle matrix, respectively. Each mask value
     is multiplied by its respective image pixel and all of them are summed up.
     Output is positioned based on localization of used image part. Process is
     performed using sliding-window approach

             -1 0 1                               -1 -2 -1                 0 1 0
             -2 0 2                               0 0 0                    1 -4 1
             -1 0 1                               1 2 1                    0 1 0
      (a) Sobel vertical kernel        (b) Sobel horizontal kernel   (c) Laplacian kernel
                          Figure 2.2: Examples of filter masks [1, 2]

 2.1.2.2 Fourier Transform

 If digital images are understood as two-dimensional signals, frequency domain
 methods could be employed in preprocessing. Fourier Transform (FT) represents one
 of the most utilised ways to processing signals. The main notion of FT is based on
 transforming whatever waveforms into a sum of different sinusoid frequencies. In the
 case of digital images, FT maps an image from its spatial domain into the frequency
 domain [1]. Also, FT is invertible, so it is possible to reverse from frequency domain
 to spatial domain.
     As introduced in the previous section, digital images belong to a discrete spatial
 domain, hence specific representation for FT must be selected. Two-dimensional
 Discrete Fourier Transform is defined by following equation:


                1              X−1 Nrows
                              Ncols  X−1                             
                                                                         xu    yu
                                                                                    
I(u, v) =                                          I(x, y) · exp −i2π        +                  (2.5)
          Ncols · Nrows                                                 Ncols Nrows
                               x=0          y=0

     where (u, v) ∈ {0, 1, ..., Ncols − 1} × {0, 1, ..., Nrows − 1} for frequencies [1].
 Exponential part of the Equation 2.5 follows Euler’s formula for θ ∈ R


                                  exp(iθ) = eiθ = cos θ + i sin θ                           (2.6)

 8
                                                                 2.1. Computer vision

            √
where i =       −1 is the imaginary unit.
     Known the frequency content by means of FT, filtering may be carried out
differentiating low- and high-frequencies of the image. This differentiation allows
to detect edges using high-pass filtering or remove noise and smooth the image with
low-pass filtering. More comprehensive introductions to FT are provided by Krig [2]
and Klette [1].


2.1.3 Feature extraction

Feature extraction aims to extract useful information from images in order to train
a vision model. This hand-crafted step is used principally in traditional approaches,
since deep learning-based approach extracts automatically features that perform model
fitting. Employed techniques are problem-dependent, therefore it is necessary to find
the technique that best fits each problem. Also, expert knowledge is required to carry
out the extraction process. For these reasons, the use of such techniques has been
considerably reduced.
     Main feature extraction techniques are presented in the following paragraphs:

■   Fourier Transform (FT) may be used as feature extractor if frequencies obtained
    in filtering are measured. Shape of objects in a scene may be obtained calculating
    the euclidean distance from coefficients of FT [23].

■   Wavelet transform processes a signal decomposing it into a set of functions named
    wavelets. Each of them represents an oscillating waveform with a specific degree
    of compression or scale and time location. They are copies of basic wavelets
    functions known as mother wavelet. Unlike FT, wavelet transform is able to
    locate signal frequency in time space [24]. Feature vectors are extracted from the
    matrix of wavelet coefficient [25] and the estimation of mother wavelet’s coefficient
    distribution.

■   Singular Value Decomposition (SVD) decomposes image pixels matrix into three
    matrixes, containing eigenvectors and eigenvalues from original image [26]. SVD
    focuses on dimensionality reduction of image components, therefore it selects
    those eigenvalues and eigenvectors that explain more image cumulative variance.
    Obtained values may be selected as image features.

■   Local Binary Pattern (LBP) is a simple but effective technique to describe image
    local properties in greyscale domains. It has been widely used in face recognition
    [27] and texture classification [28]. LBP is formed by comparing each pixel in the

                                                                                       9
2. T HEORETICAL BACKGROUND


     image with its neighbouring pixels [29]. Histograms are computed from obtained
     local binary patterns to extract the features of the greyscale images.



2.2       Deep learning in computer vision
Deep learning (DL) is a subfield of machine learning (ML) and AI that aims to build
accurate data-driven models with large artificial neural networks. DL requires a
large number of examples to identify patterns from data, and extracted information is
utilised to fit a function or model that maps inputs values to one or more output values.
      Even if machine learning and deep learning terms are used in similar situations,
since the latter is a subset of the former, it is worth emphasising the principal points
on which specific DL methods differ from other generic ML ones. ML techniques are
based on statistical learning and optimization methods and aim to identify patterns
in the dataset that help us to learn behaviour in the data or infer new observations
[30]. The range of possibilities in ML problems is wide: the type of technique we
choose depends on the characteristics of problem we are dealing with, i.e, size of
dataset, pre-processing requirements, etc. Therefore, whereas some ML techniques
are satisfied with small datasets and require an expert knowledge to select features
from data, other techniques are based on neural networks that extract features from
raw data and demand large datasets. The latter belong to the field of DL, while the
former do not. Therefore, not all ML problems can be treated with DL techniques.
      Nowadays, DL is the main technology in many different fields: computer vision,
natural language processing, cybersecurity, medical research, customer services, etc.
Artificial Neural Networks (ANNs) are the basis of deep learning. ANNs are made up
of neurons that are connected to each other, so that if one neuron is active, information
is transmitted to another neuron. This process mimics how human brain works.
Neurons are the basic building blocks of neural networks and they are distributed in
three types of layers: input, hidden and output layers (see Figure 2.3). ANNs are built
by stacking layers on top of each other, and it is considered DL network when it has
more than two hidden layers (but most of DL networks has many more than two [31]).
      Surprisingly, neural networks are principally built by simple operations such as
additions and multiplications. Perceptron is the first neural network: a single layer of
neurons used as a supervised binary classifier. Perceptron is built multiplying each
input by their respective weight and adding them (known as weighted sum) in order to
compare the result through a threshold function. The way perceptron is built (additions
and multiplications only) entails that its decision boundary will be linear, so good
results will only be obtained in a linearly separable dataset.

10
                                                 2.2. Deep learning in computer vision


                 Input layer                                  Output layer
                                     Hidden layers


            I1

            I2

            I3

            I4


            Figure 2.3: Topological illustration of an fully connected ANN


     In order to overcome this drawback, non-linear activation functions are utilised
to add non-linearity to neural networks. Rectified Linear Unit (ReLU) [32], sigmoid
or hyperbolic tangent [33] are some of the most commonly used activation functions
(Figure 2.4). Those functions allow neural network to learn more powerful represen-
tations and improve their performance in more challenging problems. Back to
perceptron case, whether it is composed of two or more layers and has arbitrary
activation functions, it is called Multilayer Perceptron (MLP) [34].
     Basically, ANN’s training is a optimization problem that aims to minimise an
specific objective function. It focuses on achieving the weights of the networks that
most closely match the data distribution without overfit. At the beginning, weights
are initialized to random values and they are being updated during the training. The
network is fed with data samples and the first hidden layer process them to obtain first
output values. Those output values are the input values for the next layer, and so on.
This process is known as forward propagation [31]. Network’s output is compared
with the objective value using a loss function, and the aim is to find the global minimum
for that loss function. Weights of the network are updated by backpropagation [33]
and optimizers such as Gradient Descent [35] or Adam [36] algorithms.
     Training process and data are essential parts in model building. Depending on the
data target and training stage, DL approach is classified into four categories:

■   Supervised learning is the most common approach in ML and DL. It requires
    labelled data to train the model. Model can be viewed as a function that maps input
    data to target values. During the training, input data and its corresponding label are
    used to calibrate weights of the network. When model is trained satisfactorily, it is
    able to predict values for new input data.

■   Unsupervised learning does not require labelled data. Nowadays, unlabelled data

                                                                                       11
2. T HEORETICAL BACKGROUND


           y = max(x, 0)                                     1                                        ex −e−x
                                                y=         1+e−1                           y=
                     y
                                                                                                      ex +e−x
                                                           y                                          y
                 4
                                                      1                                          1

                 3                                   0.8
                                                                                                0.5
                                                     0.6
                 2
                                                                                                                        x
                                                     0.4                         −6   −4   −2               2   4   6
                 1
                                                     0.2                                   −0.5

                                  x                                          x
      −4    −2           2    4       −6   −4   −2               2   4   6                      −1


                 (a)                                       (b)                                        (c)

      Figure 2.4: Several activation functions [3]. (a) ReLU function, (b) sigmoid
      function and (c) tanh function


     is abundantly available in digital form, so collecting it allows to obtain large datasets
     at low cost. Unsupervised learning tries to gather the underlying structure of data in
     order to obtain features that cluster observations with common patterns (clustering)
     or data-representations that reduce feature space (dimensionality reduction).

■    Semi-supervised learning is utilised when small amounts of labelled data are
     available. This is usually due to labelling data is expensive or time consuming. In
     order to carry out the training process, a large amount of unlabelled examples is
     added to labelled data. Model is fitted using labelled data and tries to label rest of
     examples. Otherwise, unlabelled data could be used to train the model, keeping
     labelled one for the validation. Semi-supervised learning aims to combine those
     two types of data to obtain useful information from both of them.

■    Reinforcement learning approach is about an agent that learns an optimal policy by
     trial and error search in a dynamic environment. The training process is as follows
     [37]: the agent receives an input denoting the current state of the environment
     and chooses an action to generate an output. Selected action modifies the state
     of the environment and agent is informed about the alteration, named reward or
     reinforcement signal. The objective is to optimize final reward.

      Currently, unsupervised learning is achieving remarkable performance in Natural
Language Processing (NLP) due to the current possibility of model pre-training
through huge datasets without any explicit supervision [38]. Nevertheless, in other
domains such as computer vision, unsupervised learning has not benefits compared to
supervised learning. In that case, semi-supervised obtains significant results with a
few labelled observations [39].
      In the following sections, the field of DL has been narrowed down to a computer
vision approach. Within the scope of computer vision problems, the most relevant

12
                                                2.2. Deep learning in computer vision


DL-based architectures are presented. Furthermore, the main applications and the
most commonly used evaluation metrics are discussed.


2.2.1 Feature extraction backbones
As has been mentioned, DL-based models complete a wide range of computer vision
tasks achieving outstanding results. These tasks differ considerably from each other,
so task-specific models are developed to obtain optimum performance. Although those
models are constructed for different purposes, many of them share feature extraction
as a common element. DL-based architectures extract representative features from
images and process them in order to achieve problem-specific goals. Therefore, feature
extraction is one of the most important phase for all computer vision tasks.
     The structure responsible for performing feature extraction is known as backbone
or feature extractor. Backbones are usually at the beginning of DL models and
have a deep architecture to obtain hierarchical feature maps. They are trained as
image classifier with huge datasets and transfer knowledge to domain-specific tasks
removing fully connected layers from the networks. It is noteworthy that good results
in classification problems tend to transfer quality gains in others application domains
[40]. The two main reference backbones, convolutional and transformer-based neural
networks, are presented below.


2.2.1.1 Convolutional Neural Network

Convolutional Neural Network (CNN) is one of the most popular deep neural network
and it is developed by Yann LeCun to improve performance in image processing
tasks [33]. Convolutional layer is formed by several kernels that apply convolutions
to extract features from input data and record them in feature maps. Subsequently,
activation function adds non-linearity and pooling layer downsamples feature maps.
     Figure 2.5 shows a basic CNN. Convolutional layers are stacked on top of each
other, so features of different scales are obtained. In this way, shallower layers extract
simple local features whereas deeper layers combine low-level features to represent
high-level ones. Thus, information of the image is preserved through the network in
an hierarchical way. Furthermore, they are computationally efficient, as each kernel
apply same weights throughout convolution process.
     In the following, the main CNN-based backbones are listed and their characteristics
are briefly explained.

■   AlexNet [41]: This CNN is the earliest network to obtain outstanding results on
    image classification problem and has been the reference for networks that have

                                                                                       13
2. T HEORETICAL BACKGROUND



 Image
                                                                                 Output



                                                                                  Fully
                                                                                Connected
         Convolution         Pooling    Convolution        Pooling   Flatten      layer
              +               layer           +             layer
            ReLU                             ReLU


Figure 2.5: Architecture of Convolutional Neural Network. This scheme is formed by
two convolutional layers, two pooling layers, one flatten layer and two fully connected
layers. In the case of using the CNN as feature extractor, fully connected layers are
removed from network


     been developed since then. It is formed by 5 convolutional layers, some of which
     are followed by max-pooling layers. Furthermore, it introduces ReLU as activation
     function.

■    GoogLeNet [42]: first network to obtain 22-layer depth. It is based on Inception
     module, a block of convolution layers with 1 × 1, 3 × 3 and 5 × 5, which outputs
     concatenation of multiple filter banks as input for next stage. Furthermore, it allows
     to extract multi-scale features. The major drawback of this architecture is its crafted
     complexity. An upgrade in the Inception module (reducing kernel sizes) improves
     performance considerably, while slightly increasing the computational cost [40].

■    VGGNet [43]: this family of networks has been develop with the aim of increasing
     network depth, while reducing complexity and maintaining remarkable results.
     They follow structure presented by Krizhevsky et al. with AlexNet, so the impro-
     vement is determined by the increase in network depth. VGGNet avoids previous
     approaches of using large receptive fields in shallow layers by reducing convolu-
     tional filters to 3 × 3 kernels.

■    ResNet [44]: those networks apply a residual learning approach in order to optimize
     the training, avoiding accuracy degradation as network depth increases. Their
     structure is based on VGGNet, but shortcut connections are inserted between
     stacked layers. Shortcut connections add an identity mapping to the underlying
     mapping of the stacked layers, which provides higher accuracy for deep networks
     rather than their plain counterparts.

■    EfficientNet [45]: as the depth or width of the network has been shown to influence
     model performance, it is important to choose an reliable scaling process in order to

14
                                                2.2. Deep learning in computer vision


    obtain optimal results. Therefore, instead of arbitrarily and independently scaling
    up a single dimension, a better option is to scale up the depth, width and resolution
    of the network based on the ratio among them. This compound scaling approach is
    used for building EfficientNet architecture.

■   DetNet [46]: Even if above-mentioned feature extractors demonstrate satisfactory
    performance for object detection tasks, backbones based on image classification
    may find that their object locate ability is affected because of extreme downsampling
    of receptive fields. DetNet is built as an object detection specific backbone but
    maintains counterparts performance in image classification. It maintains high spatial
    resolution in deeper layers, as an additional phase is added and downsampling of
    receptive fields is stopped.

     One of the main advantages of these networks is the translation invariance, which
allows objects in different parts of the image to be detected. This characteristic is
useful in problems such as object detection and is an advantage over architectures that
lack it, such as Vision Transformers (ViT).


2.2.1.2 Transformer-based neural networks

Transformers are one of the latest achievement in DL [47]. Due to the success of this
type of networks in language processing tasks, research on transformers has rapidly
spread to computer vision [48], adapting NLP models and designing novels based on
vision problems constraints.
     Transformers are based on a self-attention approach. Self-attention is an attention
mechanism that relates several positions of a sequence in order to obtain sequence
representation. This approach replaces the recursive processing of Recurrent Neural
Networks (RNNs) by modelling the entire sequence, allowing to draw global depen-
dencies between sequence elements [47]. Input sequence may be a sentence or
paragraph in NLP, whereas, in vision problems, it may be formed by image patches
[49]. Several attention approaches are used in computer vision [50], but self-attention
transformer provides both best performance and high scalability to complex models
and large-scale datasets [48].
     Let X = (x1 , x2 , ..., xn ) ∈ Rn×d a n-dimensional sequence where d represents
entity dimension. Self-attention layer is formed by three learnable weight matrices
(W Q ∈ Rd×dq , W K ∈ Rd×dk and W V ∈ Rd×dv ), which are used to model interaction
between entities. Input sequence X is projected onto these three weight matrices:

               Q = XW Q ,          K = XW K         and      V = XW V ,

                                                                                      15
2. T HEORETICAL BACKGROUND


queries, keys and values, respectively. Subsequently, self-attention layer computes
scores between queries and keys. These scores show the attention of sequence entities
toward that entity at current position. Then, scores are normalized and transformed
into probabilities. Finally, they are multiplied by values matrix. Above process is
summarised in Equation 2.7 and Z ∈ Rn×dv represents interactions between each
entity in the sequence [47]

                                                            Q · KT
                                                                    
                   Z = Attention(Q, K, V ) = softmax         √           ·V          (2.7)
                                                                dk
     Self-attention layer is the building block for Transformer-based architectures.
There are several architectures that combine this method with CNNs [48], however,
performance is limited, as single self-attention layer is not able to analyse same entity
for two different sequence positions at the same time [51]. In the case of Transformers,
the use of self-attention reaches a next level and avoids previous drawback. In
order to model complex relationships between sequence entities, Transformers-based
architectures apply multiple self-attention layers in parallel formed by random initia-
lised query, key and value matrices. This is known as multi-head attention block and
allows modelling relationship between entities for different sequence orderings. A
more comprehensive explanation can be found in the review of Khan et al. [48] and
Han et al. [51].
     Transformers do not have convolutional layers in their structure. In order to obtain
best performance, Transformers are pre-trained on a huge dataset and afterwards they
are fine-tuned in a problem specific dataset. Furthermore, they have demonstrated
an outstanding scalability to high-complexity networks with billions of parameters
[49, 51].
     Dosovitskiy et al. [49] propose the first Transformer-based method capable of
achieving comparable results to those obtained by CNNs in computer vision problems:
ViT. They closely replicate the work of Vaswani et al. [47], using image patches as
tokens of a sequence. For model pre-training step, they use datasets of different sizes.
Best performance is achieved with largest dataset, formed by three hundred million
images (JFT-300M [52]). This work shows that Transformers require a large number
of images to outperform CNN-based architectures.
     ViT models are taken as a foundation for the construction of new Transformed-
based structures [53] for image classification problems. However, their multi-head
self-attention blocks are applied on a constant image scale, so they are unable to
capture fine spatial details at different scales [48]. Therefore, they fail in tasks where
fine features are decisive, such as object detection or image segmentation. In order to
avoid this issue, multi-scale Transformers show that extracting features at different

16
                                                2.2. Deep learning in computer vision


scales keeps both local and global image representations [54, 55]. These hierarchical
structures, which are able to learn the subtle image details, may be applied satisfactorily
as general-purpose feature extractor in computer vision problems [54].

2.2.2 Applications
Deep learning techniques have become the state of the art for many fields in computer
vision. Following paragraphs dive into the peculiarities of the main problems in
DL-based computer vision.

2.2.2.1 Image classification

Image classification is the most typical problem in computer vision. An image
classification model aims to assign a label to an image from a predefined number of
classes based on image features. Depending on the number of classes, the problem is
summed up in a binary classification (two classes) or multiclass classification (three
or more classes).
    Although the problem of image classification is already a benchmark for computer
vision researchers, it is not until the advent of deep learning and CNNs that results
reached the crowning moment. The starting point of this trend is the ImageNet Large
Scale Visual Recognition Challenge in 2012, in which a novel deep CNN won the first
place classifying 1.2 million images into one thousand classes [41].
    Since that relevant event, CNN research has increased significantly. Some of the
most important improvements from that moment are the increase of the network’s
depth [42, 43], residual learning [44] and a well-thought-out scaling up of the network
[45]. Based on those characteristics, SoTA deep CNN models are able to extract useful
representations from different image datasets and learn the patterns that appear in the
images of each class. Subsequently, new images are classified using this information.
    However, a huge dataset and a large number of resources are needed to train
such models. While the advent of GPU has influenced in the training process of the
networks, this process is still time and resource expensive. Therefore, transfer learning
is employed in order to avoid those disadvantages. DL-based models are pre-trained
through huge datasets, such as ImageNet [56] or JFT-300M [52], and later they are
fine-tuned using a target dataset. Thus, it is possible to save training time by using
the model weights for those datasets, shared by other researchers, reducing the cost
associated to fitting the weights to problem-specific data.
    State of the art for image classification is principally determined by ImageNet
benchmark dataset. The models are compared on their accuracy in making inference
on the image label. Although until the 2020s state of the art is based on CNNs, in

                                                                                        17
2. T HEORETICAL BACKGROUND


the last few years there has been a change in the trend. Currently, Transformer-based
models and models combining CNNs and Transformers are at the top of ImageNet
ranking [57–59]. While improvements have been made in development of novel fully
CNNs, i.e. based on the training of transformer-based networks such as ConvNeXt,
they have not been able to improve Transformers classification capability [60]. This is
due to Transformers high-scalability to large-scale dataset [48]: first eight models of
the raking are trained with a dataset of three billion images [53], ninth with a dataset
of 1.5 billion [61] and tenth, a CNN EfficientNet [62], is trained with 300 million
images. The complete ranking can be checked on the papers with code website [63].


2.2.2.2 Object detection

Object detection task focuses on estimating the concepts and locations of objects
contained in each image, in order to gain a complete semantic understanding of the
image [64]. Therefore, it is formed by two steps: the former indicates where objects
are located in a image (Object localization), and the latter determines the category
each object belongs to (Object classification).
     Object detection can be differentiated into several problems depending on their
main objective. Generic object detection locates and classifies all objects in the image,
and identifies them within the image using rectangles, known as bounding-boxes. In
this case, all objects have the same importance for the network. However, Salient
object detection [65] aims to identify visual distinctive and most dominant region of
the image precisely, so networks focuses only on those parts of image that stand out
the most. Other problems within object detection are face detection [66], pedestrian
detection [67] and video tracking [68].
     There are two approaches to detect an object within an image. On the one hand,
region-based approach generates many region proposals that can contain an object.
Subsequently, this approach examinates whether proposals include any object, and
classifies each object into different categories. Models that follows this approach
are also known as two-stage methods. R-CNN [69], Fast R-CNN [70] and Faster
R-CNN [71] are of this type. On the other hand, one-stage methods do not need to
form regions proposals, so classification and prediction of bounding-boxes are carried
out in a single step. Some of the most popular one-stage architectures are YOLO (You
only look once) [72], EfficientDet [73], Single Shot Multibox Detector (SSD) [74]
and Feature Pyramid Network (FPN) [75].
     One drawback of region-based method is that its multi-staged training process
is time consuming and requires large computational costs. In addition, at test time
the detection speed of those models is slow because features are extracted from each

18
                                               2.2. Deep learning in computer vision


object proposal in each test image [76]. Therefore, one-stage methods are faster and
are a better choice when time for forecasting is limited.
    It is worth mentioning that both one-stage and two-stage methods use previously
discussed deep CNN models to carried out feature extraction. Fully connected layers
are removed from the top of CNN model and remaining network is used as a backbone
leveraging feature maps obtained from pre-training.



2.2.2.3 Image segmentation

Image segmentation is the task that divides an image into several non-overlapping
and relevant sections according to an specific criteria. Partitioning of the image into
different segments is highly accurate as it is done by pixel-wise classification [77].
Image segmentation is widely applied in fields such as medical analysis, even to the
point of developing specific architectures as UNet [78].
    Depending on the criteria used to partition the image, different types of segmenta-
tion can be found [79]. Semantic segmentation assigns a class label from a set of
categories to each pixel in the image. Instance segmentation follows previous criteria
but in this case only significant objects are located, delineating them in the image.
Hence, adjacent objects of the same class are precisely differentiated, which is not
the case of semantic segmentation. Background or irrelevant objects are disregarded.
Finally, panoptic segmentation is a combination of the two previous ones: it classifies
each pixel according to its semantic meaning and according to the instance to which it
belongs.
    Fully Convolutional Networks (FCNs) represent a landmark in image segmentation
problem [80]. These networks are based on image classification architectures, but with
the difference that blocks of fully connected layers are replaced by convolutional layers.
Moreover, skip connections are added to combine semantic information from deep
layers and spatial information from shallow fine layers. Thus, accurate recognition
and pixel-wise localization is carried out. However, standard FCNs require a high
computational inference time and extraction of global information is not efficient.
    In addition, object detection methods may also be used for segmentation. Many
successful object detection architectures are slightly adapted in order to perform pixel-
level prediction. In FPN, as its structure is based only on fully convolutional layers,
segmentation is performed adding two MLPs to predict masks on top of each feature
pyramid level. Furthermore, Mask R-CNN [81] extends Faster R-CNN by adding
FCN-based binary mask for each region of interest. This pixel-wise branch is obtained
parallelly with bounding boxes and classification predictions, so its computational

                                                                                      19
2. T HEORETICAL BACKGROUND


cost is only marginally higher than its counterpart for object detection and their
combination carries out instance segmentation.


2.2.2.4 Image generation

Image generation is the process of artificially generate new images from real images
data. The subtle difference compared to image synthesis is that the images generated
by the latter contain some particular desired content. In both cases, similar generative
approaches are used in order to produce realistic images.
     Generative Adversarial Networks (GANs) [82] aim to estimate the probability
density function of the data based on unsupervised learning. They are formed by two
networks, generator and discriminator, that interact with each other. Generator aims
to make realistic images trying to falsify input ones. Discriminator trains to distinguish
real images from those created by the generator. However, generator does not know
real images in the creation process so both of networks are trained in competition with
each other.
     Applications of this type of networks are gathered in Wang et al. [83], e.g.
generation of high-resolution images from low-resolution images, reparation of
damaged or masked sections, video generation, etc. Currently, image generation
from text description is being researched [84], and noteworthy results have been
reported. DALL-E [85] is one of the most relevant text-to-image generator model. It
is able to generate images faithfully based on a transformer autoregressive architecture
and using natural language as input. Furthermore, DALL-E trains twelve billion
parameters over two hundred and fifty million image-text pairs.
     Lastly, one of the most prominent applications of this type is Data Augmentation
[86]. Those techniques modify slightly images from the data applying translations,
rotations, cropping, noise addition, and so on, to obtain new images. The increase in
the number of images allows for a better generalization (Regularization), thus reducing
the risk of overfitting.


2.2.2.5 Anomaly detection

Chandola et al. [87] define anomaly detection as “the problem of finding patterns in
data that do not conform to expected behaviour”. In other words, anomaly detection
models learn from the data in order to classify samples into normal or anomalous
classes. Those models define a region formed by normal behaviour and consider
anomalies all samples that fall outside this region. Therefore, anomaly detection is
reduced to a binary classification.

20
                                                2.2. Deep learning in computer vision


    By definition, anomalous conditions are less common than normality. It is more
difficult to collect anomalies than samples of normal behaviour, so dataset will be
unbalanced [88]. It is possible to use supervised learning approach to solve anomaly
detection problems, but it is necessary to use techniques to overcome the imbalance
problem [89]. At the end, if the dataset can be balanced, discriminative model learns a
boundary based on features from both classes to divide normal and anomalous samples.
However, anomalous behaviour is continuously changing in real-world settings, so
new types of anomalies not seen in the training phase may arise. Discriminative model
does not learn its features and will therefore fail predicting the class.

    Semi-supervised approach is presented as an alternative to overcome the issues
discussed above [89]. This approach uses only samples of the normal class for model
training, so no labelling of anomalous samples is needed. As will be explained in
Section 3.3.2, there are different methods for building an anomaly detector.

    Originally, reconstruction-based methods, specifically architectures known as
Autoencoders (AE), are the most commonly used [88]. AE is an unsupervised learning-
based neural network that learns to reconstruct data as closely as possible. It is formed
by an encoder-decoder structure. Encoder reduces layers’ dimension progressively
and compresses data representation in Bottleneck layer, which is the layer with fewest
neurons. Decoder proposes the inverse process, obtaining an output of the same
dimensions as the input. Weights of the network are updated in order to minimise
dissimilarity between input and output, also known as reconstruction error. AE is
also used in dimensionality reduction task [90]. There are several types of AEs, but
Convolutional Autoencoder (CAE) [91] is the most used in vision problems.

    Nowadays, novel methods are based on estimating the distribution of normal
samples [92–94] or on self-supervised classification [95, 96]. Basically, these models
learn the behaviour of normal class samples using different techniques and define a
threshold (anomaly score) to classify as an anomaly any sample that surpasses that
value. As there are no anomalies involved in the training, the emergence of new types
of anomalies will not affect the accuracy of the models.

    Sometimes, anomaly detection and defect detection are interchanged in the
literature. Nevertheless, defect detection task tries to detect shifts in the composition
of an image (a broken chair image in a dataset of correct chairs) whereas anomaly
detection is also able to detect semantic or contextual shifts (a cat image in a dog
dataset). Throughout the document, we use both anomaly and defect detection terms
interchangeably, but we attempt to make the latter prevail over the former.

                                                                                      21
2. T HEORETICAL BACKGROUND


2.2.3     Evaluation metrics
An evaluation phase is necessary for each statistical or ML method to assess the quality
of the process that has been carried out. It is useful to determine whether the training
of the model has been completed correctly. Evaluation metrics help us to evaluate
the quality of models and measure their performance. Once a performance value is
assigned for each model, different models can be compared and therefore the best
one can be chosen based on the metrics. Depending on the problem, different metrics
could be used. Nevertheless, this section will focus on those that are most used in
computer vision problems.
     As has been mentioned in Section 2.2.2.1, image classification techniques aims to
correctly label each image with its respective class. So, the most accurate model is the
one that correctly labels all images. That is,

                              N umber of correctly labelled images
                Accuracy =                                                          (2.8)
                                T otal number of labelled images
equation obtains model’s accuracy. It is trivial that when the result of the equation is
equal to one, all images have been correctly classified, while if the result is equal to
zero, no image has been correctly classified.
     Equation 2.8 can be reliable for problems with a small number of classes. However,
when the number of classes increases, this metric extremely penalises that the image
label does not match the most likelihood class. This is a problem for datasets with
thousands of images, where some classes are closely similar to each other and are
extremely difficult to differentiate. In order to solve this issue, top-N accuracy is
proposed: prediction will be considered correct when any of the N highest probability
classes match the image class.
     In the case of binary image classification and anomaly detection, one of the most
representative metrics is the confusion matrix (see Figure 2.6). Confusion matrix
counts correctly classified samples in True Positive (T P ) and True Negative (T N ),
for positive and negative classes, and misclassified samples in False Positive (F P ) and
False Negative (F N ). The following metrics can be derived from that representation:



                      TP                                          TP
     P recision =                (2.9a)              Recall =                    (2.9b)
                    TP + FP                                     TP + FN


     Even if accuracy is the simplest and best known metric for binary classification,
there are situations where drawing conclusions from this metric is not straightforward.
In the case of unbalanced datasets, Precision and Recall are more suitable metrics,

22
                                             2.3. Defect detection in industry quality control


                                                 Prediction
                                      Positive                   Negative



                  Negative Positive
                                        TP                          FN
         Actual



                                        FP                          TN



Figure 2.6: Scheme of a confusion matrix for binary classification. T P , F P , F N and
T N are the abbreviations of True Positive, False Positive, False Negative and True
Negative, respectively


because they focus only on one type of errors, that is, positive class errors (Equation
2.9a and 2.9b). Based on two metrics above, F1 score and Receiver Operating
Characteristic (ROC) curve are defined. The latter visualizes the trade-off between
                   N
Specif icity ( F PT+T N ) and Sensitivity (same expression as Recall) applying different
decision thresholds to the model. Area Under the ROC Curve (AUC) summarizes
ROC into a value and the closer the AUC value is to one, the better the model will be.
    The assessment of object detection problems differs from the above. In this case,
a different metric is needed to assess the object localisation capability of the model. In
addition, this metric has to evaluate the accuracy of the model for detecting objects of
different classes in the same image. Mean Average-Precision metric (mAP) resolves
those requests [72, 74]. Firstly, Intersection over Union (IoU) calculates the difference
between the ground-truth and the bounding-boxes predicted by the model. A threshold
is used to determine whether the prediction is True Positive or False Positive, and thus,
calculate Precision and Recall. Finally, Precision values are calculated for each of the
11 arbitrary Recall values (from zero to one in steps of one decimal place) and are
averaged over the eleven values. mAP will be the average of APs for all classes in the
dataset. Image segmentation problems are similarly assessed. However, in order to
calculate the IoU, segmentation models do not generate bounding-boxes, so these are
replaced by the number of correctly classified pixels of each class [81].


2.3     Defect detection in industry quality control
Quality control can be understood as the process that tests a product throughout the
manufacturing line in order to ensure that meets market standards, and therefore, it

                                                                                            23
2. T HEORETICAL BACKGROUND


can be marketed safely. A thorough and product-specific quality control is able to
detect possible faults in the product, which allows to correct these defects in time and
provide a higher quality product.
     Depending on the strategy employed, quality control processes can be of two
types: destructive or non-destructive. In the former case, the aim is to learn how the
material performs under extreme pressure conditions. In most cases, products are
deformed or completely destroyed during testing, so those products must be discarded.
Destructive analysis allows us to know the specific conditions that products are capable
of withstanding, such as maximum pressure, elastic limit or temperature, among others.
This information is necessary to define recommendations for the correct operation
and maintenance of products. Product destruction does not entail a high cost for
mass-production companies, as the economic value of a small number of products
is insignificant in manufacturing environment. For example, traditional automotive
industry relied on physical destructive testing of prototypes to verify car’s ability
protecting its occupants in survivable crashes [97]. The latter aims to detect and
measure defects that appear in products, using techniques that do not extract samples
or cause any kind of irreparable damage. These non-destructive techniques are used in
many steps of the production line validating product reliability, minimizing production
costs and meeting quality standards [98]. As these techniques do not destroy the
product, they are often used to confirm the quality of products just before those are
shipped to the customer.
     Figure 2.7 shows the most relevant methods for industry quality control, classifying
them in destructive or non-destructive methods [99, 100].
     Methods at the top are related to the material characteristics of products and aim
to stress them in order to understand properties of those materials [100]. Hardness
testing is based on the material’s resistance to indentation stress. The depth of
indentation determines hardness of the material and is measured with the Rockwell
and Brinell scales [101]. Tensile and Torsion testing are also stress-based tests [100].
Tensile testing discovers, through stress analysis, the highest point of elongation or
compression of test piece prior to failure. Torsion testing, by contrast, performs a
similar process but with the aim of determining the instant when test piece deforms.
Finally, Fatigue testing [102] aim to determine the number of cycles that the test piece
withstands by subjecting it to a repetitive sequence of stress amplitudes.
     Non-destructive methods perform a non-invasive inspection of test pieces without
stressing them or changing their properties. Several of these methods are well
established in the quality control of manufacturing industry. Ultrasonic testing
is successfully employed in additive manufacturing, where porosity failures are

24
                                     2.3. Defect detection in industry quality control


                                                                  Hardness testing
                                                                   Tensile testing
                                                                   Torsion testing
                                    Destructive
                                     methods                       Fatigue testing



        Industry                                                 Visual inspection
      Quality Control
                                                                 Ultrasonic testing
                                                             Acoustic emission testing
                                                                  Infrared testing
                                                              Liquid penetrant testing
                                  Non-destructive
                                     methods                 Magnetic particle testing
                                                                 Vibration analysis


     Figure 2.7: Diagram of most commonly employed quality control methods


a significant problem [103, 104]. Liquid penetrant testing is used extensively in
automotive and aviation industry for surface cracks inspection [105]. Vibration
analysis is carried out to accurately assess quality of rotating machines [106]. Infrared
testing is widely used in aluminium inspection, applied in aerospace industry due to is
low weight and high specific strength. For this reason, Infrared testing is a suitable
option for testing component quality in aircraft and helicopters [107].

2.3.1 Optical defect detection
In manufacturing industry, visual-based or Optical Quality Control (OQC´) is typically
used as the standard procedure for defect detection [108–110]. In order to perform an
effective inspection, numerous images are taken to obtain an overall representation
of the manufacturing product in two dimensions. These images can be either surface
[109] or X-ray images [108].
     X-ray testing or Digital Radiography (DR) is one of the most widely accepted
methods for non-destructive OQC [111], since it can detect defects that are not on
the surface or that are masked, without destroying the final product. An overview of
X-ray image formation is explained below [112]:

■   Radiation leaves X-ray source towards the object.

■   The object absorbs the radiation when X-rays are passed through it.

                                                                                      25
2. T HEORETICAL BACKGROUND


■    A detector captures the radiation intensity attenuated by the object and an X-ray
     image is formed.

■    A manipulator moves the product so that new images can be obtained from different
     angles.

      By repeating this process, several images can be obtained from different perspectives
of the same object, obtaining relevant information of the product as a whole. Therefore,
X-ray imaging is a reliable method to know product internally. However, depending
on the product characteristics, special importance is sometimes attached to the product
surface. In such cases, surface images are chosen for product inspection [113].
      While there are some industries where this process is carried out by operators,
most industries require a higher performance method. Undetectable defects to the
human eye or a large number of parts produced in a short period of time make the task
of inspection unfeasible for the operator. The need for an automatic method for defect
detection in these cases has driven research in this field for years.
      Optical defect detection research has undergone significant changes in recent years.
The success of DL in computer vision and pattern recognition has pushed traditional
techniques aside to make way for novel methods with higher performance and
accuracy. Furthermore, DL eliminates the necessity for problem-specific knowledge
and automates image processing, allowing some of the knowledge gained in the
process to be transferred to a less resource-intensive process.
      Following with OQC, Chapter 3 gathers the most relevant and well-known casting
defect detection techniques and considers a taxonomy according to the approach they
are based on. Although this chapter focuses on castings, the methods are similarly
applied to the quality control of other manufacturing products.




26
                                                                         Chapter 3


                       State of the Art

This chapter contains the literature review of the state of the art in defect detection
in industrial casting processes through computer vision, and it aims to present the
latest research in this field. In a first section, the methodology used throughout the
literature review is presented. Then, similar studies related to OQC in castings are
presented. In this section, works are sorted in chronological order, starting with an
outline of traditional techniques and ending with an exposition of the current state of
the art of DL-based approaches. Afterwards, the last section provides a taxonomy of
semi-supervised learning methods for common defect detection problem. Finally, the
chapter ends with a critical review summarising the conclusions drawn in the previous
sections.


3.1     Methodology of Literature Review
This section contains the methodology used throughout the literature review. Firstly,
Figure 3.1 shows a funnel scheme in which the way of collecting and synthesizing
previous research is shown. At first, some broad topics are reviewed, in order to obtain
and review basic knowledge publications. These topics are computer vision, ML, DL,
and transfer learning. As the review continues, information gathering dives into more
specific works within above themes. Finally, the research gap is identified, leading to
a number of specific hypotheses to be tested.
    In order to accomplish the goal of obtaining a quality literature review, it is
necessary to collect a large number of articles. Thus, some of the most known
databases are utilized: from multi-disciplinary databases such as Web of Science,
Scopus and SpringerLink, to engineering-specific databases such as Engineering
Village and IEEE-Explore. Also, Google Scholar has been used as a search engine.
    The evolution in the number of publications associated to casting defect detection
between 1977 and 2022 is shown in Figure 3.2. The ascending trend is related to the

                                                                                     27
3. S TATE OF THE A RT



                 Deep learning and                 Computer vision and
                 machine learning                   transfer learning


                Semi-supervised               Supervised casting
                   learning                    defect detection


                                  One-class approach
                                  in defect detection



         Figure 3.1: The method used to perform the literature review
         represented by a funnel scheme




         Figure 3.2: Number of publications related to casting defect
         detection (1977-2022) [4]


increasing popularity of ML and DL techniques, and also to the growth of I4.0. So,
as a way to handle the large volume of works, keywords and logical operators are
used to delimit research area. Terms such as ("defect" OR "flaw") AND "detection"
and "deep learning" are searched, and they are combined with secondary search terms
such as "castings", "x-ray", "convolutional neural networks", "surface inspection" and
"automated visual inspection".
     In order to conclude summing up the methodology of literature review, two figures
are presented. On the one hand, Figure 3.3 shows which publishers have published

28
                                               3.2. Optical Quality Control in castings




                                               Figure 3.4: Histogram of publications
                                               according to the type of article and
      Figure 3.3:       Publishing             techniques for casting defect detection
      companies relating to casting            problems.     Statistics and spectral
      defect detection                         belong to traditional approaches
                                               discussed below


articles related to castings defect detection, and on the other hand, Figure 3.4 shows
the distribution of the studies according to the type of article and the techniques that
have been addressed. From those figures we can conclude that the most relevant
publishers show interest in this topic and the trend within defect detection in castings
is towards deep learning research.


3.2     Optical Quality Control in castings
This section gathers the most relevant researches based on OQC processes in castings.
Literature review is explained chronologically: first traditional methods are classified
and drawbacks are mentioned; later, ML-based traditional techniques are presented;
finally, DL-based approaches are analysed.


3.2.1 Traditional approaches

A suitable way to start a literature review is by reading previous research around
the field is being investigated. In this field, several researches have been carried out
a taxonomy of different methods used in the literature for detection of defects by
automated visual inspection. To the best of our knowledge, Kumar [114] presents the
first classification for techniques applied to the problem of the fabric surfaces defect
detection using computer vision. In this case, the techniques are divided into Statistical,
Spectral and Model-based. Statistical methods employ techniques such as first order
or grey-level statistics and edge detection in order to extract relevant image features
[115]. Spectral techniques [114] focus mainly on applying filters to those images

                                                                                       29
3. S TATE OF THE A RT


composed of repetitive basic textures. For those with a stochastic distribution of
textures, spectral methods are not reliable. Therefore, such images are analysed using
stochastic models. Czimmermann et al. [116] not only follow the same scheme for a
wide range of industrial applications, but also add a new category named Structural.
They stat that structural approaches aim to extract geometric relationships and spatial
statistics in order to locate texture elements. They explain that those elements are
generally formed by simple greyscale regions, lines or individual pixels. However, as
spectral techniques use ground rules and localization, they do not work well when
the image is non-deterministic. Therefore, the use of such techniques is reduced
to the examination of images with little or no texture. Regarding the inspection of
castings, a first idea is presented by Boerner et al. [117] focusing the problem towards
Aluminium castings, while a more current study by Luo et al. [118] makes a review of
the techniques applied in the detection of defects in Flat Steel Surface.
      Furthermore, Mery et al. [119] divide casting defect detection approaches in three
main groups:

■    Reference Methods: Specific filters are configured for each part of the product, so
     this approach is dependent on the shape and size (among others) of the manufactured
     part. They require prior knowledge of the product and the proposed algorithms
     have a complex structure. Furthermore, they are not suitable when the product has
     a random texture.

■    Methods without a priori knowledge: Relevant features of images are extracted
     using statistical techniques. Subsequently, a discriminator is built based on those
     features and image classification is carried out between defective and non-defective
     images. There is no knowledge of the test image shape. These techniques become
     unreliable when illumination changes occur and defects are very subtle.

■    Computer Tomography (CT): This method takes images of different sections
     of the product from many angles, producing a 3D reconstruction. CT needs an
     advanced equipment, more expensive that the equipment used in DR (Section 2.3.1).
     Also, CT is time-consuming process. Therefore, DR is the main approach used in
     OQC in castings, instead of CT.

      Besides the classification for traditional approaches discussed above, they compile
different techniques considered state of the art from that time, based on the proposed
classification. To the best of our knowledge, the first vision-based methods that
addressed the problem of casting defect detection use an error-free reference image to
subtract it from test images [120]. The error-free image is obtained applying a bank of

30
                                              3.2. Optical Quality Control in castings


filters specifically chosen taking into a count the features of the casting piece. The
result of the subtraction is a new image containing defects only. Therefore, if the
residual image retained any defect, the product is considered defective.
    Tsai et al. [121] propose a unsupervised method to detect defects in a range of
statistical textures, including surface images of castings, based on a global image
reconstruction scheme using FT. Applying this method, repetitive and periodic
patterns are removed from randomly textured images. Therefore, anomalies are
detected using a threshold in the restored image. They avoid the main problems of
extracting local textural features: the selection of adequate textural features and the
high dimensionality of the feature vector.
    Typical defects found in castings are presented in [122]. Defects such as shrinkage
cavities, air holes and foreign objects are detected using three different methods. While
second-order derivative and morphology-based operations and row-by-row adaptative
thresholding obtain weak results for some defect types, the approach based on Wavelet
Transform can detect all defects from each defect type.
    Instead of using filters to isolate the defects from the entire image, statistical
techniques follow 3-step workflow:

1. Image segmentation to identify regions in which a real defect may appear. These
   algorithms aim to narrow down the image to some interesting patches and thus
   reduce the processing time in the following steps [123].

2. Feature extraction from the regions with potential defects. Features are divided
   in two groups [124]: geometric and grey level features. This process is carried out
   using statistics such as crossing line profile [125], LBP [126] and co-occurrence
   matrix [127]. The significant features are gathered in a feature vector.

3. Classification aims to feed the feature vector into a discriminant model to sort
   the images in flaws or regular structures. Images with similar feature vector will
   be classified with the same label. It is worth mentioning that the purpose of this
   final step is to remove false alarms obtained in segmentation while keeping the real
   defects.

    A comprehensive study following this workflow is carried out by Mery et al. [124].
They divide the image in potential flaws and background using LoG and zero crossing
algorithm. Then, seventy one features from regions with defects are extracted. In this
step, a feature selection is done through sequential forward selection [128], which
consists of selecting features one by one by means of highest performance until the
accuracy of the classifier does not get any improvement. Finally, five classifiers are

                                                                                      31
3. S TATE OF THE A RT


designed to classify potential flaws: linear with Least-square approach, threshold
classifier, nearest neighbour, Mahalanobis and Bayes. A conclusion drawn from
this study is that the geometric features do not provide information for discriminate
between the classes. The authors stat that it is because defective regions and regular
structures have similar shapes.
     The most relevant part from the statistical approach is the feature extraction
process. Praus [129] presents a research comparing three different techniques to
obtain significant features from casting images. First-order statistics, singular values
obtained from SVD and second-order statistics derived from co-occurrence matrices
are the extracted features. Then, feature vectors are divided in two clusters using
Ward’s method. In a similar work, Mery [125] proposes a novel approach to extract
features ensuring the homogeneity of the potential flaw window. Crossing line profiles
is based on the idea of crossing straight lines from different angles of the potential
flaw window and selecting the profile that include mayor grey value similarity at the
edges of the window. Many features are obtained from those profiles.
     Statistical based techniques are useful when a fast response is needed because their
computation time may be smaller than other more sophisticated techniques. Moreover,
statistics are white box methods. This means that all the results can be clearly traced
and explained.
     However, this type of techniques are unreliable as additional knowledge of each
specific problem is needed to obtain the maximum model performance. The lack of
generalization is a clear disadvantage for this type of techniques, as they are inflexible
to changes [119]. Moreover, improvements in some areas of data analysis lead them
to become obsolete. In their place, new techniques of ML and DL rise up to deal
with general vision problems and defect detection in industrial processes, particularly.
Nevertheless, the three steps presented above, i.e., segmentation, feature extraction
and classification, are still in use with the new approaches.



3.2.2     Machine learning-based approaches

This section includes the most relevant techniques for casting defect detection related
to ML approach.
     In comparison with statistical techniques, ML focuses on classification step, as
has been seen in the workflow from Section 3.2.1. It means that ML models also
extract the features from the images using similar computer vision techniques used by
traditional approaches, but feature vectors are classified by traditional ML algorithms.
The main benefit of this approach is an improvement on the classification step, using

32
                                             3.2. Optical Quality Control in castings


a higher quality discriminator that builds the boundary dividing flaws and regular
images.
    The work by Pastor-Lopez et al. [130] compares the performance of established
ML algorithms in defect classification of castings surface images. Several configura-
tions of Bayesian networks [131], Support Vector Machine (SVM) [132], k Nearest
Neighbors (k-NN) [133] and random forest [134] are evaluated using features obtained
from greyscale and colour segmented images. The principal drawback is that the
segmentation of the potential flaws does not perform well and further research is
needed.
    Hernández et al. [135] propose a method based in Self-Organizing Map to reduce
dimensionality in feature space and a neuro-fuzzy model as classifier. Improvement
of this approach is the reduction in false alarms rate, compared to traditional computer
vision techniques.
    Potential defect regions are obtained applying reference-based methods to an
entire image by Zhao et al. [123]. Reference-based methods aim to detect defects
by comparing test images with its non-defective counterpart. They declare in their
work that the evaluation of the whole image is not accurate enough to reject the false
detections caused by the local part-to-part variation. In order to solve segmentation’s
shortcomings, they propose a local registration-based approach, obtaining accurate
defect segmentation mask. Later, twenty nine dimensional feature vector is fed into
random forest model. The results show that local segmentation technique decreased
the number of false alarms, increasing performance of the model.
    Domingo et al. [126] perform an extensive study including twenty four computer
vision techniques and create a new dataset to evaluate them. Different feature vectors
and ML classifiers are analysed. In addition, DL models for image classification are
mentioned shortly, but their results are worse than the rest. In another work, Wu et al.
[136] have to deal with the comparison of different ML algorithms to obtain the best
performing model, but to the best of our knowledge, this study is the first to use DL
techniques in OQC. Apart from that, a new dataset is presented based on benchmark
database GDXray [137], a casting dataset formed by patches of 32x32 pixels. The
process consists of extracting the cropped images of the defects and applying data
augmentation techniques to obtain twenty four new patches. The same process is
carried out for patches without defects.
    The ML-based models discussed above follow a discriminant approach. This
approach obtains information from images of different classes and learns a boundary
that divides optimally these classes using an hyperplane in the feature space. Training
is carried out through error-minimisation algorithms.

                                                                                     33
3. S TATE OF THE A RT


     However, defect detection problems may sometimes suffer from unbalanced
dataset, as defect images are quite difficult to gather in a real production line. Therefore,
One-class Classification (OCC) approach appears to solve this issue. One-class
classifiers are trained using only one type of images, normally defect-free images.
This process is based on semi-supervised learning. Thus, a new image that is situated
further from training data than a threshold is considered defective image.
     In order to deal with the shortcomings of a unbalanced dataset, Ramirez et al.
[138] perform a comparative study between one-class and discriminant approaches.
They improve a one-class classifier using boundary-based method named Support
Vector Domain Descriptor (SVDD). SVDD aims to obtain an optimal hypersphere
surrounding all training defects-free images and therefore new data out of the hyper-
sphere is considered defect. The work concludes that, although discriminative models
obtain better results, the good performance of one-class classifiers and the difficulty
of balancing datasets make the latter a better choice for real industrial processes.
     Following this approach, Riaz et al. [139] propose a unsupervised classification
method for surface microscopic images based on K-means clustering. Meanwhile,
Pastor-Lopez et al. [130] face unbalanced data with Synthetic Minority Over-sampling
TEchnique (SMOTE). They over-sample the less populated classes creating synthetic
images and under-sample the more populated ones, removing them. Therefore,
instances are still unique and classes become more balanced.
     Before DL methods are presented, it is worth mentioning that DL usually requires
a large amount of coherent data to address the proposed problem. Sometimes obtaining
that many samples is infeasible for companies and DL models perform worse than
traditional ML-based models. Therefore, traditional ML approach may be a good tool
dealing with small datasets [127].



3.2.3    Deep learning-based approaches

This section reviews the most relevant DL-based works addressing the problem of
casting defect detection.
     The main objective of DL is to replace handcrafted feature extraction process,
where expert knowledge is needed and has a poor scalability, with an automatic and
high representational feature extraction approach. It is proven that results obtained by
DL improve those from complicated problem-specific architectures, principally when
available data and computation resources are sufficient [41]. Those approaches follow
the workflow mentioned in Section 3.2.1, i.e., segmentation, feature extraction and
classification, but they carry out all three steps in an end-to-end model.

34
                                               3.2. Optical Quality Control in castings


     Traditional feature extraction in castings is carried out by highly qualified experts.
This process is time consuming and results obtained for a product line may not usually
be transferred to another line, because of the algorithm’s specificity and complexity.
DL, as in many areas of computer vision, has been established as state of the art in
casting defect detection. Techniques based on DL learn directly from images, so expert
knowledge is not as necessary as in previous approaches. Moreover, an algorithm built
for a specific problem can be transferred to a similar problem, due to its generalization
learning capacity.
     In Figure 3.5, a diagram of casting defect detection problems for which deep
learning techniques are used is shown, and it can be concluded that defect detection
using DL techniques are summarised in three categories:

■   Image Classification: Mainly, classify casting images into two classes: defective
    or non-defective. It may be used to identify a single object in the image or to carry
    out a multi-class classification between different defect types.

■   Defect localization: Locate defects throughout the image, and place a tight-fitting
    bounding box for each defect. Defect localization may be carried out splitting
    the image into small patches and classifying them. This approach is known as
    sliding-window. Nevertheless, object detection algorithms perform better in this
    type of tasks.

■   Defect segmentation: Classify each pixel in the image as non-defective or defective.
    Consequently, segmentation is a more accurate solution than localization to locate
    defects in the image. Also, it is generally more difficult to accomplish.

     Nowadays, researches mainly use convolutional layers as principal element to
perform casting defect detection. A CNN backbone (see Section 2.2.1) is used to
extract the most representative features from the image. These models are pre-trained
on a huge base dataset (ImageNet has 1000 classes with approx. 1000 images per
class [56]), so the knowledge is transferred to a new model that is trained on a specific
target dataset. This method is known as transfer learning. The aim of this approach is
to obtain relevant visual information from the first layers of the backbone. The last
layers related to the base dataset are removed from the model and these are replaced
by layers trained using a specific dataset via fine-tuning. Transfer learning is used to
speed up convergence in the training phase and avoid overfitting. Furthermore, the
final model tends to perform better than those with randomly initialized weights [140].
     A CNN backbone and a classification head are the two building-blocks of the
model architecture for image classification. The latter include a set of fully connected

                                                                                       35
3. S TATE OF THE A RT


                                         Casting defect
                                        detection in DL


           Image                             Defect                           Defect
       classification                     localization                     segmentation




                                                      Object                  Image
 Entire
                    Sliding-window                   detection             segmentation
 image
                                                    algorithms              algorithms


  Figure 3.5: Summary of DL techniques applied to Defect Detection in castings


layer with a sigmoid layer (binary classification) or softmax layer (multi-class cla-
ssification) to get the probability of each class. If K is the number of possible classes,
last layer will return K outputs with the probability of each class. Jiang et al. [141]
apply a ResNet model pre-trained on the ImageNet dataset to obtain six defect types
on castings surface images. They also propose an improved activation function to
avoid neuron-death problem, i.e. condition when a neuron’s parameters are not
renewed and it always outputs null value. As mentioned in Section 3.2.2, Mery et
al. [126] design a CNN model formed by ten layers, including convolutional layers
and activation functions, max-pool layers and drop-out. This model is trained from
scratch and does not obtain better performance than ML methods. It even overfits.
The poor performance may be due to the fact that small amount of data is available, as
mentioned at the end of the section above.
     Furthermore, Kuo et al. [109] compare the performance of four classic CNN
models (AlexNet [41], VGGNet, GoogLeNet [42] and ResNet) in the detection of
sandblasting defects using surface images. They measure models’ results using
different images and batch sizes. The study suggests that batch size influences the
training results to some degree but more training data and tests are required to prove
the hypothesis. Moreover, the bad performance of GoogLeNet and ResNet indicates
that models with residual learning architectures are not suitable for the detection of
sandblasting defects in investment castings. Nevertheless, this type of methods, i.e.
those that analyse the entire image by a feature extractor, are not sometimes able to
detect small defects. In order to solve this issue, Wang et al. [110] present a novel
strategy extracting features based on self-attention guided module from car casting
images. The method is based on a first network that extracts general features from
X-ray images, following by a combination of a self-attention guided module and

36
                                               3.2. Optical Quality Control in castings


a residual block that carries out a subtle feature extraction, which provides a more
accurate representation of small defects.
    Even though classifying a casting into a defective or non-defective image is a
challenging problem, defect localization is gathering much of the research in defect
detection. A first attempt divides the image into small regular patches by a detection
window moved over both horizontal and vertical directions [111] and analyse each
individually. This is known as sliding-window approach. Each patch contained
location context of the image, so classifying them, position of defect regarding the
image is obtained. The smaller the patch, the higher the accuracy of defect localization.
    Mery [142] builds a CNN classifier and uses a 32×32 pixel window to obtain
the localization of possible defects. In order to avoid issues caused by a small and
unbalanced dataset, Mery simulates defects in defect-free patches using 3D ellipsoidal
models and GANs. This method balances the dataset. It concludes that models trained
with real defect and ellipsoidal simulations obtains better performance that those
trained with GANs.
    However, sliding-window approach lacks the global information of the casting.
Furthermore, patch-size is dependent on the defects: big defects are not well detected
by small patches, and vice versa. So as the size of the defects varies in the dataset, the
problem becomes computationally expensive [111]. In order to solve this issue, object
detection algorithms are used.
    Following the classification made in Section 2.2.2.2, object detection algorithms
applied to casting defect detection are presented. It is worth mentioning that these
algorithms are developed to solve other tasks within computer vision (generic object
detection, salient object detection, face detection). Moreover, one of the main problem
to overcome in casting defect detection is the poor semantic difference between
background and foreground, which avoids to obtain relevant features to differentiate
defects throughout the image. Therefore, modifications to defect detection algorithms
are necessary to overcome the drawbacks of this type of problem.
    Ferguson et al. [143] compare sliding-window approach with SoTA object
detectors. They apply two region-based (Faster R-CNN, R-FCN) and one-stage
detector (SSD) as object detectors, whereas the sliding classifier is based on XNet
architecture [126]. VGGNet and ResNet are used as feature extractor. The highest
accuracy is obtained with Faster R-CNN and ResNet-101 as feature extractor (mAP =
0.921), but inference time for this model is slower than the rest. Instead, SSD with
VGG-16 is ×16 faster, but the accuracy is significantly lower. The results of this work
suggest that the location accuracy is inversely proportional to inference time. One year
later, they present an improved model capable to beat the above models in terms of

                                                                                       37
3. S TATE OF THE A RT


accuracy [144]. However, even if the new model’s evaluation time is faster than Faster
R-CNN, it fails to overcome SSD’s evaluation time. The model is based on Mask
Region architecture, so the research is extended to an image segmentation problem.
     Following the idea of work above, nowadays the state of the art is situated in
object detection techniques [145, 146]. Nevertheless, two-stage methods (Section
2.2.2.2) based on region proposals may have the disadvantage of not detecting some
small defects. Instead, one-stage detectors learn image representations of different
sizes, obtaining local and global context of image, so both large and small defects
may be detected. Also their inference’s rapidity make them a better option against
two-stage methods.
     For the first time, Du et al. [76] apply an FPN to casting defect detection,
since Faster R-CNN occasionally fails to detect small defects. According to FPN
architecture, knowledge obtained from the image has a strong semantic information
and is able to locate accurately defects of different sizes. Additionally, they employ
RoIAlign [81], a bilinear interpolation to resolve the misalignment by extracting
regions of interest from feature maps. In the article published one year later [147], Du
et al. aim to solve two main issues: loss of location information in deep feature maps
caused by down sampling layers and suppression of high number of defect proposals.
For the former, they utilise DetNet [46] as backbone and PANet [148] to reduce
distance between low- and high-level features, obtaining more locate information
from high-level features. For the latter, they implemented soft-NMS to remain more
proposals that have high classification confidence. They also define soft-IoU metric
to solve drawbacks caused by IoU when many ground-truths are inside a predicted
bounding-box.
     Other one-stage object detection algorithms are used to detect defects in castings
(SSD [149], YOLO [145, 150]). These models speed up inference time and keep
performance of previous models, covering industry expenses in terms of speed and
accuracy. Nevertheless, in the case of YOLO, its high speed is weighed down by its
lower accuracy compared to non-real-time models [145].
     As seen in the classification models, unbalanced datasets also affect the performance
of object detection models. In addition to the issue of having too few defective samples,
this approach also has the difficulty of labelling all the defects from images. Drawing
bounding-boxes for each defect implies more effort than image-wise labelling so, in
many cases, the process turns infeasible.
     Mery [108] builds a training set of X-ray images employing only a few defect-free
samples. Defects are created using ellipsoidal models and embedded to non-defective
images. This process avoids labelling each defect and allows to do automatically. Mery

38
                                              3.2. Optical Quality Control in castings


compares eight different object detection models and YOLO-based model obtains best
results in terms of speed and accuracy. In another work, Mery et al. [151] propose
another method for simulation of realistic defects. It is based on applying low-pass
filter to binary images of lines in random orientations, and adding the result to a
defect-free image. They conclude that this strategy performs better than ellipsoidal
defects.
    Additionally, new no-identified defects may appear over time in a production line.
In this case, discriminant models are not able to detect new defects because the model
has not learnt their features. Therefore, if a new defect appears in the production
line, these models will not be able to identify it. This is a compromising situation for
companies and it may lead to a large number of economic losses. In order to solve
this issue, semi-supervised learning-based new techniques need to be adopted in the
defect detection problem.
    To the best of our knowledge, Thang et al. [152] publish the most comprehensive
work relating to casting defect detection using a semi-supervised learning approach.
This work proposes a DL framework based on a CAE to detect defects in castings
images. As semi-supervised learning technique, only good samples of data are used to
train the model. Hence, test images that are far from training behaviour are classified
as defects. Defectiveness is measured by pixel-wise subtraction between input and
output images. High subtraction values indicated that image is poorly reconstructed,
so if the model is correctly trained, it would signify that image is defective. Otherwise,
correct reconstruction indicates that the initial image is non-defective. They propose
a sliding-window approach based on two-steps. In the first step, sliding-window is
applied through the image obtaining easy-to-detect defects. The second step handles
the detection of small defects, applying an edge detection to remove model noise and
distinguish from defects. They achieve 97.5% of accuracy in their dataset with only
30 images. Also, Lehr et al. [153] compare a CAE built from scratch with a ResNet
used as feature extractor. ResNet is pre-trained on ImageNet dataset and they apply
fine-tunning to learn features from target data. They achieve absolute accuracy on
their private dataset. Also, they suggest to cluster the defective images into different
groups based on their similarity, reducing the time of categorization of the defect
carried out by employees.
    Many of the works that use semi-supervised learning-based approach are not
closely linked to a real industrial process. Some of them focus on solving less
complicated problems such as automatic surface inspection [154–157]. This type of
problem is easier to solve because the uniform texture of the products does not have a
large variability, which is the case of casting products. In this work, automatic surface

                                                                                       39
3. S TATE OF THE A RT


inspection problems are not taken into consideration.


3.3 Semi-supervised learning in Defect Detection
The research carried out for defect detection using DL-based techniques is more
extensive than the one presented in Section 3.2.3. Unlike the section above, this section
is not focused on castings products but it gathers the works related to general defect
detection problem with a semi-supervised learning-based approach. Nevertheless, the
studies mentioned in this section are not based totally on an industrial approach.
      While some studies apply semi-supervised learning-based techniques for defect
detection in datasets containing industrial products, the vast majority does not take into
account situations that may occur in these processes, such as, fluctuating illumination
conditions or the large variability on the orientation of the objects. Previous DL-based
works aim to develop a robust method that will not be susceptible to small or medium
variations and generalize well to conditions on production line [153], however, these
semi-supervised learning techniques focus on obtaining top accuracy in benchmark
datasets with the aid of no labels. In addition, even if these methods achieve good
results in specific datasets, the performance in casting images is not proven. So, the
efficiency of SoTA semi-supervised learning-based techniques is yet to be proven in
environments where inter-class variance between non-defective and defective products
is slight.
      This section classifies current SoTA semi-supervised learning-based methods
applied defect detection problems, and presents the benchmark datasets that are being
used to evaluate those techniques.


3.3.1        Benchmark datasets

In a recent work [108], Mery suggest that one of the reason why casting defect
detection research has been slower than others computer vision tasks is the poor
availability of public datasets. Mery highlights that most of the datasets used in the
experiments reported by the industry and academia are private. This situation does not
help to evaluate novel works or create a benchmark to compare developed techniques.
However, when the scope is widened to industry manufacturing products in general,
the number of benchmark datasets increases. In the last few years, two industrial
defect detection public datasets have been released:

■    MVTec AD (MVTec Anomaly Detection Dataset) [11] gathers 5354 images from
     real-world manufacturing inspection scenarios. 10 different objects (Bottle, Cable,

40
                                   3.3. Semi-supervised learning in Defect Detection


    Capsule, Hazelnut, Metal Nut, Pill, Screw, Toothbrush, Transistor and Zipper)
    and 5 unique textures (Carpet, Grid, Leather, Tile and Wood) are provided, with
    73 types of defects. Defects are labelled by pixel-wise ground-truth, accurately
    identifying where defects are located in the image.

■   BTAD (beanTech Anomaly Detection) [158] contains 2830 real-world 2D images
    from 3 different industrial products. For each defective image, a pixel-wise ground-
    truth mask is given. BTAD is the dataset most closely related to the aim of this
    research.

     Both datasets have non-defective images for training subset, i.e., only normal
products are used to train the model. For this reason, one-class techniques are applied
to these datasets instead of discriminative ones. Therefore, the target is reduced
to a OCC problem. Nevertheless, pixel-wise ground-truth allows to evaluate more
precisely defect localization models. So, not only image classification into defective
and non-defective is carried out, but also localization of defects throughout the image.


3.3.2 One-class methods
As has been mentioned in the end of the previous subsection, semi-supervised learning-
based techniques are based on one-class approach in order to train models for a specific
problem. One-class approach is used when obtaining samples from one or more of
classes are difficult to conduct or simply infeasible. The model is built using examples
of the target class, so it obtains knowledge from that class only. The aim is to be able
to distinguish other classes samples from those learnt in training phase. In the case
of defect detection, the target class is composed of images with no defects, whereas
images of defective products form another class.
     First attempts to approach defect detection problem based on semi-supervised
methods are carried out using CAE [159] and GAN-based [160, 161] techniques.
Even if they are straightforward architectures and their performance is noteworthy,
they suffer from noisy training data [89]. Besides, they are not able to detect subtle
defects. Subsequently, Yi et al. [92] propose a patch-level Deep SVDD-based [162]
defect detector that improves previous methods performance. They claim that patch-
wise training allows for locate defects throughout the image and such fine-grained
examination improves overall detection performance.
     In order to avoid the drawbacks of reconstruction methods, Rippel et al. [163]
attempt to model distribution of normal data. They show that representations obtained
by discriminative CNN models trained on large datasets can be transferred to detect
defects in a target dataset. Their experiment indicates that those feature combinations

                                                                                     41
3. S TATE OF THE A RT


that retained little variance from feature space, are the crucial ones for discriminating
between defective and non-defective images, as well as considerably reducing feature
space.
      The work published by Liang et al. [93] summarises principal semi-supervised
approaches for defect detection into a concise taxonomy, categorising current methods
in Density-based, Classification-based and Reconstruction-based approaches.

■    Density-based approach tries to estimate the distribution of the normal data.
     Features are obtained by convolutional backbones pre-trained on ImageNet and they
     are transformed [94] by NF or fitted to a multivariate Gaussian [163]. As anomalies
     should be out of normal data distribution, models built using this approach output
     lower likelihood for abnormal samples and therefore use likelihood of those samples
     as anomaly score [164].

■    Classification-based methods aim to find the classification boundaries of normal
     data in the latent space [93]. Most of the methods define a proxy task based on
     self-supervised learning in order to detect flaws in normal data. Some of them
     create synthetic anomalies from normal samples and build a model to discriminate
     between both classes [165, 166].

■    Reconstruction-based methods learn to reconstruct normal images from scratch,
     so when the model tries to reconstruct a defective image, it is far away from the
     input image [167]. An anomaly score is set as threshold and inferred images with
     a reconstruction error higher than it are classified as defective. Traditional CAE
     follows this approach.

      Following this strategy, Table 3.1 splits principal approaches into several methods
based on the literature, and advantages and drawbacks of those approaches are
presented.
      Recently, Mishra et al. [158] propose a Vision-Transformer model based on image
reconstruction and patch embedding approaches. The input image is split into patches
and an CAE method is built to reconstruct it. The features obtained in the encoding are
modelled by a Gaussian mixture density network, in order to estimate the distribution
of the normal data in the latent space. The latter allows to locate defects throughout
the image. They evaluate their method in BTAD benchmark dataset.
      Meanwhile, Yang et al. [173] propose an image reconstruction based on multi-
scale representations of the image. A pre-trained CNN model extracts features with
different levels of information to have both local and global knowledge of the image.
Those features are concatenated and an CAE is trained to detect and localize defects.

42
                                     3.3. Semi-supervised learning in Defect Detection


Approach             Methods           Advantages               Drawbacks               Ref.

               - Entire image
                                                                Pre-trained models
               - Patch-wise            High performance
                                                                are required. Lack     [92, 94,
               - Normalizing           for specific datasets.
Density-                                                        of interpretability.     163,
                 Flow (NF)             Some of them need
based                                                           NF-based models          168,
                                       slight computation
               - Knowledge                                      are computationally     169]
                                       time.
                 Distillation (KD)                              expensive.

                                                                Pre-trained models
                                       Able to reproduce
                                                                and extra-training
                                       synthetic     faults.
                                                                data are required.
Classification- - Self-supervised      High performance
                                                                Those methods may [95, 96]
based                                  in         capturing
                                                                not perform well on
                                       semantic      object
                                                                a dataset other than
                                       representation.
                                                                the training one.
               - Encoder-              Intuitive      and
                 Decoder                                        Suffer for low
                                       interpretable                                    [167,
Reconstruction-                                                 performance due to
               - Inpainting            methods.      Only                               170–
based                                                           reconstruction poor
                                       target dataset is                                173]
               - Omni-frequency                                 ability.
                                       used.


 Table 3.1: Summary of the principal semi-supervised learning-based approaches used
 in defect detection


 The main benefit of that approach is that extracted features from CNN’s deep layers
 not only contain local information of the subregion itself, but also global spatial
 characteristics due to large receptive fields.
     The work by Rudolph et al. [94] aims to detect defects estimating density
 distribution from multi-scale image features by Normalizing Flow (NF). NF is able to
 assign a likelihood to each feature vector obtained from a pre-trained model. Anomaly
 score infers the class of new images. Even though the model does not aim to locate
 defects from images, the bijective function of NF obtains pixel-wise localization.
     During the last years, other works have been released with the target of obtain top
 performance in MVTec AD dataset (Section 3.3.1), in both detection and localization
 tasks. The Table 3.2 contains a summary of the most relevant techniques, indicating
 the name of the model, approach, method, a short explanation of how it works and
 AUC value of both detection and segmentation tasks. It is worth mentioning that all
 models’ code are available in GitHub [174] and implementations are in Pytorch [175],
 an open source machine learning framework.

                                                                                       43
    3. S TATE OF THE A RT


                                                                                  Performance
Model             Approach   Method       Explanation
                                                                                  Detect Segment


                                          Mid-level feature representation
                                          patches obtained from feature
                                          extraction backbones are learnt. To
PatchCore [176]      D       Patch-wise   decrease redundancy and inference       99.6   98.4
                                          time, memory bank of patch-level
                                          features is reduced by coreset
                                          subsampling.


                                          Convolutional layers are applied in
                                          NF to maintain relative position
FastFlow [164]       D       NF           information. Vision Transformers        99.4   98.5
                                          as feature extractors learn local and
                                          global feature relations.


                                          Self-supervised block to reconstruct
                                          masked areas by convolutional
DRAEM+                                    layers. Attention module is used
                     R       Inpainting                                           98.9   97.2
SSPCAB [177]                              to calibrate importance of channels.
                                          This method can be integrated in
                                          other SoTA techniques.


                                          Feature maps of different scales
                                          are processed jointly through NF
                                          with convolution blocks.      Multi-
CS-Flow [178]        D       NF                                                   98.7    -
                                          scale approach allows to increase
                                          discriminability between classes and
                                          keep spatial resolution.




    44
                               3.3. Semi-supervised learning in Defect Detection


                                                                                  Performance
Model          Approach   Method       Explanation
                                                                                  Detect Segment


                                       Teacher-Student strategy where
                                       encoder       structure        transfers
Reverse                                knowledge to a decoder.            One-
Distillation      D       KD           class bottleneck embedding module          98.5   97.8
[179]                                  concatenates different levels of
                                       features from pre-trained teacher to
                                       feed into the student.


                                       Frequency Domain module to
                                       obtain low- and high-frequencies.
                                       Interaction of frequencies and
                                       adaptative selection of channel
                          Omni-
OCR-GAN [93]      R                    features are used. Discriminator           98.3    -
                          frequency
                                       to   distinguish    normal      images
                                       from reconstructed and forgery
                                       abnormal images generated by data
                                       augmentation.


                                       Multi-scale     pyramid         pooling
                                       encoder to obtain feature vectors
                                       from different scales.            Also,
CFLOW-AD                               condition vector is generated to
                  D       NF                                                      98.26 98.62
[180]                                  get spatial information from a
                                       positional encoder.       Feature and
                                       condition vectors are fed into a set
                                       of different decoders.


                                       Reconstruction of images with
                          Encoder-     synthetical        anomalies        and
DRAEM [181]       R                                                               98.0   97.3
                          Decoder      discriminant approach to distinguish
                                       those from input images.




                                                                                  45
    3. S TATE OF THE A RT


                                                                                        Performance
Model             Approach    Method         Explanation
                                                                                        Detect Segment


                                             Patch-features from pre-trained
                                             CNN’s activation maps of different
                                             levels are concatenated in patch
                                             embedding vectors and described
PaDiM [182]           D       Patch-wise                                                97.9   97.5
                                             by    a     Gaussian       distribution.
                                             Random dimensionality reduction
                                             keeps performance and decrease
                                             redundant information.


                                             Image-        and          feature-level
                                             coarse alignment for aligning
                                             global     context   and     high-level
                              Entire-
FYD [183]             D                      representations.       Features are        97.7   98.2
                              Image
                                             modelled with Gaussian distribution
                                             and      Mahalanobis   distance       is
                                             calculated to discriminate samples.


                                             Poisson image editing to create
                                             synthetic anomalies from normal
                              Self-          patches.      This method obtains
NSA [165]             C                                                                 97.2   96.3
                              supervised     more realistic flaws, keeping overall
                                             distribution of the replaced image
                                             and avoiding discontinuities.


                                             Data augmentation strategy to attach
                              Self-          local irregular patterns or defective
CutPaste[166]         C                                                                 97.1   96.0
                              supervised     patches in different locations of
                                             normal images.

    Table 3.2: One-class methods in MVTec AD benchmark dataset. D: Density-based,
    C: Classification-based, R: Reconstruction-based. Model performance is evaluated
    using image-level AUC for detection and pixel-level AUC for localization




    46
                                               3.4. Explainable Artificial Intelligence


    Keeping in mind table above, the performance obtained by the SoTA techniques
on benchmark dataset is almost error-free. Nevertheless, although these datasets are
useful to evaluate the performance of different defect detection techniques, they do
not cover all the situations that may occur in industrial OQC: changes in brightness,
displacements, product evolution, subtle defects, etc. Therefore, it is necessary to
analyse if those methods are able to maintain satisfactory results in a real industrial
dataset. Furthermore, those methods have been developed offline, and so, their
performance has not been tested under operational conditions. Thus, performance of
those methods in an industrial environment is yet to be demonstrated.



3.4     Explainable Artificial Intelligence
Today, deep learning-based methods are used, with excellent performance, on a wide
range of problems where large amounts of data are available. However, these results
are achieved using sophisticated architectures based on the combination of several
learning algorithms with millions of parameters. These architectures demonstrate lack
of transparency that does not allow a suitable interpretation and explanation of results
[184]. Consequently, they are called black-box models because of their opacity to
humans.
    The use of deep learning-based models for critical tasks such as medical diagnosis
raises a social/ethical/legal debate, as the techniques are not interpretable and the
decision sources are not known, limiting model information to a simple class or
probability prediction [185]. In this context, explainability has emerged as a topic of
interest in AI research, giving rise to the well-established term explainable AI (XAI)
[186]. XAI refers to the research subfield of AI that aims to make AI systems more
understandable to humans [187], while maintaining their high learning performance
[186]. In fact, the latter part is of fundamental concern, as there is a trade-off
between model interpretability and performance [188]. Low complexity models,
such as linear models and decision trees, are easily interpretable compared to deep
neural networks, but their performance is significantly worse. Therefore, the aim
is to become those high performance architectures more interpretable. In addition,
improvements in the understanding of models may lead to performance-related gains
[185]: detecting impartialities in decision-making due to biased training dataset,
highlighting adversarial perturbations that may compromise model robustness, limiting
model inference to meaningful variables only, etc.
    To the best of our knowledge, Barredo Arrieta et al. [185] present the most
extensive survey of explainability in ML. They define terminology revolving around

                                                                                     47
3. S TATE OF THE A RT


XAI and propose a novel definition for XAI according to the fact that explainability
aims to convert a non-interpretable model into understandable one. Definition is as
follows: Given an audience, an explainable AI is one that produces details or reasons
to make its functioning clear or easy to understand. Following that definition, they
propose a taxonomy based on interpretability of each technique, differencing two main
branches: intrinsic or transparent models and models with post-hoc explainability.
On the one branch, intrinsic models are those that are self-understandable, mainly
owing to their straightforward architecture. Of that type are linear/logistic regression,
k-NN or decision trees, as they do not require external tools for interpretation. On
the other branch, most models consist of complex structures that hinder their correct
understanding. Therefore, after model building, an extended step is required to find
out explainability. That post-hoc step is carried out by explainability techniques that 1)
can be applied to any model regardless its architecture or processing (model-agnostic)
or 2) are specifically designed to explain certain models (model-specific). Both cases
aim for explainability by simplifying models, obtaining relevant features, analysing
results partially/locally or visualising them.
      Regarding to CNNs in computer vision problems, humans are unable to track the
operations performed on thousands of convolutional and pooling layers. Essentially,
those stacked layers extract complex image features to learn image representations,
but in most cases these features are difficult to understand. Therefore, just like others
DL-based models, CNNs also require external tools to become explainable.
      In the case of CNNs, research focuses on explainability of both model decision
and model architecture [185, 189]. Ibrahim et al. [189] present a taxonomy for those
two approaches based on the nature of the techniques. Decision-based explainability
is obtained applying backpropagation in order to identify those input pixels that
are meaningful for the model, identifying its relevance for layers of different levels
(layer-wise relevance propagation [190]) . Therefore, it is possible to segment parts
of the image that most influence model decisions. In contrast, architecture-based
explainability dives into network functioning and attempts to understand how interme-
diate layers extract information from images. Following the idea presented by Ibrahim
et al., CNN explainable models are divided in four categories:

■    Architecture modification: CNN architecture is modified in order to improve their
     interpretability. Replacement of intermediate layers or loss functions [191], addition
     of attention layers [192, 193] or capsule networks [194] are among the techniques.

■    Architecture simplification: network image features are summarized in human-
     understandable rules, enhancing influence of the most relevant ones and eliminating

48
                                                3.4. Explainable Artificial Intelligence


    those that are redundant. Decision-tree [195] or explanatory graph [196] decompose
    feature representation from convolutional layers into meaningful object parts.

■   Feature relevance: ranking of features according to their influence on CNN output.
    The aim is to identify and limit the use of features to those most relevant [197].

■   Visual explanation: networks generate heatmaps in order to highlight relevant image
    pixels. In addition, as human capacities are benefited by visual data, explainability
    increased using visual explanations [185]. In such techniques is common to leverage
    gradient information to extract the importance of each neuron on the network output
    [198] and generate class activation maps to locate discriminative image regions
    that influence model decision [199, 200].


3.4.1 Explainability in defect detection
Over last few years, anomaly or defect detection models have also improved their
performance significantly, although at the cost of complex architectures that are poorly
interpretable [201]. This has led to some mistrust in the use of these models in real-
world problems for which they have been designed [202]. However, current trend
in anomaly detectors is shifting to explainable models [203]. The focus is currently
not only on model performance and detection accuracy, but also on algorithms that
are able to explain the predictions to end-users with the purpose of increase their
confidence [202].
     Li et al. [204] define explainability in anomaly detection as the extraction of
relevant knowledge from an anomaly detection model concerning relationships either
contained in data or learned by the model. This definition suggests that anomalies
can be explained by expert knowledge of the data or by interpretation of anomaly
detection models, so different approaches may be applied. Specifically, in image
defect detection, explainability is mainly achieved by checking model processing of
normal images [205].
     In fact, interpretability and explainability of defect detection models represent one
of the main problems concerning the adoption of AI in industrial environments [206].
In particular, Predictive Maintenance (PdM) is especially focused on the development
of explainable models. PdM [207] aims to anticipate machinery requirements and
optimise maintenance tasks by monitoring and analysing data from machine sensors.
In this way, explainable models help us to understand differences between theoretical
knowledge, e.g. point in time when a machine will fail according to the manufacturer,
and their predictions. Those differences may sometimes be significant, since the latter
do take into account tolerances, variations in environmental conditions or assembly

                                                                                         49
3. S TATE OF THE A RT


settings, among other factors [208]. While research is widely carried out on PdM,
casting defect detection has not addressed thoroughly explainability research with an
industrial approach yet.
     Hu et al. [209] present a CNN-based approach for detection and visualization
of casting defects. They propose an attention mechanism relying on moving image
analysis from a shallow task to a more complex one. A first model is trained using
images of different types of castings to capture object-level features. Subsequently,
a second model employs both features extracted by first model and a dataset with
defective and non-defective castings to obtain discriminative features. They use
bilinear pooling [210] to improve feature representation for subtle defects. Besides,
they adapt class activation maps to bilinear architecture in order to visualize relationships
between image parts and model predictions. They conclude that the proposed method
is suitable for real-time detection because of its efficiency, accuracy and explanation
for decision making.
     Visualisation-based explainability methods allow accurate localisation of defects.
Lee et al. [211] apply different explainability methods to understand the results of
a VGGNet-based anomaly detector. By visualising defects, they determine that
guided backpropagation [212] and some layer-wise relevance propagation [190]
configurations are the most appropriate XAI methods for identifying defects. Besides,
in order to extend the explanation, they train a decision tree that adjusts human-
interpretable rules and helps domain expert decision making.
     Zavrtanik et al. [167] propose an reconstruction-based method that builds an
anomaly score map for defect visualization using pixel-wise differences between input
and reconstructed image. They claim that many DL-based models learn the image
representations so thoroughly that they are capable of reconstructing the anomalies
faithfully, causing difficulties in detection. Therefore, their model is trained by
randomly removing portions of the image and reconstructing it from partial inpaintings.
Anomaly score map is built from several reconstructions of the same image.
     However, reconstruction methods without additional training data are worse at
visualising defects than other methods because of the poor reconstruction capability
of the generator [93]. Therefore, alternative methods leverage feature extraction
performed for defective and non-defective image classification to upscale feature maps
to input resolution, and thus localise the defects [164, 176].
     For example, Pang et al. [213] examine the effectiveness of their inspection
model on real-world manufacturing quality control images by using a few labelled
anomaly examples to train sample-efficient discriminative detection models. They use
a gradient-based backpropagation method to construct a saliency map showing the

50
                                                                  3.5. Critical Review


contribution of each pixel to the model decision in order to localise defective region in
those images classified as defective.
    Finally, Liznerski et al. [205] employ an FCN to preserve spatial information
from features in a downsampled anomaly map. They do not use pixel-wise ground-
truth during experiment, so they can not train a deconvolutional layer in order to
upsample the anomaly map. However, based on the idea that receptive fields follow a
Gaussian distribution, they calculate the anomaly value of each pixel with the centre
of its receptive field as mean and an arbitrary standard deviation, which allows the
visualisation of influential pixels in model decision. They conclude that a smaller
receptive field improves accuracy of defect localisation.



3.5     Critical Review
This section presents the insights gathered during review of the state of the art,
summarising extensive research on the collection of different works and techniques
related to casting defect detection. Furthermore, it provides the conclusions that can
be drawn from this literature review.
    In recent years, the evolution of I4.0 and the conversion to an Intelligent Industry
have led to implementation of a large number of sensors and optical systems capable
of increasing the amount of data collected in manufacturing processes. Together with
the advances made in the field of AI, the change in the trend of detecting defects in
castings seems straightforward. Traditional computer vision techniques, widely used
several years ago, have become obsolete because of hand-crafted feature extraction
processes that need expert knowledge and poor performance with subtle defects or
complex datasets.
    Instead, DL techniques mitigate the need for expert knowledge in the extraction of
features from the image, because their extraction process relies on an automatic and
high representational approach. Current SoTA techniques not only obtain satisfactory
performance detecting faulty products, but also they are able to precisely locate defects
throughout the product, so piece’s inspection is easier to carry out by the operator.
    However, the approaches that have been observed are not applicable to a wide
spectrum in the industrial environment. Most work covers a context where labelled
images are available, either at the image level or via ground-truth, and a discriminative
image classification model is applied to infer between defective and non-defective
products. Therefore, those industrial processes that do not have labelled images, which
is common due to the laborious labelling work, can not apply this type of methods to
OQC.

                                                                                      51
3. S TATE OF THE A RT


     Moreover, in the case of manufacturing processes in general, and castings in
particular, many of the samples are from non-defective or reliable products, and
defective ones appear rarely. Therefore, datasets used in those problems are unbalanced.
Some works that aim to solve this problem [108, 142] have been mentioned, but the
proportions of defective products in each line can vary the performance of those
techniques.
     Semi-supervised approaches tend to broaden application spectrum as they do not
require labelled images, which can facilitate implementation in industrial environments
[152, 153]. However, there are few works addressing this type of methods in real
OQC processes, and most of them are reduced to ranking performance on benchmark
datasets, limiting knowledge to academic domain.
     Furthermore, transfer learning is applied in this type of methods in order to avoid
building models from scratch, exploiting those sophisticated models that have been
trained on huge datasets and require a lot of computational time. However, to the
best of our knowledge, we do not observe widespread research on knowledge transfer
between images with low variance, such as products from different production lines.
     In this context where there is no single method to deal with casting OQC, it
is vital to build an agnostic methodology adaptable to each specific case and that
takes into account all the requirements and limitations of the company. To the best
of our knowledge, there is no similar methodology for a real industrial production
environment. This methodology aims to present a specific solution based on the
context of the issue. Moreover, it should cover a wide variety of scenarios within
casting OQC, providing a taxonomy of casting defect detection problems: target of the
problem (classification and/or localization), image type (X-ray or surface), number of
images, availability of labelling (without labels, image-level labelling, ground-truth),
etc. Finally, this methodology will make it possible to find the best solution according
to the conditions of the problem.
     Additionally, companies are sometimes reluctant to implement in their systems
non-human-understandable methods to perform critical or costly tasks. This is a
particular issue because the high performance of SoTA techniques is dependent on a
complex architecture that the operator is unable to understand. Hence, the role of XAI
in understanding how those approaches work is critical.
     Some works have tried to reduce the complexity of defect detection models
by applying visualisation-based explainability methods to identify image parts that
influence model decisions, as well as to provide defects localisation [209, 211].
Nevertheless, those methods are problem-specific, so their explainability in different
contexts remains unexplored.

52
                                                                3.5. Critical Review


   All in all, we consider that casting defect detection demands a methodology able to
be applied whatever the limitations of each case are, providing adequate performance
and transparency in the comprehension of the methods.




                                                                                   53
                                                                           Chapter 4


         Objectives and hypotheses

This chapter presents the hypotheses and objectives established in order to carry out
the research work. For the purpose of addressing the aim of the research, motivation
presented in Section 1.3 and the gap of the state of the art in Section 3.5 have been
taken into account.



4.1       Hypotheses
This thesis attempts to demonstrate that


      SoTA computer vision and explanation techniques can improve the
performance of OQC processes in manufacturing industry in production line
agnostic environments.



      In order to set the foundations on which the above hypothesis will be developed,
the following additional hypotheses are proposed:


■    H1: AI computer vision techniques are diverse enough to tackle casting OQC
     processes in manufacturing industry.

■    H2: XAI techniques might be used to perform the diagnosis of casting OQC
     processes.

■    H3: proposed methodology can be adjusted for several use cases due to its flexibility,
     adapting itself according to individual needs.



54
                                                                        4.2. Objectives


4.2      Objectives
The main goal of this thesis is to


     design and implement an adaptable methodology based on computer vision
and explainable AI techniques capable of improving current industrial casting
OQC processes.


     Next, the main objective is divided into specific objectives, which are necessary to
demonstrate the hypotheses described above:

■   O1: design and implement a computer vision-based method to improve casting
    OQC processes in manufacturing industry.

■   O2: conduct the diagnosis of defects detected in control processes by means of
    explainability techniques.

■   O3: implement a cross-line methodology capable of transferring knowledge from a
    specific line to resource-poor lines.

■   O4: demonstrate that proposed methodology is problem agnostic and modular, so
    it could obtain high performance in several use cases.




                                                                                      55
                                                                         Chapter 5


Research methodology and work
            plan

This chapter includes research methodology utilised throughout research work. It
also presents the work plan, publication plan and possible research internships of the
thesis.


5.1 Research and data methodologies
The methodology followed in this work has a bipartite structure: research methodology
and data analysis project methodology. Former refers to the way research is designed
in order to reach research objectives and it answers the question how. It is systematised
and can be invariable or modular, i.e. it adapts to the requirements of the research.
Latter is limited to data analysis projects and provides a reference for the procedure to
be used. This research work is based on the union of these two methodologies.
     Firstly, empirical research is chosen as the research methodology. The aim of
this methodology is to obtain knowledge based on scientific experimentation or
evidences [214]. Empirical research is also successful to learn in which background a
method performs properly or to choose best method in a concrete situation. In this
work, several use cases and experiments are carried out in order to prove proposed
hypothesis. Therefore, quantitative empirical research is employed, as methods for
gathering information use statistics and numerical data.
     Furthermore, specific part of data analysis is based on CRISP-DM data mining
methodology [5]. We use this methodology as it is useful to differentiate and relate
phases of a data analysis project and allows for a straightforward distribution of the
tasks in the respective phases. Moreover, it is currently the most widely-used analytic
methodology and it has stood the test of time, since the first version of CRISP-DM
is introduced in 1999 [215]. Finally, CRISP-DM methodology is extended with the

56
                                               5.1. Research and data methodologies


strategies discussed in Section 3.2. This mixture narrows down an effective and
general data analysis strategy for the specific problem of defect detection.
    The steps on which this methodology is based are shown in Figure 5.1 and are the
following:

1. Understand the problem we deal with and define it. We must know fundamental
   aspects of the problem and current research situation on the topic. It helps to find
   out a suitable approach to use.

2. Know available data. A preliminary study of the data helps to obtain initial
   information and assess quality of the data. In addition, it is important to know data
   sources, in order to deal with erroneously or incompletely collected data. This step
   is directly related to the step above, as chosen approach may be data-specific and
   not conform to available data. Ideal approach must fit data adequately.

3. Prepare data and model it. Raw data is processed to obtain the final dataset,
   with which model training is carried out. This task strongly influences model
   performance, so it takes up a large part of the project. Subsequently, expert
   knowledge is required to build a model that accurately solves given problem. In
   this phase, several techniques are selected and compared in order to keep optimal
   solution. Choice of the comparison criteria is a key part of this stage. As any of
   techniques may require specific data structure, model training is closely related to
   data preparation.

4. Evaluate model quality. Once the optimal model is available and prior to
   deployment, final evaluation is necessary to ensure correct performance of the
   model. At this stage, we simulate as realistically as possible situations that
   may occur once the model is deployed. Therefore, we can observe to what
   extent it meets business demands and whether it satisfactorily solves the business
   problem. Ultimately, evaluation process allows anticipating issues that may occur
   in production environment and solve them beforehand, thus avoiding costs that
   would arise if they were to occur once the solution is deployed.

5. Solution deployment. In this stage, model is ready to contribute its knowledge
   to the needs of business. This knowledge is applied integrating the model in
   production environment. Consequently, some aspects of the deployment must be
   chosen, for example location and periodicity. However, obtained knowledge can
   also be used to improve transversal aspects of the business, such as organisation or
   business strategy.

                                                                                     57
5. R ESEARCH METHODOLOGY AND WORK PLAN




                        Business                     Data
                      understanding              understanding


                                                            Data
                                                         preparation
                 Deployment


                                                          Modelling
                                       Data

                                      Evaluation




                     Figure 5.1: CRISP-DM reference model [5]


     We believe that the methodology in this research is suitable for completing the
research objective. This methodology allows us to demonstrate through experiments
the hypotheses formulated and identify under which conditions best results are
obtained. In addition, using a mixed methodology that combines research methodology
with a methodology focused on data analysis projects provides the necessary specificity
to perform the experiments successfully.


5.2 Work plan
This section presents the work plan to be carried out throughout the thesis. Figure
5.2 shows conducting tasks for three years, dividing them in six work packages: from
the problem statement and background, researching different fields to identify the
contribution of knowledge and publishing several research papers, and concluding
with the writing and defence of the thesis.
     The purpose of dividing the work plan into six work packages is to clearly identify
all the steps involved in the completion of the research project. In addition, Figure 5.2
allows to visualize when each task has been performed. It is worth mentioning that
the work packages do not follow a sequential order and they overlap each other, so
that a work package is taken up months after its last interaction to start a new phase.


58
                                                                                            Research plan (Dec-21)
                                                                                2022                                    2023                                   2024
                                                   Duration        Q1     Q2           Q3     Q4          Q1       Q2          Q3     Q4        Q1        Q2          Q3     Q4
                                                              M1 M2 M3 M4 M5 M6 M7 M8 M9 M10 M11 M12   M1 M2 M3 M4 M5 M6 M7 M8 M9 M10 M11 M12 M1 M2 M3 M4 M5 M6 M7 M8 M9 M10 M11 M12
 WP1 - Motivation and research background
 1.1 Thesis motivation                                1
 1.2 Research question                                1
 1.3 Theoretical Background                           2
 WP2 - State of the art review in casting defect detection
 2.1 Supervised learning                              6
 2.2 Semi-supervised learning                         8
 2.3 Transfer learning                                8
 2.4 Explainable Artificial Intelligence              8
 2.5 Critical review                                  3
 WP3 - Supervised casting defect detection for an industrial enviroment
 3.1 Experimental definition                          1
 3.2 Data collection and processing                   2
 3.3 Model development and evaluation                 5
 3.4 Critical analysis                                3
 WP4 - Benchmarking of semi-supervised approaches and explainable model development
 4.1 Experimental definition                          6
 4.2 Data collection                                  6
 4.3 Model development and evaluation                 10
 4.4 Critical analysis                                6
 WP5 - Visual inspection methodology for casting quality control
 5.1 Supervised-based solutions                       3
 5.2 Semi-supervised-based solutions                  6
 5.3 X-ray images                                     3
 5.4 Surface images                                   3
 5.5 Transfer knowledge across lines                  4
 5.6 Explainability for defect localization           3
 5.7 Defect clustering                                3
 WP6 - Research dissemination
 6.1 Research project writing and defence             6
 6.2 Papers writing                                   17
 6.3 Research internship                              6
 6.4 Thesis writing and defence                       6



Figure 5.2: Work plan is represented by a Gantt diagram and covers all three years of the thesis. It is divided in six work packages and each package contains several tasks.
Estimated duration for each task is shown in months and is highlighted with the colour of its respective work package. Submissions are identified by a diamond
5. R ESEARCH METHODOLOGY AND WORK PLAN


      Each of the tasks shown in Figure 5.2 is briefly outlined below:


■    Work Package 1 (WP1): in this work package a first approximation to the problem
     is proposed, defining the context and motivation. We focus on figuring out
     fundamental aspects of the problem we deal with. It includes a review of the
     theoretical background needed to understand the methods applied in the research
     topic. In fact, knowing the context in which methods techniques arise facilitates the
     establishment of new ideas. Finally, it also covers cross-cutting processes involved
     in the research, such as an overview of industrial quality controls.

■    Work Package 2 (WP2): this work package focuses on gathering current knowledge
     relating to casting defect detection. This helps us to understand current research
     trends, learn about published work and identify the strengths and weaknesses
     of previous works. This review is divided into supervised and semi-supervised
     learning, with the aim of comparing both approaches. In addition, works on transfer
     learning and XAI are reviewed, in order to find out about efficient and explainable
     methods. This work package is worked on at several periods during the thesis to
     keep up to date with the latest advances in the field. It is intended to publish a
     survey containing the research conducted in this work package.

■    Work Package 3 (WP3): in this stage, we work on a computer vision-based
     methodology for OQC in casting images using supervised DL methods. A real
     industrial problem is presented and a methodological approach based on the
     research of the state of the art is proposed to find a solution. This research work
     is compiled and submitted to a conference. It presents a comparative study on
     different DL methods for defect detection in castings, using a methodology based
     on analyses images by patches. This experiment can be viewed as working on a
     specific instance of the methodology to be developed in WP5.

■    Work Package 4 (WP4): this work package is organised in two phases. In first
     phase, we present an approximation of semi-supervised learning in defect detection.
     The aim is to propose a semi-supervised method and evaluate its performance
     on a benchmark dataset. This first phase ends with the presentation of the work
     at a conference. In second phase, we work on an explainable and transferable
     method for defect detection in surface casting images, as an instance of proposed
     methodology. In this case, the aim is to facilitate final decision of the operator by
     using a method that is easy to understand. Completed work is submitted to a high
     impact conference.

60
                                                                                5.3. Publication plan


 ■      Work Package 5 (WP5): Last but not least, we propose a modular visual inspection
        methodology applicable to manufacturing casting OQC processes. This methodo-
        logy must adapt to the needs of the company, considering different approaches
        depending on its resources: labelled or unlabelled images, X-ray or surface images,
        and so on. Use of XAI techniques in order to conduct defects diagnosis and
        facilitate operator final decision. Moreover, proposed methodology must transfer
        knowledge from a specific production line to resource-poor lines. The aim is to
        publish this work in a high impact journal.


 ■      Work Package 6 (WP6): this work package gathers all the work done within the
        three year of the thesis. It serves to identify when we are writing a research paper
        or conducting a research internship.




 5.3         Publication plan

 Publication plan of the thesis is shown in Table 5.1. Each of the publication is identified
 with a number, a description of paper, journal or conference in which it is published,
 journal and conference rating and its current status.


Number                Description                       Journal/Conference            Rating     Status
           A comparative study of patch-based
   I       deep learning approaches in casting     VISIGRAPP 2022 conference          Core B   Submitted
           defect detection
                                                 2023 IEEE International Workshop
           Semi-supervised approach for
                                                       of Electronics, Control,
  II       defect detection in benchmark                                              Core B   Submitted
                                                  Measurement, Signals and their
           dataset
                                                    application to Mechatronics
           A survey of deep learning methods
           in casting defect detection.
  III                                                ACM Computing Surveys             Q1      In process
           Discussing trends and examining
           research gaps
           A modular methodology for
                                                       Journal of Intelligent
  IV       manufacturing visual inspection of                                          Q1      Not started
                                                          Manufacturing
           castings
           An explainable method based on
                                                   Computer Vision and Pattern
  V        deep learning for defect detection                                          A++     Not started
                                                       Recognition 2025
           in surface casting images

                            Table 5.1: Publication plan for the thesis



                                                                                                    61
5. R ESEARCH METHODOLOGY AND WORK PLAN


5.4 Research internship
As shown in Figure 5.2 we aim to carry out an international stay in a research centre
with expertise in the topic of the research work, in order to receive feedback and
suggestions for future work. From review of the state of the art, we have identified
two leading teams in the detection of defects in manufacturing products using semi-
supervised approaches.
      Information on the two research teams is presented below:

■    Visual Cognitive Systems Laboratory, Faculty of Computer and Information Science
     at University of Ljubljana, Slovenia. This group has presented leading work in the
     detection of surface defects in manufacturing products using a semi-supervised
     approach [167, 181]. Dr Danijel Skočaj [216], as the head of the laboratory, and Dr
     Matej Kristan [217], as vice chair of Department of Artificial Intelligence, form
     the team for research in the field they call "Visual anomaly detection". Several
     pre-doctoral researches are working on this topic. From this stay we would be
     interested in learning about the technologies they use for defect detection using
     only non-defective images.

■    Explainable Machine Learning Tübingen, Faculty of Science at University of
     Tübingen, Germany. Zeynep Akata [218] is the principal investigator. Group
     research topics are focused on XAI, zero-shot learning and multimodal learning.
     Some researches have also dealt with defect detection in manufacturing images
     [176]. For this stay our aim would be to delve into methods of explainability for
     model understanding in order to transfer the knowledge to vision defect detection
     methods.




62
                                                                         Chapter 6


                 Preliminary Results

This chapter describes conducted work during the first year of the thesis. Each use case
consists of a concise introduction and a presentation of the dataset used. Subsequently,
the experimental procedure is explained and preliminary results are presented. Each
use case is concluded with a final discussion.


6.1     Use case I: supervised learning approaches for
        Defect Detection in Castings

6.1.1 Introduction

Due to the globalization of markets, some companies are addressing their strategy to
specialization and qualification, i.e., they are focused on offering superior quality and
highly reliable goods. In addition, complex manufacture of products implies that many
factors interact with each other in the production line, thus making the manufacturing
process technically complex. In this context, industrial quality control processes play
an important role throughout the production chain.
    As it is compulsory to deliver non-defective products to the customers, companies
devote many resources keeping efficient and reliable quality control processes. As
shown in Section 2.3.1, visual-based or optical inspection is typically used as the
standard quality control process. The aim of this strategy is to inspect products in a
visual non-destructive manner that does not affect the final composition and shape of
the product. This analysis allows to identify those products that differ from quality
standards.
    Casting pieces are considered essential components in some industrial sectors
(aeronautics [219], pipelines weld [220, 221], automotive industry [76, 149]). Many
of the castings employed on those sectors encompass safety, performance or structural
functions. Therefore, final product must possess some specific characteristics and must

                                                                                      63
6. P RELIMINARY R ESULTS


be identical with the product standards, in order to guarantee security and efficiency
to the customers. In summary, all subtle difference regarding the expected piece must
be detected and analysed.
     In castings case, an appreciable part of the habitual defects are usually within the
piece and they are undetectable to the human eye. Thus, X-ray testing is used for the
inspection of casting products [76, 112, 149]. X-ray testing or DR is one of the most
widely accepted methods for non-destructive OQC, as it can detect non-surface or
masked defects without destroying the final product.
     This first use case of the thesis is focused on aluminium alloy castings defect
detection problem in automotive sector. In this case, the aim is to identify X-ray images
as defective or non-defective using casting images from different views. The images
have been collected by Fagor Ederlan and belong to automotive product manufactured
at Bergara plant.
     Manufacturing process of the pieces is carried out entirely at Bergara plant. First,
aluminium is melted in smelting furnaces. Afterwards, aluminium is alloyed with
specifics components and obtained alloy is degassed in order to remove foreign bodies.
Once the liquid metal is prepared, it is putted into the mould and pieces are moulded.
Right after, castings must be cooled by the immersion in water tanks. Finally, each
casting is marked with identifying code and leftover materials are removed. In order to
complete the process, castings go through OQC using an X-ray model. Our research
is focused on the last step of the manufacturing process.
     Nowadays, Fagor Ederlan utilizes a traditional computer vision model to identify
whether an image contains defects or not, and marks the image where defect is located.
Current model obtains satisfactory results identifying defects. Nevertheless, this model
tends to be too conservative: the model classifies many images as defectives, that is,
products that have no defects are classifying as defectives. This condition is due to
companies’ responsibility for not supplying any defective product. To a certain extent,
cost related to misclassifying some non-defective products is lower than misclassifying
defective one. Therefore, in order to reduce costs and increase safety, this kind of
models are biassed in favour of classifying large proportion of pieces as defects.
     For this reason, many false defects are appearing in the final inventory. Furthermore,
some false defects appear systematically: slight casting displacements, dirt in X-ray
camera, etc. This all means that raw material and energy used in the fabrication of
those pieces turn into added cost since those false defect products are not delivered to
the final customer. This situation also entails substantial losses for companies.
     Following the criteria of the X-ray model, the goal is to create a DL-based model
to decrease the number of false defects, keeping the amount of misclassified real

64
   6.1. Use case I: supervised learning approaches for Defect Detection in Castings


defect low. Therefore, DL-based model uses X-ray model’s output and X-ray images
as input in order to classify images into defective or non-defective.
    As the casting images are labelled, the problem is summed up in a binary
classification problem using a supervised learning-based approach. That is, DL-
based model outputs the probability that a sample is defective or non-defective. For
this purpose, model attempts to locate boundaries between defective and non-defective
classes throughout the training phase.
    As the casting images are labelled, the problem is summed up in a binary
classification problem using a supervised learning-based approach. That is, DL-
based model is trained using samples from both defective and non-defective classes
and attempts to learn boundaries that separate their features. In the inference stage, it
uses that knowledge to discriminate new images in both classes.




6.1.2 Description of the dataset

The images provided by Fagor Ederlan belong to a specific automotive part produced
at its plant in Bergara. Eight pictures of each product are taken from eight different
points of view. Aforementioned images are divided into two datasets, according to the
date on which the pieces have been produced: the former was gathered in 2021 and
the latter has been gathered in 2022.
    Images in the datasets are classified as NOK and FNOK. The images labelled
as NOK contains one or more defect that are identified by the X-ray model and
supported by experts. Those products cannot be distributed to the customer, so they
are discarded or reused to produce new pieces. Otherwise, FNOK images, also known
as false defective, are those which X-ray model classifies as defective but, after an
analysis carried out by X-ray expert operators, are found to be correct or non-defective
(see Figure 6.1). Both classes have ground truth to identify defects or false defects
throughout the image.
    The size of all X-ray images in the datasets is 1024×1024 pixels. Furthermore,
X-ray images are greyscale or achromatic, so they have only one channel and each
pixel is an integer between zero and two hundred and fifty five. The dataset gathered
in 2021 consists of 4708 images, of which 3336 are from defective products and 1372
are from false defective products. Furthermore, the dataset gathered in 2022 consists
of 3099 images, of which 1777 are from defective products and 1322 are from false
defective products.

                                                                                      65
   6. P RELIMINARY R ESULTS


                                                             Customer
                                         OK
                                                                                  Save
                       X-ray                                                    reference
Casting
                       model                                      FNOK

                                     NOK                 X-ray                   Discard/
                                                         expert       NOK        Reutilize



    Figure 6.1: Workflow diagram of casting product OQC process in Fagor Ederlan


   6.1.3 Experimental procedure
   6.1.3.1 Image preprocessing

   The preprocessing of the X-ray images has been performed following the sliding-
   window approach presented in Section 3.2.3, adapting it to fit the needs of this case.
   Since defects of the pieces are small in comparison with the size of images, model is
   trained over smaller patches, and thus, it obtain local context from different parts of
   the image.
        The preprocess strategy is presented in pseudocode Algorithm 6.1 (visual explan-
   ation is shown in Figure 6.2) and has been carried out as follows:

   1. Original image is cropped to isolate and extract defects throughout the image (line
        5). Defects and false defects are marked with a distinguishing colour in the ground
        truth.

   2. Sub-image is divided into patches; size of the patches can be changed using
        patch_size parameter (line 6). Background patches, i.e, those that are white, are
        eliminated from the set of patches (line 7) .

   3. If processed image belongs to a False Defective product, obtained patches will
        not include any defect, although X-ray model expresses the opposite. Therefore,
        those patches are used as OK samples to train the model. Conversely, those patches
        related to defective products are labelled as NOK (lines 8-13).

   4. Finally, cropped image is subtracted from original. This process returns the
        complementary image of the cropped one, so no defects appear in it. In the
        same way, the original image can be used after the defects have been masked out.
        Step 2 is repeated to obtain OK valid patches.


   66
   6.1. Use case I: supervised learning approaches for Defect Detection in Castings


                                                                              NOK patches

Defective image with ground-truth               Original image




                                    Bring
                                    back




                               OK patches




Figure 6.2: Patch-level pre-processing strategy for defective images. The process is
similar for false defective images, except that patches obtained from ground-truth are
classified as OK patches.


    Image preprocessing is the key to this problem. This process must be reliable
and accurate, as obtaining adequate patches is decisive for a successful training
performance of the models. An error in gathering patches means not making the most
of DL-based models.


6.1.3.2 Model architecture

This experiment carries out a comparative study between SoTA DL-based computer
vision architectures in a casting defect detection problem. The key idea is to leverage
those architectures which obtain good performance in image classification problems
and transfer that potential to specific task.
    Selected architectures for this experiment are ResNet, EfficientNet, ConvNeXt
and Vision-Transformers. All of them obtain good results in the ImageNet Large
Scale Visual Recognition Challenge [222]. Nevertheless, ImageNet is formed by real
life situation images, so it differs significantly from our dataset. We want to verify if
architectures’ good performance keeps in casting images evaluation.
    As the casting images are made up of one channel, selected model’s architecture
must be restructured from three channels to one. Moreover, those architectures utilize
images of 224 × 224 pixels as input images. Thus, as the obtained patches are smaller

                                                                                      67
6. P RELIMINARY R ESULTS


      Input: images: Images of castings from Fagor Ederlan dataset, Patch size
      Parameter: patch_size: Integer indicating the size of patches
      Output:

      - patches_NOK: patches of size patch_size from defective zones
      - patches_OK: patches of size patch_size from non-defective zones

 1    begin
 2       patches_NOK ←− {}
 3       patches_OK ←− {}
 4       for each image in images do
             /* Obtain ground truth (gt) patches                                 */
 5           cropped_image ←− crop_ground_truth(image)
 6           patches_gt ←− divide_in_patches(cropped_image, patch_size)
 7           patches_gt ←− drop_white_patches(patches_gt)
 8           if label(image) == ‘FNOK’ then
  9               patches_OK ←− append(patches_OK, patches_gt)
 10          end
 11          else if label(image)== ‘NOK’ then
 12               patches_NOK ←− append(patches_NOK, patches_gt)
 13          end
             /* Obtain non defective patches                                     */
 14      non_defect_image ←− substract(image, cropped_image)
15       non_defect_patches ←− divide_in_patches(non_defect_image,
          patch_size)
16       non_defect_patches ←− drop_white_patches(non_defect_patches)
17       patches_OK ←− append(patches_OK, non_defect_patches)
18    end
19 end

Algorithm 6.1: Preprocessing of dataset images to obtain training patches



than input images, we also adapt the size of input images instead of interpolate the
patches. These changes affect along the model, and it does not allow for using the
weights of the neural net trained in ImageNet. In short, models must be built from
scratch.
      Those SoTA architectures are compared with our baseline convolutional-based
model, formed of several convolutional layers, max pooling layers and fully connected
layers. Figure 6.3 shows the proposed architecture for our baseline convolutional
model. In the first half of the net, convolutional layers are combined with max pooling
layers to extract different feature maps from images. The deeper convolutional layer in
the network is, the smaller the feature maps are. Also, whereas shallower layers focus
on edges and colours, deeper layers extract subtler elements from images. The second

68
   6.1. Use case I: supervised learning approaches for Defect Detection in Castings




              Figure 6.3: Proposed architecture of our baseline CNN


half aims to identify relevant information from feature vector using fully connected
layers and outputs a probability of being a non-defective product.


6.1.3.3 Training, validation and test sets

As has been explained in Section 6.1.3.1, two types of patches are obtained from
preprocessing. This strategy is adopted only on the casting dataset gathered in 2021.
Dataset gathered in 2022 is saved untouched for test evaluation.
   In Table 6.1 can be seen the number of patches obtained in preprocessing step.
This patches are used in model training. Since both FNOK and OK classes belong to
non-defective products, if they are summed, patches from defective and non-defective
products will be balanced. We assign the same label for both classes in model training.
Moreover, in the inference phase, models infer the probability that image contains
no defects. Therefore, we encourage models to learn that FNOK and OK patches are
equivalent in essence.

    Date            NOK patches              FNOK patches             OK patches
    2021                 217 322                 17 880                 208 768

Table 6.1: Number of 64 × 64 patches obtained in preprocessing for casting dataset
gathered in 2021

   Patches from preprocessed dataset are randomly divided into train and validation

                                                                                   69
6. P RELIMINARY R ESULTS


sets, %70-%30 respectively. This provides us with a subset of data to avoid model
overfitting.
     In order to evaluate model performance in test dataset, we perform a sliding-
window strategy throughout ground-truth of test images. Models classify images as
defectives or non-defectives according to the predicted classes of all patches.

6.1.4      Results
We carry out several experiments in order to compare four SoTA architectures
performance in castings defect detection. Moreover, we study the importance of
the number of images in the training phase and optimise model performance according
to the patch-size. Finally, we add expert knowledge preprocessing to raw images in
order to improve feature extraction. For selected architectures, Wide Resnet-101-2,
EfficientNet-B7, ConvNeXt-B and ViT-B/16 are the versions of the models.

6.1.4.1 Dataset-size influence

DL-based models require large amounts of observations to be able to recognise
patterns throughout the data. In the case of defect detection, it can happen that only a
small number of images are available, either because the image acquisition process is
complex or because the product is constantly changing. Therefore, it is important that
the model is able to handle a small set of images and can get the most out of them.
     In order to analyse the performance of the architectures based on the number of
available images, the models are trained with different proportions of the dataset: the
full dataset, half of the dataset and 10% of the dataset. It should be noted that the
images are chosen randomly, but maintaining a balance between normal and defective
classes.
     Accuracy results for the three partitions of the dataset are shown in Figure 6.4.
The results confirm literature statement, i.e., deep models perform better the larger
the number of observations available. In this case, the models achieve an increase
in accuracy of 5.146% when increasing the dataset from 10% to 50% and 4% when
increasing from 50% to the entire dataset.
     Moreover, if we consider the models trained with the full dataset, the best results
are obtained with the ConvNeXt, ViT and baseline CNN architectures. Specifically,
based on the accuracy metric, the best performing model is baseline CNN. Furthermore,
as previously discussed, it is critical that no defective parts are passed as correct, so
special attention must be paid to Recall Defective value. In this scenario, ConvNext
and ViT obtain slightly better results than baseline CNN. It is because those two
models are biased: they classify more images as defective than non-defective, and so,

70
   6.1. Use case I: supervised learning approaches for Defect Detection in Castings




    Figure 6.4: Comparison of models’ performance using different dataset sizes:
    10% of the dataset, 50% and whole dataset


their Recall Non-defective values are lows (see Table 6.2). Therefore, baseline CNN
appears to be a more balanced alternative.

                           Accuracy           Recall Defective       Recall FNOK
ConvNeXt                     0.853                    0.926               0.754
ViT                          0.855                    0.921               0.767
baseline CNN                 0.875                    0.902               0.840

Table 6.2: Comparison of the best performing models, trained on the complete image
dataset



6.1.4.2 Patch-size influence

One of the most important points in the sliding-window method is the size of the
window itself, or in other words, the size of the patches to be analysed. Patch-size is
conditioned to image resolution and defect size, so a preliminary study of defects is
highly recommended. Several patch sizes are compared in order to determine which
size blends better with the proposed architectures.
    Figure 6.5 shows the distributions for defect height and width in the dataset. It
can be seen how the majority of the defects are smaller than a 64×64 pixel grid.

                                                                                    71
6. P RELIMINARY R ESULTS




Figure 6.5: Defect height (left) and width (right) distribution in the casting dataset


Nevertheless, another major part of analysed defects are larger than 64×64 pixels, and
some even are almost 448×448 pixels. Therefore, small patch sizes could be accurate
with small defects but disregard global information of large defects. By contrast, large
patches delimit better large defects, but they could blur small ones.
     In order to evaluate the patch-size influence in model performance, we train
proposed models with three different patch-sizes: tiny (32×32 pixels), medium
(64×64 pixels) and large (128×128 pixels). As there are a large number of possible
combinations between models and patch sizes, in this case, we use only accuracy
to determine the best performing models and patch sizes. We carry out a more
comprehensive comparison of the best models in section below.
     As shown in Table 6.3, the accuracy of the models increases when using larger
patches. For all architectures, training with 128×128 pixels patches outputs the best
accuracy values. Furthermore, the difference in accuracy between the small and
medium-sized patches becomes quite noticeable, which may indicate that in many
cases large defects are not being correctly detected. This can happen because smaller
patches are not able to capture all the defect information in a single patch, so the
knowledge is dissipated.


                                    Tiny              Medium                Large
Wide-ResNet                        0.663                0.697               0.834
EfficientNet                       0.651                0.754               0.821
ConvNeXt                           0.791                0.853               0.866
ViT                                0.749                0.855               0.895
baseline CNN                       0.789                0.875               0.884

Table 6.3: Detection accuracy of proposed models trained with different patch-sizes

72
   6.1. Use case I: supervised learning approaches for Defect Detection in Castings


    From this experiment, we identify ViT and baseline CNN as the most accuracy
models, so they are compared more thoroughly with their counterparts trained with
pre-processed patches in section below.



6.1.4.3 Expert knowledge influence

Deep models are able to automatically extract features from raw data and build a
discriminator in order to classify the different classes. However, it may sometimes be
desirable to apply a preprocessing step to increase the value of the data.
    Additionally, the responsibility of detecting each and every defective part throughout
the production process is a complex task that requires experienced and skilled operators.
In most cases where novel automated detection techniques are implemented, the final
check is carried out by expert X-ray imaging operators. Hence, it is interesting to
use the human knowledge of the expert operators to perform a preprocessing of the
images to assist in the training of the deep models.
    In this experiment, a production line specific filter is applied in the preprocessing
of the images. The filter is used as an edge detector, to highlight those areas with
pronounced changes in intensity and texture. This process reduces image noise and
facilitates the feature extraction from the models. For this experiment, the filter is first
applied to the whole image and then the image is divided into different patches using
the same procedure as explained above. The models are trained using the patches of
the filtered images.
    Based on the results of the experiment shown in Table 6.4, we conclude that the
models trained with filter-preprocessed images correctly classify more images from
the test set. This is because they correctly classify more normal product images. In the
case of ViT, the Recall FNOK improves by almost 10% respect the ViT model trained
with no pre-processed images. However, as there is a trade-off between Recall FNOK
and Recall Defective, so when the former increases, the latter decreases. Therefore,
defect detection in these models is lower than in the models trained with raw images.
    This implies that model selection must be made according to the criteria set by the
company. In the case of being required to deliver as many parts as possible regardless
of whether any of them contain defects, the model to choose is ViT trained with
pre-processed patches. Whereas if the aim is to identify as many defective parts as
possible, with the possibility that some non-defective parts may be discarded, the most
appropriate model is baseline CNN with raw patches. For our industrial use case with
Fagor Ederlan, where for safety reasons all defective parts must be detected, the most
suitable model is the latter.

                                                                                         73
6. P RELIMINARY R ESULTS


                                                                              Recall
                          Filter        Accuracy       Recall Defective
                                                                              FNOK
 ViT                         ✗            0.895             0.915              0.868
                            ✓             0.911             0.886              0.945
 baseline CNN                ✗            0.884             0.933              0.818
                            ✓             0.878             0.891              0.862

Table 6.4: Comparison of ViT and baseline CNN architectures trained with raw and
filtered images


6.1.5     Conclusions

The goal of this work is twofold. On the one hand, it aims to study the performance
of several SoTA architectures for defect detection in casting images. On the other
hand, it aims to reduce the proportion of false defectives, thus meeting a need for
Fagor Ederlan. Simultaneously, it attempts to emphasise the high performance of
the patch-based approach, due to the infeasibility of using complete images in many
cases.
     The results show that image classification architectures such as vision-transformers
and some convolutional networks may be used as accurate defect detectors. These
methods, combined with expert operator supervision, may be a reliable solution for
manufacturing OQC. Nevertheless, we have shown that a shallow convolutional
model performs better than its more complex counterpart. This may be because
the architectures used are designed to classify real-world colour images, so using
them to extract information from greyscale images with little semantic information is
over-parametrising the problem.
     We show that the number of images influences the model performance, and
therefore a large dataset is required in order to obtain satisfactory results. Moreover,
we highlight that a previous analysis of defect sizes allow us to obtain better results
by adjusting the size of the patches. We show that large patches retain information
from small defects, whereas the reverse is not always true. At the same time, however,
the sliding-window method restricts the use to the same patch size, which reduces
the learning capacity of the models. Therefore, in cases where the size of defects
has a large variability, object detection architectures are a more suitable alternative.
Finally, we study the influence of adding an image preprocessing step based on expert
knowledge. We conclude by saying that preprocessing is problem-specific and each
filter works for a specific task.
     Based on the experiments and Fagor Ederlan requirements, we conclude that for

74
   6.2. Use case II: defect detection in surface images via semi-supervised methods


our industrial use case the 128 × 128 pixels patch-based baseline CNN is the most
suitable solution for defect detection in casting images, considering fast learning
and inference capability, accuracy and performance compared to the other proposed
models.
    The main limitation of the work reside in the necessity of labelled images for
supervised learning training. New defect types in the production line will not be
detected by the models, which is a major risk for the company. We believe that
semi-supervised learning approach, training only with normal images and differentiate
them from defect images in the testing phase, will avoid this issue.


6.2       Use case II: defect detection in surface images
        via semi-supervised methods
6.2.1 Introduction
Currently, defect or anomaly detection is turning towards semi-supervised learning
approaches where models are trained exclusively on non-defective images, in order to
learn the characteristics of these images and discriminate them from defective ones
in the inference phase. Moreover, in many cases in the literature, defect or anomaly
detection term is used to refer to identification and localisation of defects or anomalies
without prior knowledge of the defectiveness [179].
    It is worth noting that, for a classification problem, semi-supervised approaches
do not perform as well as discriminative approaches of supervised learning when they
are applied on an appropriate and balanced dataset [88]. However, defect detection
problem is not balanced: it is significantly complicated to obtain an industrial dataset
containing samples of defective and non-defective products in similar proportions,
as the latter are difficult to obtain. Therefore, supervised approaches are usually
discarded if a large number of images of both types are not available [89], which is
often the case for manufacturing OQC processes.
    However, semi-supervised approaches for defect detection are not widely esta-
blished in current manufacturing processes. The review in Section 3.2.3 shows that
most of the work on casting defect detection is based on supervised approaches.
Additionally, the works presented in Section 3.3.2 do not encompass an industrial
approach. Hence, this implies that the implementation of a method capable of handling
defect detection problem in an industrial environment using only non-defective images,
and thus avoiding image labelling issue, would mean a contribution to knowledge.
    Furthermore, surface images of products provide another way to conduct visual
inspection in manufacturing OQC [109]. In situations in which external part of the

                                                                                       75
6. P RELIMINARY R ESULTS


product is of particular importance, this approach is more satisfactory than X-ray
imaging, as attention is focused on the surface of the product [113]. In addition,
surface image inspection is carried out when defects are on the superficial side of the
product, since this method does not allow for internal defect detection.
     This use case is focused on developing a defect detection method that performs
satisfactorily on casting surface images. The aim is to provide a framework that can be
used in those manufacturing scenarios where labelled defect images are not available.
Additionally, this method has to deal with the drawbacks that appear in industrial
images and that are not usually reflected in benchmark datasets, such as changes in
brightness or focus.

6.2.2     Description of the dataset
The images used in this experiment have been obtained from a casting product image
dataset available on Kaggle [223]. Those images belong to a single product type and
show the top and bottom sides of a submersible pump impeller in 512 × 512 pixel
size.
     The dataset consist of more than one thousand surface images of defective and
non-defective products. As the aim of this use case is to simulate a situation where
there are no defective products to train the model with, training set contains only
images of non-defective products. Furthermore, model evaluation is carried out on a
test set which contains images of both non-defective and defective products (see Table
6.5).

                            Non-defective images         Defective images
             Training                 332                        -
            Validation                 18                       114
               Test                   86                        667

         Table 6.5: Distribution of non-defective and defective product images
         in training, validation and test sets

     A preliminary analysis of the images reveals that the samples of defective products
show, in some cases, obvious defects, while in others the defects are considerably
subtle. Figure 6.6 compares a non-defective image with a clearly defective image and
with another image that is more challenging to classify. This comparison suggests
that while some of the images may be straightforward to detect by a baseline model,
obtaining a satisfactory result for the full spectrum of defects requires the correct
classification of the more subtle defects, which could require a more sophisticated
model.

76
   6.2. Use case II: defect detection in surface images via semi-supervised methods




Figure 6.6: Representation of some images from the dataset. A non-defective product
(left), an obvious defective product (middle) and a subtle defective product (right)
are shown


6.2.3 Experimental procedure

As discussed in Section 3.3.2, semi-supervised approaches focus on gathering know-
ledge from a target class (in the case of defect detection, the non-defective or normal
class) in training phase, in order to discriminate other classes samples in test phase.
    In particular, it has been observed that the density-based approach achieves
superior results (see Table 3.2). The aim of this approach is to estimate the probability
distribution of the non-defective class, using the features extracted from the image. In
this way, those images that have a lower probability than a normality threshold are
classified as defective.
    Inspired by the success of this approach, we develop a density-based method with
the aim of detecting defective products found in the surface image dataset. The idea
is to provide a model as simple as possible with which we can obtain satisfactory
preliminary results.
    The pipeline of our approach for semi-supervised defect detection is outlined in
Figure 6.7. Firstly, feature extractor backbone extracts relevant features from the
normal images in the training phase. It is worth mentioning that feature extraction
backbone uses the weights trained on ImageNet dataset, and they are not retrained.
In addition, to increase the number of normal images, data augmentation techniques
may be applied. Subsequently, to reduce the dimensionality of the feature vector,
Principal Component Analysis (PCA) is applied. In this way, the dimension of the
feature vector is reduced while maintaining most of the variability of normal data.
Finally, an anomaly detection method is applied and a threshold is defined to indicate
whether the part is defective or not defective. In the testing phase, the same backbone
extracts the image features and the dimensionality is reduced based on the components

                                                                                      77
6. P RELIMINARY R ESULTS


     Training                                                        Test
                                     Normal features                 Test features
      Normal                                                                           Test
                                                        Principal
      images                                           components                     image

                                        Train PCA                   Transform




                                                        Anomaly
                                                        detector
                   Data          EfficientNet
                augmentation                                          Anomaly score
                                                       Threshold
                               Feature extraction
                                                       Anomaly
                                                       detection


Figure 6.7: Overview of our semi-supervised approach for defect detection. This
pipeline consists in three stages: obtaining training images by data augmentation,
extracting features from the images by feature extractor backbones, and finally,
reducing the dimensionality of the feature vector by PCA and image classification by
anomaly detection methods


adjusted in the training phase. Data is passed back through the anomaly detection
method and whether score is below the threshold, image is classified as non-defective.
If it exceeds the threshold, image is classified as defective.
     To this end, the procedure is divided into three fundamental pillars: data augmenta-
tion, feature extraction and anomaly detection algorithms.

6.2.3.1 Data augmentation

One of the main problems of DL approaches is the lack of sufficient training samples
[86]. In the case of computer vision problems, data augmentation deals with this issue
applying a set of transformations (translation, rotation, cropping, etc.) to the original
image, in order to obtain new images but maintaining intrinsic characteristics of the
original. Moreover, data augmentation can also yield faster convergence, as well as
reduce the possibility of overfitting and thus improve generalisation. For some small
datasets, data augmentation has been shown to boost model performance by more than
20% [77].
     Usually, data augmentation is applied only in the training phase, in order to
increase the number of normal images available to train the model. It is decided to
apply only random rotations and translations in order to modify the characteristics
of the normal images as little as possible. The application of a more aggressive data
augmentation could influence the training phase so that the normality distribution of
the images is not correctly estimated.

78
   6.2. Use case II: defect detection in surface images via semi-supervised methods


6.2.3.2 Feature extraction

Several works have shown that density-based approach achieves successful results by
applying transfer learning to feature extraction process [176]. Pre-trained models are
used to extract relevant features from the images of the target dataset. These features
contain an accurate representation of the images and also turn out to be considerably
discriminative between defective and non-defective images. Furthermore, it is not
necessary to estimate the weights, i.e., train the model, that optimise the model
discriminative ability. This considerably reduces model development time. Moreover,
research has shown that these features are difficult to learn from scratch using only
normal samples[163]. Ultimately, these features are used to construct another ML
method to model the distribution of normal images [173].
    Pre-trained models that are used as feature extraction backbones are described in
Section 2.2.1. It is mentioned that these models are trained as image classifiers on a
huge dataset in order to transfer the knowledge to a specific dataset. This is achieved
by removing the final part of the model known as head, where fully connected layers
are located. As a result, the network outputs several feature maps that compile image
representation. However, it is sometimes interesting to extract feature maps from
shallow layers of the network in order to obtain multi-scale information from the
image. These shallower feature maps retain spatial information about the objects and
are therefore useful in defect location problems. This spatial context is obtained from
CNNs, because they have building blocks made up of stacked convolutional layers to
extract different levels of abstraction.
    In this method, we apply EfficientNet-B5 [45] as feature extractor backbone, so
we remove fully connected layers in order to extract feature maps. As our objective is
only defect detection and not defect localisation, we extract features only from the last
layer of the backbone.

6.2.3.3 Anomaly detectors

Once normal image features are extracted by the backbone, we apply anomaly
detection algorithms in order to learn the behaviour of these features, and therefore to
detect those samples that differ from normal behaviour.
    Previous to this step, it is recommended to reduce the dimension of the extracted
features. Following Rippel et al. [163], we apply PCA to obtain the principal
components that best explain the variability of the features. Hence, we solve the
dimensionality problem posed by the high number of features provided by the
backbone. Reducing the dimensionality of the feature vector improves anomaly
detector performance, as the number of features to be analysed is considerably

                                                                                      79
6. P RELIMINARY R ESULTS


decreased. In our case, we reduce the feature vector of length 2048 to twenty seven
principal components, which explains 95% data variability.
      The two methods of anomaly detection used are briefly explained below:

■    PCA reconstruction error: the principal components, which have been obtained
     from features extracted from normal images, are re-transformed to reconstruct
     original features. The reconstruction error between the original features and the
     PCA reconstructed features is calculated and used as a threshold for the test phase.
     Theoretically, if the features are reconstructed well, the variability of the sample is
     still the same as the normal samples, while if the reconstruction is poor, the sample
     does not belong to the normal class.

■    Local Outlier Factor (LOF) [224]: LOF is a density-based outlier detection algorithm
     that assign a degree of outlierness to each sample in the dataset. This assignment
     allows to extend the scope of other outlier detection algorithms that reduce the
     problem to a binary classification. The degree of outlierness is dependent on how
     isolated the sample is with respect to a limited area of surrounding neighbours. we
     must indicate the number of neighbours to be compared with, so that the algorithm
     can identify the distance for which the sample is considered an outlier. A high
     number of neighbours in a small training sample may increase the distances between
     observations, which would decrease model’s outliers detection ability.


6.2.4 Results
We perform several experiments with the aim of finding the combination of techniques
that optimises defect detection for this casting product dataset.
      Firstly, we tune the hyperparameters of anomaly detection algorithms using
a validation set containing non-defective and defective images. Specifically, the
validation set is used to tune the optimal number of neighbours for the LOF method and
is independent of the test set. Moreover, in this first experiment, no data augmentation
is applied, so the number of observations used to apply PCA is the number of original
normal images.
      Figure 6.8 illustrates the trade-off between model’s AUC obtained with LOF
method and number of neighbours used. The results show that model performance
decreases as the number of neighbours used to define the outlier score increases. This
makes sense since, in a small dataset, each cluster is formed by not many observations,
so analysing a large number of neighbours would force to compare observations from
different clusters. This would decrease the influence of neighbours on the score and
converge to the same result for all observations.

80
   6.2. Use case II: defect detection in surface images via semi-supervised methods




           Figure 6.8: Influence of number of neighbours in LOF method



    Based on the validation results, we can indicate that for the number of training
images, two or three is the optimal number of neighbours. We choose three as the
number of neighbours that influence the LOF method. With that number, the AUC
obtained by the model is 0.9927. Besides, the reconstruction error of the PCA does not
need to adjust hyperparameters, so it is trained with the normal images and evaluated
directly on the test set. The result of the model is AUC = 0.9985.

    Furthermore, we hypothesise that increasing the number of normal images can
improve model generalisability and reduce overfitting. By applying data augmentation
techniques, such as rotation and translation, we can obtain new non-defective images
from original ones. In the following experiment, the aim is to show whether the
application of data augmentation can improve defect detection capability of models in
order to obtain an error-free classification. We increase the number of normal training
images by data augmentation and we train the models again. For the LOF method, we
have to retune the hyperparameter that decides the number of neighbours to examine.
In this case, the number of neighbours is increased to seven. Table 6.6 shows the
results of the experiment.

    Generally for all configurations, our method yields outstanding defect detection
performance in this specific dataset. Table 6.6 indicates that the model using the LOF
anomaly detector together with data augmentation obtains the best results. This model
is able to successfully detect all defective products, while correctly identifying all
non-defective products. In short, for this specific dataset, it is an error-free classifier,
as highlighted in the confusion matrix of the Figure 6.9.

                                                                                        81
6. P RELIMINARY R ESULTS


                                                                           Recall
                                        AUC         Recall Defective
                                                                           Normal
  PCA recons.                           0.998             0.997                1
  LOF                                   0.993             0.997              0.988
  PCA recons. + Data aug.               0.982             0.998              0.965
  LOF + Data aug.                         1                 1                  1

Table 6.6: Comparison of different configurations of the proposed method. In case of
applying data augmentation, the training set is increased by a factor of four




         Figure 6.9: Confusion matrix of the test set for the model with LOF
         and data augmentation. All images are correctly classified.


6.2.5 Conclusions

This experiment aims to study the performance of semi-supervised approaches to
defect detection in industrial environments. Specifically, the wide range of cases in the
industrial environment forces to investigate towards methods where image labelling is
not necessary. Currently, the trend is towards the use of semi-supervised approaches
which only employ non-defective images for training.
     In this case, we propose a method capable of detecting defective product images in
an industrial dataset. This dataset is formed by surface images of a single aluminium
product, with defective and non-defective images. The method is based on current
state-of-the-art techniques to extract the discriminative characteristics of the normal
images and the subsequent processing to differentiate images of both types.
     The results obtained by the different models are considerably robust. Both the
PCA reconstruction error and the LOF method perform remarkably well using only

82
   6.2. Use case II: defect detection in surface images via semi-supervised methods


the normal training images. In the case of LOF, the performance can be slightly
improved by adding more training images by means of data augmentation. However,
more test images would be needed to confirm the hypothesis. Moreover, this type of
strategy may be useful in other cases where the dataset is smaller, in order to increase
model generalisation and to avoid overfitting.
    Finally, although these results can suggest a promising performance in the imple-
mentation phase, we recognise that it is necessary to know how the model works in
order to be able to apply it in an industrial environment. This type of black-box models
are not attractive in the industrial environment because the decision-making process is
not intuitive for humans. In situations where the costs of potential losses are high, the
optimal model is one that efficiently balances performance and explainability. Thus, a
model with high performance but opaque operation may be unreliable in industrial
decisions.
    Based on the above, the next logical step in this experiment is to apply XAI
techniques with the aim of finding out the aspects that influence the decision making
of the model. The application of the techniques should focus on the features extracted
from the backbone and the transformation of these features to reduce dimensionality.
This will increase the interpretability of the model and shed light on the outputs of the
model, as well as increase the ability to address possible deficiencies and even locate
the failure in the image itself.




                                                                                      83
                                                                           Chapter 7


                            Conclusions

This chapter summarises work covered throughout the document, from review of the
state of the art and identification of research gap to the results obtained in preliminary
experiments. It is therefore focused on assessing the accomplishment of objectives
and reviewing future research lines.
     During the first year of thesis, research has been focused on defect detection for
casting quality using computer vision. An exhaustive review of the literature has
been carried out in order to discover current state of the art in casting OQC and also
to determine the applicability of semi-supervised techniques in this type of process.
Through this research we have formulated the hypothesis to be demonstrated with this
thesis, as well as objectives and methodology to be fulfilled in order to accomplish
this demonstration.
     The review of the state of the art have shown an evident trend in research on casting
defect detection. Traditional computer vision techniques used for years have been
replaced by novel deep learning methods, removing handcrafted and problem-specific
feature extraction with an automatic and widespread process capable of dealing with
raw data. Deep learning approaches significantly increase performance and accuracy
in defect detection and localisation. Nevertheless, difficulty in dataset labelling
or emergence of new defects on the production line suggests avoiding supervised
approaches and including those that do not require labelled samples. Currently,
however, most work based on semi-supervised approaches is confined to an academic
environment. Finally, it has been observed the importance of explainability methods
for the adoption of deep learning models in industrial environments.
     This literature review has identified the need for a methodology to address casting
defect detection problem based on the requirements and limitations of any OQC
process. Constructing this modular methodology demonstrates the hypothesis on
which this research is based. In addition, several methods for different industrial
contexts can be proposed as instances of this novel methodology.

84
   The methodology used in this research is appropriately adapted to the research
objective: demonstrate through experiments the hypotheses formulated and identify
under which conditions best results are obtained. In addition, combining the research
methodology with a methodology focused on data analysis projects has provided the
specificity needed to conduct the experiments.
   In the experiments section, first use case have been conducted with the aim
of measure deep learning models’ performance in casting defect detection for a
real industrial OQC. Several SoTA architectures for image classification have been
compared, and experiments have demonstrated that CNN model is accurate enough
to deploy in production environment. Experiments have shown that image patch size
influences in model performance. To sum up, for this industrial use case 128 × 128
pixels patch-based baseline CNN is the most suitable solution for defect detection
in X-ray images, considering fast learning and inference capability, accuracy and
performance compared to the other proposed models. A conference paper has been
sent with the results of this use case and experiment.
   In the second use case, we have developed a method for defect detection in
industrial surface images. This case has been aimed at reproducing a situation where
there are no defective product images to train the model. The method has been based
on extracting features from images of non-defective products in order to establish the
distribution of normal features. Thereafter, the dimensionality of the feature vector
has been reduced and anomaly detection algorithms have been applied to discriminate
between defective and non-defective products. Both the PCA reconstruction error and
LOF method obtain an accuracy close to 99%. In addition, if data augmentation is
added, LOF method correctly classifies all evaluation images. A conference paper has
been prepared and sent with the results of this use case and experiment.
   As future lines of the thesis, we strongly believe that it is worth researching the
application of XAI techniques to provide explainability to the proposed methods. This
will provide added value to developed solutions, offering accurate and understandable
methods for industrial specific OQC cases. Furthermore, it is contemplated to broaden
the spectrum of requirements and limitations of the proposed methods in order to
design a methodology applicable in the industrial environment. In this way, a precise,
adaptable and explainable solution to different real use cases is offered, which will
encourage industry’s interest in the application of these methods.




                                                                                   85
                          Bibliography

[1]   R. Klette, Concise Computer Vision, ser. Undergraduate Topics in
      Computer Science.      London: Springer London, 2014. [Online]. Available:
      http://link.springer.com/10.1007/978-1-4471-6320-6 (Accessed 2022-08-01).

[2]   S. Krig, “Image Pre-Processing,” in Computer vision metrics.     Springer, 2016,
      pp. 39–85.

[3]   S. R. Dubey, S. K. Singh, and B. B. Chaudhuri, “Activation functions in deep
      learning: A comprehensive survey and benchmark,” Neurocomputing, vol. 503,
      pp. 92–108, Sep. 2022. [Online]. Available: https://linkinghub.elsevier.com/
      retrieve/pii/S0925231222008426 (Accessed 2022-10-20).

[4]   “Document search - Web of Science Core Collection.” [Online]. Available:
      https://www.webofscience.com/wos/woscc/basic-search (Accessed 2022-10-
      21).

[5]   P. Chapman, J. Clinton, R. Kerber, T. Khabaza, T. Reinartz, C. Shearer, R. Wirth,
      and others, “CRISP-DM 1.0: Step-by-step data mining guide,” SPSS inc, vol. 9,
      no. 13, pp. 1–73, 2000.

[6]   F. Tao, Q. Qi, A. Liu, and A. Kusiak, “Data-driven smart manufacturing,”
      Journal of Manufacturing Systems, vol. 48, pp. 157–169, Jul. 2018. [Online].
      Available: https://linkinghub.elsevier.com/retrieve/pii/S0278612518300062
      (Accessed 2022-10-21).

[7]   A. G. Frank, L. S. Dalenogare, and N. F. Ayala, “Industry 4.0 technologies:
      Implementation patterns in manufacturing companies,” International Journal
      of Production Economics, vol. 210, pp. 15–26, 2019, publisher: Elsevier.

[8]   C. Salkin, M. Oner, A. Ustundag, and E. Cevikcan, “A Conceptual
      Framework for Industry 4.0,” in Industry 4.0: Managing The Digital

                                                                                    87
B IBLIOGRAPHY


      Transformation, A. Ustundag and E. Cevikcan, Eds.            Cham: Springer
      International Publishing, 2018, pp. 3–23. [Online]. Available:            https:
      //doi.org/10.1007/978-3-319-57870-5_1 (Accessed 2022-03-14).

[9]   R. Godina and J. C. O. Matias, “Quality Control in the Context of Industry 4.0,”
      in Industrial Engineering and Operations Management II, J. Reis, S. Pinelas,
      and N. Melão, Eds.      Cham: Springer International Publishing, 2019, pp.
      177–187.

[10] A. Frankó and P. Varga, “A Survey on Machine Learning based Smart
      Maintenance and Quality Control Solutions,” Infocommunications Journal,
      vol. 13, no. 4, pp. 28–35, 2021.

[11] P. Bergmann, M. Fauser, D. Sattlegger, and C. Steger, “MVTec AD — A
      Comprehensive Real-World Dataset for Unsupervised Anomaly Detection,”
      in 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition
      (CVPR).     Long Beach, CA, USA: IEEE, Jun. 2019, pp. 9584–9592.
      [Online]. Available: https://ieeexplore.ieee.org/document/8954181/ (Accessed
      2022-05-23).

[12] P. Corke, Robotics, Vision and Control, ser. Springer Tracts in Advanced
      Robotics, B. Siciliano and O. Khatib, Eds.      Berlin, Heidelberg: Springer
      Berlin Heidelberg, 2011, vol. 73. [Online]. Available: http://link.springer.com/
      10.1007/978-3-642-20144-8 (Accessed 2022-10-07).

[13] R. Szeliski, Computer vision: algorithms and applications.      Springer Nature,
      2022.

[14] R. Gross and V. Brajovic, “An Image Preprocessing Algorithm for Illumination
      Invariant Face Recognition,” in Audio- and Video-Based Biometric Person
      Authentication, G. Goos, J. Hartmanis, J. van Leeuwen, J. Kittler, and M. S.
      Nixon, Eds. Berlin, Heidelberg: Springer Berlin Heidelberg, 2003, vol. 2688,
      pp. 10–18, series Title: Lecture Notes in Computer Science. [Online]. Available:
      http://link.springer.com/10.1007/3-540-44887-X_2 (Accessed 2022-10-19).

[15] R. Klette, “Image Processing,” in Concise Computer Vision.              London:
      Springer London, 2014, pp. 43–87, series Title: Undergraduate Topics
      in Computer Science. [Online]. Available: http://link.springer.com/10.1007/
      978-1-4471-6320-6_2 (Accessed 2022-07-30).

[16] H. Zhou, R. Wang, and C. Wang, “A novel extended local-binary-pattern
      operator for texture analysis,” Information Sciences, vol. 178, no. 22, pp.

88
                                                                        Bibliography


       4314–4325, Nov. 2008. [Online]. Available: https://linkinghub.elsevier.com/
       retrieve/pii/S0020025508002715 (Accessed 2022-10-19).

[17]   A. Fathi and A. R. Naghsh-Nilchi, “Noise tolerant local binary pattern
       operator for efficient texture analysis,” Pattern Recognition Letters,
       vol. 33, no. 9, pp. 1093–1100, Jul. 2012. [Online]. Available: https:
       //linkinghub.elsevier.com/retrieve/pii/S0167865512000281 (Accessed 2022-10-
       19).

[18]   “Introduction,” in Introduction to Digital Signal Processing and Filter
       Design. Hoboken, NJ, USA: John Wiley & Sons, Inc., Oct. 2005, pp. 1–31.
       [Online]. Available: https://onlinelibrary.wiley.com/doi/10.1002/0471656372.
       ch1 (Accessed 2022-10-07).

[19]   G. Wunder, P. Jung, M. Kasparick, T. Wild, F. Schaich, Y. Chen, S. T. Brink,
       I. Gaspar, N. Michailow, A. Festag, L. Mendes, N. Cassiau, D. Ktenas,
       M. Dryjanski, S. Pietrzyk, B. Eged, P. Vago, and F. Wiedmann, “5GNOW:
       non-orthogonal, asynchronous waveforms for future mobile applications,”
       IEEE Communications Magazine, vol. 52, no. 2, pp. 97–105, Feb. 2014.
       [Online]. Available: https://ieeexplore.ieee.org/document/6736749/ (Accessed
       2022-10-07).

[20]   Jingdong Chen, J. Benesty, Yiteng Huang, and S. Doclo, “New insights
       into the noise reduction Wiener filter,” IEEE Transactions on Audio, Speech
       and Language Processing, vol. 14, no. 4, pp. 1218–1234, Jul. 2006.
       [Online]. Available: http://ieeexplore.ieee.org/document/1643650/ (Accessed
       2022-10-07).

[21]   R. Sameni, M. Shamsollahi, C. Jutten, and G. Clifford, “A Nonlinear Bayesian
       Filtering Framework for ECG Denoising,” IEEE Transactions on Biomedical
       Engineering, vol. 54, no. 12, pp. 2172–2185, Dec. 2007. [Online]. Available:
       http://ieeexplore.ieee.org/document/4360034/ (Accessed 2022-10-07).

[22]   S. R. Gunn, “On the discrete representation of the Laplacian of Gaussian,”
       Pattern Recognition, vol. 32, no. 8, pp. 1463–1472, Aug. 1999. [Online].
       Available: https://linkinghub.elsevier.com/retrieve/pii/S0031320398001630
       (Accessed 2022-10-20).

[23]   E. Persoon and K.-S. Fu, “Shape discrimination using Fourier descriptors,”
       IEEE Transactions on systems, man, and cybernetics, vol. 7, no. 3, pp. 170–179,
       1977, publisher: IEEE.

                                                                                   89
B IBLIOGRAPHY


[24] A. Barbhuiya and K. Hemachandran, “Wavelet Tranformations & Its
      Major Applications In Digital Image Processing,” International Journal of
      Engineering Research, vol. 2, no. 3, p. 5, 2013.

[25] K. H. Ghazali, M. F. Mansor, M. M. Mustafa, and A. Hussain,
     “Feature Extraction Technique using Discrete Wavelet Transform for
      Image Classification,” in 2007 5th Student Conference on Research and
     Development. Selangor, Malaysia: IEEE, 2007, pp. 1–4. [Online]. Available:
      http://ieeexplore.ieee.org/document/4451366/ (Accessed 2022-09-02).

[26] H. Andrews and C. Patterson, “Singular value decompositions and digital
      image processing,” IEEE Transactions on Acoustics, Speech, and Signal
      Processing, vol. 24, no. 1, pp. 26–53, Feb. 1976. [Online]. Available:
      http://ieeexplore.ieee.org/document/1162766/ (Accessed 2022-10-20).

[27] G. Zhang, X. Huang, S. Z. Li, Y. Wang, and X. Wu, “Boosting Local Binary
      Pattern (LBP)-Based Face Recognition,” in Advances in Biometric Person
     Authentication, D. Hutchison, T. Kanade, J. Kittler, J. M. Kleinberg, F. Mattern,
      J. C. Mitchell, M. Naor, O. Nierstrasz, C. Pandu Rangan, B. Steffen, M. Sudan,
      D. Terzopoulos, D. Tygar, M. Y. Vardi, G. Weikum, S. Z. Li, J. Lai, T. Tan,
      G. Feng, and Y. Wang, Eds.    Berlin, Heidelberg: Springer Berlin Heidelberg,
      2004, vol. 3338, pp. 179–186, series Title: Lecture Notes in Computer Science.
      [Online]. Available: http://link.springer.com/10.1007/978-3-540-30548-4_21
     (Accessed 2022-09-13).

[28] Zhenhua Guo, Lei Zhang, and D. Zhang, “A Completed Modeling of Local
      Binary Pattern Operator for Texture Classification,” IEEE Transactions on
      Image Processing, vol. 19, no. 6, pp. 1657–1663, Jun. 2010. [Online]. Available:
      http://ieeexplore.ieee.org/document/5427137/ (Accessed 2022-09-13).

[29] T. Ojala, M. Pietikainen, and T. Maenpaa, “Multiresolution gray-
      scale and rotation invariant texture classification with local binary
      patterns,” IEEE Transactions on Pattern Analysis and Machine Intelligence,
      vol. 24, no. 7, pp. 971–987, Jul. 2002. [Online]. Available:               http:
      //ieeexplore.ieee.org/document/1017623/ (Accessed 2022-09-13).

[30] “What Is Machine Learning (ML)?” Jun. 2020. [Online]. Available:
      https://ischoolonline.berkeley.edu/blog/what-is-machine-learning/ (Accessed
      2022-10-08).

90
                                                                          Bibliography


[31]   J. D. Kelleher, Deep learning, ser. The MIT press essential knowledge series.
       Cambridge, Massachusetts: The MIT Press, 2019.

[32]   M. Zeiler, M. Ranzato, R. Monga, M. Mao, K. Yang, Q. Le, P. Nguyen,
       A. Senior, V. Vanhoucke, J. Dean, and G. Hinton, “On rectified linear units
       for speech processing,” in 2013 IEEE International Conference on Acoustics,
       Speech and Signal Processing. Vancouver, BC, Canada: IEEE, May 2013, pp.
       3517–3521. [Online]. Available: http://ieeexplore.ieee.org/document/6638312/
       (Accessed 2022-10-20).

[33]   Y. Lecun, “Gradient-Based Learning Applied to Document Recognition,”
       PROCEEDINGS OF THE IEEE, vol. 86, no. 11, p. 47, 1998.

[34]   F. Murtagh, “Multilayer perceptrons for classification and regression,”
       Neurocomputing, vol. 2, no. 5-6, pp. 183–197, Jul. 1991. [Online].
       Available:    https://linkinghub.elsevier.com/retrieve/pii/0925231291900235
       (Accessed 2022-11-11).

[35]   S. Ruder, “An overview of gradient descent optimization algorithms,” Jun. 2017,
       arXiv:1609.04747 [cs]. [Online]. Available: http://arxiv.org/abs/1609.04747
       (Accessed 2022-09-07).

[36]   D. P. Kingma and J. Ba, “Adam: A Method for Stochastic Optimization,” Jan.
       2017, arXiv:1412.6980 [cs]. [Online]. Available: http://arxiv.org/abs/1412.6980
       (Accessed 2022-09-07).

[37]   L. P. Kaelbling, M. L. Littman, and A. W. Moore, “Reinforcement learning: A
       survey,” Journal of artificial intelligence research, vol. 4, pp. 237–285, 1996.

[38]   A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, and others,
       “Language models are unsupervised multitask learners,” OpenAI blog, vol. 1,
       no. 8, p. 9, 2019.

[39]   I. Goodfellow, Y. Bengio, and A. Courville, “Practical Methodology,” in Deep
       Learning. MIT Press, 2016, p. 421.

[40]   C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking the
       Inception Architecture for Computer Vision,” in 2016 IEEE Conference on
       Computer Vision and Pattern Recognition (CVPR), 2016, pp. 2818–2826.

[41]   A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet classification
       with deep convolutional neural networks,” Communications of the

                                                                                     91
B IBLIOGRAPHY


      ACM, vol. 60, no. 6, pp. 84–90, May 2017. [Online]. Available:
      https://doi.org/10.1145/3065386 (Accessed 2022-05-19).

[42] C. Szegedy, Wei Liu, Yangqing Jia, P. Sermanet, S. Reed, D. Anguelov,
      D. Erhan, V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,”
      in 2015 IEEE Conference on Computer Vision and Pattern Recognition
      (CVPR).   Boston, MA, USA: IEEE, Jun. 2015, pp. 1–9. [Online]. Available:
      http://ieeexplore.ieee.org/document/7298594/ (Accessed 2022-06-08).

[43] K. Simonyan and A. Zisserman, “Very Deep Convolutional Networks for
      Large-Scale Image Recognition,” Apr. 2015, number: arXiv:1409.1556
      arXiv:1409.1556 [cs]. [Online]. Available: http://arxiv.org/abs/1409.1556
      (Accessed 2022-06-08).

[44] K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning for Image
      Recognition,” in 2016 IEEE Conference on Computer Vision and Pattern
      Recognition (CVPR).      Las Vegas, NV, USA: IEEE, Jun. 2016, pp. 770–778.
      [Online]. Available: http://ieeexplore.ieee.org/document/7780459/ (Accessed
      2022-06-08).

[45] M. Tan and Q. Le, “Efficientnet: Rethinking model scaling for convolutional
      neural networks,” in International conference on machine learning.    PMLR,
      2019, pp. 6105–6114.

[46] Z. Li, C. Peng, G. Yu, X. Zhang, Y. Deng, and J. Sun, “DetNet:
      Design Backbone for Object Detection,” in Computer Vision – ECCV
      2018, V. Ferrari, M. Hebert, C. Sminchisescu, and Y. Weiss, Eds.
      Cham: Springer International Publishing, 2018, vol. 11213, pp. 339–354,
      series Title: Lecture Notes in Computer Science. [Online]. Available:
      http://link.springer.com/10.1007/978-3-030-01240-3_21 (Accessed 2022-06-
      08).

[47] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
      \. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in neural
      information processing systems, vol. 30, 2017.

[48] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. Shah,
     “Transformers in vision: A survey,” ACM computing surveys (CSUR), vol. 54,
      no. 10s, pp. 1–41, 2022, publisher: ACM New York, NY.

[49] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
      T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit,

92
                                                                      Bibliography


       and N. Houlsby, “An Image is Worth 16x16 Words: Transformers for Image
       Recognition at Scale,” Jun. 2021, arXiv:2010.11929 [cs]. [Online]. Available:
       http://arxiv.org/abs/2010.11929 (Accessed 2022-09-08).

[50]   M.-H. Guo, T.-X. Xu, J.-J. Liu, Z.-N. Liu, P.-T. Jiang, T.-J. Mu,
       S.-H. Zhang, R. R. Martin, M.-M. Cheng, and S.-M. Hu, “Attention
       mechanisms in computer vision:         A survey,” Computational Visual
       Media, vol. 8, no. 3, pp. 331–368, Sep. 2022. [Online]. Available:
       https://link.springer.com/10.1007/s41095-022-0271-y (Accessed 2022-10-17).

[51]   K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang, A. Xiao, C. Xu,
       Y. Xu, and others, “A survey on vision transformer,” IEEE transactions on
       pattern analysis and machine intelligence, 2022, publisher: IEEE.

[52]   C. Sun, A. Shrivastava, S. Singh, and A. Gupta, “Revisiting Unreasonable
       Effectiveness of Data in Deep Learning Era,” in 2017 IEEE International
       Conference on Computer Vision (ICCV).        Venice: IEEE, Oct. 2017, pp.
       843–852. [Online]. Available: http://ieeexplore.ieee.org/document/8237359/
       (Accessed 2022-10-18).

[53]   X. Zhai, A. Kolesnikov, N. Houlsby, and L. Beyer, “Scaling Vision
       Transformers,” in 2022 IEEE/CVF Conference on Computer Vision and
       Pattern Recognition (CVPR). New Orleans, LA, USA: IEEE, Jun. 2022, pp.
       1204–1213. [Online]. Available: https://ieeexplore.ieee.org/document/9880094/
       (Accessed 2022-10-18).

[54]   Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo,
       “Swin Transformer: Hierarchical Vision Transformer using Shifted Windows,”
       in 2021 IEEE/CVF International Conference on Computer Vision (ICCV).
       Montreal, QC, Canada: IEEE, Oct. 2021, pp. 9992–10 002. [Online]. Available:
       https://ieeexplore.ieee.org/document/9710580/ (Accessed 2022-10-18).

[55]   H. Fan, B. Xiong, K. Mangalam, Y. Li, Z. Yan, J. Malik, and C. Feichtenhofer,
       “Multiscale Vision Transformers,” in 2021 IEEE/CVF International Conference
       on Computer Vision (ICCV).     Montreal, QC, Canada: IEEE, Oct. 2021, pp.
       6804–6815. [Online]. Available: https://ieeexplore.ieee.org/document/9710800/
       (Accessed 2022-10-18).

[56]   “ImageNet.” [Online]. Available: https://www.image-net.org/ (Accessed
       2022-05-19).

                                                                                 93
B IBLIOGRAPHY


[57] J. Yu, Z. Wang, V. Vasudevan, L. Yeung, M. Seyedhosseini, and Y. Wu,
      “CoCa: Contrastive Captioners are Image-Text Foundation Models,” Jun. 2022,
      arXiv:2205.01917 [cs]. [Online]. Available: http://arxiv.org/abs/2205.01917
      (Accessed 2022-10-18).

[58] M. Wortsman, G. Ilharco, S. Y. Gadre, R. Roelofs, R. Gontijo-Lopes, A. S.
      Morcos, H. Namkoong, A. Farhadi, Y. Carmon, S. Kornblith, and others,
      “Model soups: averaging weights of multiple fine-tuned models improves
      accuracy without increasing inference time,” in International Conference on
      Machine Learning. PMLR, 2022, pp. 23 965–23 998.

[59] Z. Dai, H. Liu, Q. V. Le, and M. Tan, “Coatnet: Marrying convolution and
      attention for all data sizes,” Advances in Neural Information Processing Systems,
      vol. 34, pp. 3965–3977, 2021.

[60] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie, “A convnet
      for the 2020s,” in Proceedings of the IEEE/CVF Conference on Computer
      Vision and Pattern Recognition, 2022, pp. 11 976–11 986.

[61] M. Ding, B. Xiao, N. Codella, P. Luo, J. Wang, and L. Yuan, “DaViT:
      Dual Attention Vision Transformers,” p. 23, 2022. [Online]. Available:
      https://arxiv.org/abs/2204.03645 (Accessed 2022-11-11).

[62] H. Pham, Z. Dai, Q. Xie, and Q. V. Le, “Meta Pseudo Labels,” in 2021
      IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).
      Nashville, TN, USA: IEEE, Jun. 2021, pp. 11 552–11 563. [Online]. Available:
      https://ieeexplore.ieee.org/document/9578487/ (Accessed 2022-10-18).

[63] “Papers with Code - ImageNet Benchmark (Image Classification).” [Online].
      Available: https://paperswithcode.com/sota/image-classification-on-imagenet
      (Accessed 2022-10-18).

[64] Z.-Q. Zhao, P. Zheng, S.-T. Xu, and X. Wu, “Object Detection With Deep
      Learning: A Review,” IEEE Transactions on Neural Networks and Learning
      Systems, vol. 30, no. 11, pp. 3212–3232, Nov. 2019. [Online]. Available:
      https://ieeexplore.ieee.org/document/8627998/ (Accessed 2022-09-08).

[65] A. Borji, M.-M. Cheng, H. Jiang, and J. Li, “Salient Object Detection:
      A Benchmark,” IEEE Transactions on Image Processing, vol. 24, no. 12,
      pp. 5706–5722, Dec. 2015. [Online]. Available: http://ieeexplore.ieee.org/
      document/7293665/ (Accessed 2022-09-12).

94
                                                                        Bibliography


[66]   E. Hjelmås and B. K. Low, “Face Detection: A Survey,” Computer Vision
       and Image Understanding, vol. 83, no. 3, pp. 236–274, Sep. 2001. [Online].
       Available: https://linkinghub.elsevier.com/retrieve/pii/S107731420190921X
       (Accessed 2022-09-12).

[67]   P. Dollar, C. Wojek, B. Schiele, and P. Perona, “Pedestrian Detection:
       An Evaluation of the State of the Art,” IEEE Transactions on Pattern
       Analysis and Machine Intelligence, vol. 34, no. 4, pp. 743–761, Apr. 2012.
       [Online]. Available: http://ieeexplore.ieee.org/document/5975165/ (Accessed
       2022-09-12).

[68]   R. Yao, G. Lin, S. Xia, J. Zhao, and Y. Zhou, “Video Object Segmentation
       and Tracking: A Survey,” ACM Transactions on Intelligent Systems and
       Technology, vol. 11, no. 4, pp. 1–47, Aug. 2020. [Online]. Available:
       https://dl.acm.org/doi/10.1145/3391743 (Accessed 2022-09-12).

[69]   R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hierarchies
       for accurate object detection and semantic segmentation,” in Proceedings of
       the IEEE conference on computer vision and pattern recognition, 2014, pp.
       580–587.

[70]   R. Girshick, “Fast r-cnn,” in Proceedings of the IEEE international conference
       on computer vision, 2015, pp. 1440–1448.

[71]   S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time object
       detection with region proposal networks,” Advances in neural information
       processing systems, vol. 28, 2015.

[72]   J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once:
       Unified, real-time object detection,” in Proceedings of the IEEE conference on
       computer vision and pattern recognition, 2016, pp. 779–788.

[73]   M. Tan, R. Pang, and Q. V. Le, “Efficientdet: Scalable and efficient object
       detection,” in Proceedings of the IEEE/CVF conference on computer vision and
       pattern recognition, 2020, pp. 10 781–10 790.

[74]   W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C. Berg,
       “Ssd: Single shot multibox detector,” in European conference on computer
       vision. Springer, 2016, pp. 21–37.

                                                                                   95
B IBLIOGRAPHY


[75] T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie, “Feature
      pyramid networks for object detection,” in Proceedings of the IEEE conference
      on computer vision and pattern recognition, 2017, pp. 2117–2125.

[76] W. Du, H. Shen, J. Fu, G. Zhang, and Q. He, “Approaches for improvement of
      the X-ray image defect detection of automobile casting aluminum parts based on
      deep learning,” NDT & E International, vol. 107, p. 102144, Oct. 2019. [Online].
      Available: https://linkinghub.elsevier.com/retrieve/pii/S0963869519300192
      (Accessed 2022-04-11).

[77] S. Minaee, Y. Y. Boykov, F. Porikli, A. J. Plaza, N. Kehtarnavaz, and
      D. Terzopoulos, “Image Segmentation Using Deep Learning: A Survey,” IEEE
      Transactions on Pattern Analysis and Machine Intelligence, pp. 1–1, 2021.
      [Online]. Available: https://ieeexplore.ieee.org/document/9356353/ (Accessed
      2022-09-10).

[78] O. Ronneberger, P. Fischer, and T. Brox, “U-Net: Convolutional Networks
      for Biomedical Image Segmentation,” in Medical Image Computing and
      Computer-Assisted Intervention – MICCAI 2015, N. Navab, J. Hornegger,
      W. M. Wells, and A. F. Frangi, Eds. Cham: Springer International Publishing,
      2015, vol. 9351, pp. 234–241, series Title: Lecture Notes in Computer Science.
      [Online]. Available: http://link.springer.com/10.1007/978-3-319-24574-4_28
      (Accessed 2022-10-18).

[79] A. Kirillov, K. He, R. Girshick, C. Rother, and P. Dollar, “Panoptic
      Segmentation,” in 2019 IEEE/CVF Conference on Computer Vision and
      Pattern Recognition (CVPR).      Long Beach, CA, USA: IEEE, Jun. 2019, pp.
      9396–9405. [Online]. Available: https://ieeexplore.ieee.org/document/8953237/
      (Accessed 2022-10-19).

[80] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for
      semantic segmentation,” in Proceedings of the IEEE conference on computer
      vision and pattern recognition, 2015, pp. 3431–3440.

[81] K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask r-cnn,” in Proceedings of
      the IEEE international conference on computer vision, 2017, pp. 2961–2969.

[82] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
      S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial networks,”
      Communications of the ACM, vol. 63, no. 11, pp. 139–144, Oct. 2020. [Online].
      Available: https://dl.acm.org/doi/10.1145/3422622 (Accessed 2022-10-20).

96
                                                                        Bibliography


[83]   Z. Wang, Q. She, and T. E. Ward, “Generative adversarial networks in computer
       vision: A survey and taxonomy,” ACM Computing Surveys (CSUR), vol. 54,
       no. 2, pp. 1–38, 2021, publisher: ACM New York, NY, USA.

[84]   S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee, “Generative
       adversarial text to image synthesis,” in International conference on machine
       learning. PMLR, 2016, pp. 1060–1069.

[85]   A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and
       I. Sutskever, “Zero-shot text-to-image generation,” in International Conference
       on Machine Learning. PMLR, 2021, pp. 8821–8831.

[86]   A. Mikolajczyk and M. Grochowski, “Data augmentation for improving
       deep learning in image classification problem,” in 2018 International
       Interdisciplinary PhD Workshop (IIPhDW). Swinoujście: IEEE, May 2018, pp.
       117–122. [Online]. Available: https://ieeexplore.ieee.org/document/8388338/
       (Accessed 2022-10-20).

[87]   V. Chandola, A. Banerjee, and V. Kumar, “Anomaly detection: A survey,” ACM
       Computing Surveys, vol. 41, no. 3, pp. 1–58, Jul. 2009. [Online]. Available:
       https://dl.acm.org/doi/10.1145/1541880.1541882 (Accessed 2022-09-12).

[88]   G. Pang, C. Shen, L. Cao, and A. V. D. Hengel, “Deep Learning for Anomaly
       Detection: A Review,” ACM Computing Surveys, vol. 54, no. 2, pp. 1–38, Mar.
       2022. [Online]. Available: https://dl.acm.org/doi/10.1145/3439950 (Accessed
       2022-10-20).

[89]   R. Chalapathy and S. Chawla, “Deep Learning for Anomaly Detection: A
       Survey,” Jan. 2019, number: arXiv:1901.03407 arXiv:1901.03407 [cs, stat].
       [Online]. Available: http://arxiv.org/abs/1901.03407 (Accessed 2022-06-06).

[90]   Y. Wang, H. Yao, and S. Zhao, “Auto-encoder based dimensionality reduction,”
       Neurocomputing, vol. 184, pp. 232–242, 2016, publisher: Elsevier.

[91]   M. Chen, X. Shi, Y. Zhang, D. Wu, and M. Guizani, “Deep Feature Learning
       for Medical Image Analysis with Convolutional Autoencoder Neural Network,”
       IEEE Transactions on Big Data, vol. 7, no. 4, pp. 750–758, Oct. 2021.
       [Online]. Available: https://ieeexplore.ieee.org/document/7954012/ (Accessed
       2022-09-08).

                                                                                   97
B IBLIOGRAPHY


[92] J. Yi and S. Yoon, “Patch svdd: Patch-level svdd for anomaly detection and
      segmentation,” in Proceedings of the Asian Conference on Computer Vision,
      2020.

[93] Y. Liang, J. Zhang, S. Zhao, R. Wu, Y. Liu, and S. Pan, “Omni-frequency
      Channel-selection Representations for Unsupervised Anomaly Detection,”
      arXiv:2203.00259 [cs], Mar. 2022, arXiv: 2203.00259. [Online]. Available:
      http://arxiv.org/abs/2203.00259 (Accessed 2022-04-28).

[94] M. Rudolph, B. Wandt, and B. Rosenhahn, “Same same but differnet: Semi-
      supervised defect detection with normalizing flows,” in Proceedings of the
      IEEE/CVF winter conference on applications of computer vision, 2021, pp.
      1907–1916.

[95] C.-C. Tsai, T.-H. Wu, and S.-H. Lai, “Multi-Scale Patch-Based Representation
      Learning for Image Anomaly Detection and Segmentation,” in 2022
      IEEE/CVF Winter Conference on Applications of Computer Vision (WACV).
      Waikoloa, HI, USA: IEEE, Jan. 2022, pp. 3065–3073. [Online]. Available:
      https://ieeexplore.ieee.org/document/9707044/ (Accessed 2022-05-23).

[96] J. Song, K. Kong, Y.-I. Park, S.-G. Kim, and S.-J. Kang, “AnoSeg: Anomaly
      Segmentation Network Using Self-Supervised Learning,” arXiv:2110.03396
      [cs, eess], Oct. 2021, arXiv:        2110.03396. [Online]. Available:     http:
      //arxiv.org/abs/2110.03396 (Accessed 2022-04-28).

[97] P. Spethmann, S. H. Thomke, and C. Herstatt, “The impact of crash simulation
      on productivity and problem-solving in automotive R&D,” Working Paper,
      Tech. Rep., 2006.

[98] P. Charalampous, I. Kostavelis, and D. Tzovaras, “Non-destructive quality
      control methods in additive manufacturing: a survey,” Rapid Prototyping
      Journal, 2020, publisher: Emerald Publishing Limited.

[99] “Overview      of    Nondestructive    Testing   (NDT),”    publication   Title:
      Inspectioneering. [Online]. Available:       https://inspectioneering.com/tag/
      nondestructive+testing (Accessed 2022-09-14).

[100] F. SA, “What is destructive testing and how does it work?” publication Title:
      Flyability. [Online]. Available: https://www.flyability.com/destructive-testing
      (Accessed 2022-09-14).

98
                                                                        Bibliography


[101] “Hardness Testing Basics.” [Online]. Available: https://www.hardnesstesters.
      com/test-types/hardness-testing-basics (Accessed 2022-10-20).

[102] W. Weibull, Fatigue testing and analysis of results.   Elsevier, 2013.

[103] F. Honarvar and A. Varvani-Farahani, “A review of ultrasonic testing
      applications in additive manufacturing:          Defect evaluation, material
      characterization, and process control,” Ultrasonics, vol. 108, p. 106227,
      Dec. 2020. [Online]. Available: https://linkinghub.elsevier.com/retrieve/pii/
      S0041624X20301669 (Accessed 2022-10-20).

[104] J. A. Slotwinski, E. J. Garboczi, and K. M. Hebenstreit, “Porosity
      Measurements and Analysis for Metal Additive Manufacturing Process
      Control,” Journal of Research of the National Institute of Standards
      and Technology, vol. 119, p. 494, Oct. 2014. [Online]. Available:
      https://nvlpubs.nist.gov/nistpubs/jres/119/jres.119.019.pdf (Accessed 2022-10-
      20).

[105] M. K. Kutman, F. Z. B. Muftuler, C. Harmansah, and O. K. Guldu, “Use
      of Bacteria as Fluorescent Penetrant for Penetrant Testing (PT),” Journal of
      Nondestructive Evaluation, vol. 39, no. 1, p. 15, Mar. 2020. [Online]. Available:
      http://link.springer.com/10.1007/s10921-020-0653-y (Accessed 2022-10-20).

[106] A. Khadersab and S. Shivakumar, “Vibration Analysis Techniques
      for Rotating Machinery and its effect on Bearing Faults,” Procedia
      Manufacturing, vol. 20, pp. 247–252, 2018. [Online]. Available: https:
      //linkinghub.elsevier.com/retrieve/pii/S2351978918300696 (Accessed 2022-10-
      20).

[107] F. Ciampa, P. Mahmoodi, F. Pinto, and M. Meo, “Recent Advances in
      Active Infrared Thermography for Non-Destructive Testing of Aerospace
      Components,” Sensors, vol. 18, no. 2, p. 609, Feb. 2018. [Online]. Available:
      https://www.mdpi.com/1424-8220/18/2/609 (Accessed 2022-10-20).

[108] D. Mery, “Aluminum Casting Inspection using Deep Object Detection Methods
      and Simulated Ellipsoidal Defects,” Machine Vision and Applications, vol. 32,
      no. 3, p. 72, May 2021. [Online]. Available: https://link.springer.com/10.1007/
      s00138-021-01195-5 (Accessed 2022-04-11).

[109] J.-K. Kuo, J.-J. Wu, P.-H. Huang, and C.-Y. Cheng, “Inspection
      of sandblasting defect in investment castings by deep convolutional

                                                                                    99
B IBLIOGRAPHY


      neural network,” The International Journal of Advanced Manufacturing
      Technology, vol. 120, no. 3-4, pp. 2457–2468, May 2022. [Online].
      Available: https://link.springer.com/10.1007/s00170-022-08841-w (Accessed
      2022-05-09).

[110] Y. Wang, C. Hu, K. Chen, and Z. Yin, “Self-attention guided model for
      defect detection of aluminium alloy casting on X-ray image,” Computers &
      Electrical Engineering, vol. 88, p. 106821, Dec. 2020. [Online]. Available:
      https://linkinghub.elsevier.com/retrieve/pii/S0045790620306777 (Accessed
      2022-05-02).

[111] D. Mery, “Computer vision for X-Ray testing,” Switzerland: Springer
      International Publishing, vol. 10, pp. 978–3, 2015, publisher: Springer.

[112] D. Mery and C. Pieringer, Computer Vision for X-Ray Testing: Imaging,
      Systems, Image Databases, and Algorithms.       Cham: Springer International
      Publishing, 2021. [Online]. Available:       http://link.springer.com/10.1007/
      978-3-030-56769-9 (Accessed 2022-07-11).

[113] K. Liu, A. Li, X. Wen, H. Chen, and P. Yang, “Steel Surface Defect Detection
      Using GAN and One-Class Classifier,” in 2019 25th International Conference
      on Automation and Computing (ICAC), 2019, pp. 1–6.

[114] A. Kumar, “Computer-vision-based fabric defect detection: A survey,” IEEE
      transactions on industrial electronics, vol. 55, no. 1, pp. 348–363, 2008,
      publisher: IEEE.

[115] D. Mery and D. Filbert, “Automated flaw detection in aluminum castings based
      on the tracking of potential defects in a radioscopic image sequence,” IEEE
      Transactions on Robotics and Automation, vol. 18, no. 6, pp. 890–901, 2002,
      publisher: IEEE.

[116] T. Czimmermann, G. Ciuti, M. Milazzo, M. Chiurazzi, S. Roccella, C. M. Oddo,
      and P. Dario, “Visual-based defect detection and classification approaches for
      industrial applications—a survey,” Sensors, vol. 20, no. 5, p. 1459, 2020,
      publisher: Multidisciplinary Digital Publishing Institute.

[117] H. Boerner and H. Strecker, “Automated X-Ray Inspection of Aluminum
      Castings,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 10, pp. 79–91, 1988.

100
                                                                        Bibliography


[118] Q. Luo, X. Fang, L. Liu, C. Yang, and Y. Sun, “Automated Visual
      Defect Detection for Flat Steel Surface: A Survey,” IEEE Transactions on
      Instrumentation and Measurement, vol. 69, no. 3, pp. 626–644, 2020.

[119] D. Mery, T. Jaeger, and D. Filbert, “A Review of Methods for Automated
      Recognition of Casting Defects,” Tech. Rep., 2002. [Online]. Available:
      https://www.researchgate.net/publication/238503414

[120] A. Gayer, A. Saya, and A. Shiloh, “Automatic recognition of welding defects
      in real-time radiography,” Ndt International, vol. 23, no. 3, pp. 131–136, 1990,
      publisher: Elsevier.

[121] D.-M. Tsai and T.-Y. Huang, “Automated surface inspection for statistical
      textures,” Image and Vision Computing, vol. 21, no. 4, pp. 307–323,
      2003. [Online]. Available: https://www.sciencedirect.com/science/article/pii/
      S0262885603000076

[122] X. Li, S. K. Tso, X.-P. Guan, and Q. Huang, “Improving automatic detection
      of defects in castings by applying wavelet technique,” IEEE Transactions on
      Industrial Electronics, vol. 53, no. 6, pp. 1927–1934, 2006, publisher: IEEE.

[123] F. Zhao, P. R. S. Mendonca, J. Yu, and R. Kaucic, “Learning-based
      automatic defect recognition with computed tomographic imaging,” in
      2013 IEEE International Conference on Image Processing.            Melbourne,
      Australia:    IEEE, Sep. 2013, pp. 2762–2766. [Online]. Available:
      http://ieeexplore.ieee.org/document/6738569/ (Accessed 2022-04-13).

[124] D. Mery and D. Filbert, “Classification of potential defects in automated
      inspection of aluminium castings using statistical pattern recognition,” in 8th
      European Conference on Non-Destructive Testing (ECNDT 2002), 2002, pp.
      1–10.

[125] D. Mery, “Crossing Line Profile:          A New Approach to Detecting
      Defects in Aluminium Die Casting,” in Image Analysis, G. Goos,
      J. Hartmanis, J. van Leeuwen, J. Bigun, and T. Gustavsson, Eds.         Berlin,
      Heidelberg: Springer Berlin Heidelberg, 2003, vol. 2749, pp. 725–732,
      series Title: Lecture Notes in Computer Science. [Online]. Available:
      http://link.springer.com/10.1007/3-540-45103-X_96 (Accessed 2022-04-02).

[126] D. Mery and C. Arteta, “Automatic Defect Recognition in X-Ray Testing
      Using Computer Vision,” in 2017 IEEE Winter Conference on Applications

                                                                                 101
B IBLIOGRAPHY


      of Computer Vision (WACV).      Santa Rosa, CA, USA: IEEE, Mar. 2017, pp.
      1026–1035. [Online]. Available: http://ieeexplore.ieee.org/document/7926702/
      (Accessed 2022-04-13).

[127] I. Pastor-López, B. Sanz, A. Tellaeche, G. Psaila, J. G. de la
      Puerta, and P. G. Bringas, “Quality assessment methodology based
      on machine learning with small datasets:        Industrial castings defects,”
      Neurocomputing, vol. 456, pp. 622–628, Oct. 2021. [Online]. Available:
      https://linkinghub.elsevier.com/retrieve/pii/S0925231221009486 (Accessed
      2022-04-13).

[128] A. Marcano-Cedeno, J. Quintanilla-Dominguez, M. G. Cortina-Januchs,
      and D. Andina, “Feature selection using Sequential Forward Selection and
      classification applying Artificial Metaplasticity Neural Network,” in IECON
      2010 - 36th Annual Conference on IEEE Industrial Electronics Society.
      Glendale, AZ, USA: IEEE, Nov. 2010, pp. 2845–2850. [Online]. Available:
      http://ieeexplore.ieee.org/document/5675075/ (Accessed 2022-06-08).

[129] P. Praus, “Content-based X-ray image analysis of aluminium castings,”
      International Journal of Computational Materials Science and Surface
      Engineering, vol. 4, no. 3, pp. 219–231, 2011, publisher: Inderscience
      Publishers Ltd.

[130] I. Pastor-Lopez, I. Santos, A. Santamaria-Ibirika, M. Salazar, J. de-la
      Pena-Sordo, and P. G. Bringas, “Machine-learning-based surface defect
      detection and categorisation in high-precision foundry,” in 2012 7th
      IEEE Conference on Industrial Electronics and Applications (ICIEA).
      Singapore, Singapore: IEEE, Jul. 2012, pp. 1359–1364. [Online]. Available:
      http://ieeexplore.ieee.org/document/6360934/ (Accessed 2022-04-13).

[131] N. Friedman, D. Geiger, and M. Goldszmidt, “Bayesian network classifiers,”
      Machine learning, vol. 29, no. 2, pp. 131–163, 1997, publisher: Springer.

[132] W. S. Noble, “What is a support vector machine?” Nature Biotechnology,
      vol. 24, no. 12, pp. 1565–1567, Dec. 2006. [Online]. Available:
      http://www.nature.com/articles/nbt1206-1565 (Accessed 2022-10-21).

[133] S. Raschka, “STAT 479: Machine Learning Lecture Notes,” Machine Learning,
      p. 23, 2018.

[134] L. Breiman, “Random Forests,” Machine Learning, vol. 45, pp. 5–32, 2004.

102
                                                                        Bibliography


[135] S. Hernández, D. Sáez, and D. Mery, “Neuro-Fuzzy Method for Automated
      Defect Detection in Aluminium Castings,” in Image Analysis and Recognition,
      D. Hutchison, T. Kanade, J. Kittler, J. M. Kleinberg, F. Mattern, J. C.
      Mitchell, M. Naor, O. Nierstrasz, C. Pandu Rangan, B. Steffen, M. Sudan,
      D. Terzopoulos, D. Tygar, M. Y. Vardi, G. Weikum, A. Campilho, and
      M. Kamel, Eds.      Berlin, Heidelberg: Springer Berlin Heidelberg, 2004,
      vol. 3212, pp. 826–833, series Title: Lecture Notes in Computer Science.
      [Online]. Available: http://link.springer.com/10.1007/978-3-540-30126-4_100
      (Accessed 2022-04-12).

[136] B. Wu,     J. Zhou,     X. Ji, Y. Yin,       and X. Shen, “Research on
      Approaches for Computer Aided Detection of Casting Defects in X-
      ray Images with Feature Engineering and Machine Learning,” Procedia
      Manufacturing, vol. 37, pp. 394–401, 2019. [Online]. Available: https:
      //linkinghub.elsevier.com/retrieve/pii/S2351978919312715 (Accessed 2022-04-
      13).

[137] D. Mery, V. Riffo, U. Zscherpel, G. Mondragón, I. Lillo, I. Zuccar, H. Lobel,
      and M. Carrasco, “GDXray: The Database of X-ray Images for Nondestructive
      Testing,” Journal of Nondestructive Evaluation, vol. 34, no. 4, p. 42, Dec. 2015.
      [Online]. Available:    http://link.springer.com/10.1007/s10921-015-0315-7
      (Accessed 2022-05-16).

[138] F. Ramírez and H. Allende, “Detection of flaws in aluminium castings: a
      comparative study between generative and discriminant approaches,” Insight -
      Non-Destructive Testing and Condition Monitoring, vol. 55, no. 7, pp. 366–371,
      Jul. 2013. [Online]. Available:       http://openurl.ingenta.com/content/xref?
      genre=article&issn=1354-2575&volume=55&issue=7&spage=366 (Accessed
      2022-04-13).

[139] F. Riaz, K. Kamal, T. Zafar, and R. Qayyum, “An inspection approach for
      casting defects detection using image segmentation,” in 2017 International
      Conference on Mechanical, System and Control Engineering (ICMSC).
      St.Petersburg, Russia: IEEE, May 2017, pp. 101–105. [Online]. Available:
      http://ieeexplore.ieee.org/document/7959451/ (Accessed 2022-04-13).

[140] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, “How transferable are
      features in deep neural networks?” in Proceedings of the 27th International
      Conference on Neural Information Processing Systems - Volume 2, ser. NIPS’14.

                                                                                  103
B IBLIOGRAPHY


      Cambridge, MA, USA: MIT Press, Dec. 2014, pp. 3320–3328, (Accessed 2022-
      05-19).

[141] X. Jiang, X. f. Wang, and D. Chen, “Research on Defect Detection of Castings
      Based on Deep Residual Network,” in 2018 11th International Congress
      on Image and Signal Processing, BioMedical Engineering and Informatics
      (CISP-BMEI). Beijing, China: IEEE, Oct. 2018, pp. 1–6. [Online]. Available:
      https://ieeexplore.ieee.org/document/8633254/ (Accessed 2022-04-12).

[142] D. Mery, “Aluminum Casting Inspection Using Deep Learning: A Method
      Based on Convolutional Neural Networks,” Journal of Nondestructive
      Evaluation, vol. 39, no. 1, p. 12, Mar. 2020. [Online]. Available:
      http://link.springer.com/10.1007/s10921-020-0655-9 (Accessed 2022-04-08).

[143] M. Ferguson, R. Ak, Y.-T. T. Lee, and K. H. Law, “Automatic localization of
      casting defects with convolutional neural networks,” in 2017 IEEE International
      Conference on Big Data (Big Data).       Boston, MA: IEEE, Dec. 2017, pp.
      1726–1735. [Online]. Available: http://ieeexplore.ieee.org/document/8258115/
      (Accessed 2022-04-12).

[144] M. Ferguson, R. Ak, Y.-T. T. Lee, and K. H. Law, “Detection
      and Segmentation of Manufacturing Defects with Convolutional Neural
      Networks and Transfer Learning,” Smart and Sustainable Manufacturing
      Systems, vol. 2, no. 1, p. 20180033, Oct. 2018. [Online]. Available:
      http://www.astm.org/doiLink.cgi?SSMS20180033 (Accessed 2022-04-11).

[145] L. Duan, K. Yang, and L. Ruan, “Research on Automatic Recognition of Casting
      Defects Based on Deep Learning,” IEEE Access, vol. 9, pp. 12 209–12 216,
      2021. [Online]. Available: https://ieeexplore.ieee.org/document/9311501/
      (Accessed 2022-05-10).

[146] J. Xing and M. Jia, “A convolutional neural network-based method for
      workpiece surface defect detection,” Measurement, vol. 176, p. 109185,
      May 2021. [Online]. Available: https://linkinghub.elsevier.com/retrieve/pii/
      S0263224121002037 (Accessed 2022-05-10).

[147] W. Du, H. Shen, J. Fu, G. Zhang, X. Shi, and Q. He, “Automated detection of
      defects with low semantic information in X-ray images based on deep learning,”
      Journal of Intelligent Manufacturing, vol. 32, no. 1, pp. 141–156, Jan. 2021.
      [Online]. Available: http://link.springer.com/10.1007/s10845-020-01566-1
      (Accessed 2022-04-12).

104
                                                                         Bibliography


[148] S. Liu, L. Qi, H. Qin, J. Shi, and J. Jia, “Path aggregation network for instance
      segmentation,” in Proceedings of the IEEE conference on computer vision and
      pattern recognition, 2018, pp. 8759–8768.

[149] A. García Pérez, M. J. Gómez Silva, and A. de la Escalera Hueso, “Automated
      Defect Recognition of Castings Defects Using Neural Networks,” Journal
      of Nondestructive Evaluation, vol. 41, no. 1, p. 11, Mar. 2022. [Online].
      Available: https://link.springer.com/10.1007/s10921-021-00842-1 (Accessed
      2022-04-11).

[150] L. Xue, J. Hei, X. Chen, Y. Xu, and L. Zheng, “An Efficient Method of Casting
      Defects Detection Based on Deep Learning,” in 2020 IEEE International
      Conference on Advances in Electrical Engineering and Computer Applications(
      AEECA).     Dalian, China: IEEE, Aug. 2020, pp. 480–483. [Online]. Available:
      https://ieeexplore.ieee.org/document/9213492/ (Accessed 2022-04-12).

[151] D. Mery, A. Kaminetzky, L. Golborne, S. Figueroa, and D. Saavedra,
      “Target Detection by Target Simulation in X-ray Testing,” Journal of
      Nondestructive Evaluation, vol. 41, no. 1, p. 21, Mar. 2022. [Online].
      Available: https://link.springer.com/10.1007/s10921-022-00851-8 (Accessed
      2022-05-09).

[152] W. Tang, C. M. Vian, Z. Tang, and B. Yang, “Anomaly detection of
      core failures in die casting X-ray inspection images using a convolutional
      autoencoder,” Machine Vision and Applications, vol. 32, no. 4, p. 102, Jul. 2021.
      [Online]. Available: https://link.springer.com/10.1007/s00138-021-01226-1
      (Accessed 2022-05-02).

[153] J. Lehr, A. Sargsyan, M. Pape, J. Philipps, and J. Kruger, “Automated Optical
      Inspection Using Anomaly Detection and Unsupervised Defect Clustering,”
      in 2020 25th IEEE International Conference on Emerging Technologies
      and Factory Automation (ETFA).         Vienna, Austria: IEEE, Sep. 2020, pp.
      1235–1238. [Online]. Available: https://ieeexplore.ieee.org/document/9212172/
      (Accessed 2022-05-09).

[154] H. Di, X. Ke, Z. Peng, and Z. Dongdong, “Surface defect classification
      of steels with a new semi-supervised learning method,” Optics and Lasers
      in Engineering, vol. 117, pp. 40–48, Jun. 2019. [Online]. Available:
      https://linkinghub.elsevier.com/retrieve/pii/S0143816618313393 (Accessed
      2022-05-23).

                                                                                   105
B IBLIOGRAPHY


[155] W. Wang and C. Su, “Semi-supervised semantic segmentation network for
      surface crack detection,” Automation in Construction, vol. 128, p. 103786,
      2021, publisher: Elsevier.

[156] T. Liu and W. Ye, “A semi-supervised learning method for surface defect
      classification of magnetic tiles,” Machine Vision and Applications, vol. 33,
      no. 2, p. 35, Mar. 2022. [Online]. Available: https://link.springer.com/10.1007/
      s00138-022-01286-x (Accessed 2022-05-23).

[157] X. Zheng, H. Wang, J. Chen, Y. Kong, and S. Zheng, “A generic semi-
      supervised deep learning-based approach for automated surface inspection,”
      IEEE Access, vol. 8, pp. 114 088–114 099, 2020, publisher: IEEE.

[158] P. Mishra, R. Verk, D. Fornasier, C. Piciarelli, and G. L. Foresti, “VT-ADL:
      A vision transformer network for image anomaly detection and localization,”
      in 2021 IEEE 30th International Symposium on Industrial Electronics (ISIE).
      IEEE, 2021, pp. 01–06.

[159] P. Bergmann, S. Löwe, M. Fauser, D. Sattlegger, and C. Steger, “Improving
      Unsupervised Defect Segmentation by Applying Structural Similarity to
      Autoencoders,” in Proceedings of the 14th International Joint Conference on
      Computer Vision, Imaging and Computer Graphics Theory and Applications,
      2019, pp. 372–380.

[160] T. Schlegl, P. Seeböck, S. M. Waldstein, G. Langs, and U. Schmidt-Erfurth,
      “f-AnoGAN: Fast unsupervised anomaly detection with generative adversarial
      networks,” Medical Image Analysis, vol. 54, pp. 30–44, May 2019. [Online].
      Available: https://linkinghub.elsevier.com/retrieve/pii/S1361841518302640
      (Accessed 2022-06-06).

[161] S. Akcay, A. Atapour-Abarghouei, and T. P. Breckon, “Ganomaly: Semi-
      supervised anomaly detection via adversarial training,” in Asian conference on
      computer vision. Springer, 2018, pp. 622–637.

[162] L. Ruff, R. Vandermeulen, N. Goernitz, L. Deecke, S. A. Siddiqui, A. Binder,
      E. Müller, and M. Kloft, “Deep one-class classification,” in International
      conference on machine learning.     PMLR, 2018, pp. 4393–4402.

[163] O. Rippel, P. Mertens, and D. Merhof, “Modeling the distribution of normal data
      in pre-trained deep features for anomaly detection,” in 2020 25th International
      Conference on Pattern Recognition (ICPR).      IEEE, 2021, pp. 6726–6733.

106
                                                                       Bibliography


[164] J. Yu, Y. Zheng, X. Wang, W. Li, Y. Wu, R. Zhao, and L. Wu, “FastFlow:
      Unsupervised Anomaly Detection and Localization via 2D Normalizing Flows,”
      arXiv:2111.07677 [cs], Nov. 2021, arXiv: 2111.07677. [Online]. Available:
      http://arxiv.org/abs/2111.07677 (Accessed 2022-04-28).

[165] H. M. Schlüter, J. Tan, B. Hou, and B. Kainz, “Self-Supervised Out-of-
      Distribution Detection and Localization with Natural Synthetic Anomalies
      (NSA),” arXiv:2109.15222 [cs], Sep. 2021, arXiv: 2109.15222. [Online].
      Available: http://arxiv.org/abs/2109.15222 (Accessed 2022-04-28).

[166] C.-L. Li, K. Sohn, J. Yoon, and T. Pfister, “Cutpaste: Self-supervised learning
      for anomaly detection and localization,” in Proceedings of the IEEE/CVF
      Conference on Computer Vision and Pattern Recognition, 2021, pp. 9664–
      9674.

[167] V. Zavrtanik, M. Kristan, and D. Skočaj, “Reconstruction by inpainting
      for visual anomaly detection,” Pattern Recognition, vol. 112, p. 107706,
      Apr. 2021. [Online]. Available: https://linkinghub.elsevier.com/retrieve/pii/
      S0031320320305094 (Accessed 2022-05-26).

[168] G. Wang, S. Han, E. Ding, and D. Huang, “Student-Teacher Feature Pyramid
      Matching for Anomaly Detection,” Oct. 2021, number: arXiv:2103.04257
      arXiv:2103.04257 [cs]. [Online]. Available: http://arxiv.org/abs/2103.04257
      (Accessed 2022-05-26).

[169] P. Bergmann, M. Fauser, D. Sattlegger, and C. Steger, “Uninformed
      Students: Student-Teacher Anomaly Detection With Discriminative Latent
      Embeddings,” in 2020 IEEE/CVF Conference on Computer Vision and Pattern
      Recognition (CVPR).      Seattle, WA, USA: IEEE, Jun. 2020, pp. 4182–4191.
      [Online]. Available: https://ieeexplore.ieee.org/document/9157778/ (Accessed
      2022-05-24).

[170] Y. Shi, J. Yang, and Z. Qi, “Unsupervised anomaly segmentation
      via deep feature reconstruction,” Neurocomputing, vol. 424, pp. 9–22,
      Feb. 2021. [Online]. Available: https://linkinghub.elsevier.com/retrieve/pii/
      S0925231220317951 (Accessed 2022-05-26).

[171] J. Pirnay and K. Chai, “Inpainting transformer for anomaly detection,” in
      International Conference on Image Analysis and Processing. Springer, 2022,
      pp. 394–406.

                                                                                 107
B IBLIOGRAPHY


[172] Y. Lee and P. Kang, “AnoViT: Unsupervised Anomaly Detection and
      Localization With Vision Transformer-Based Encoder-Decoder,” IEEE Access,
      vol. 10, pp. 46 717–46 724, 2022, publisher: IEEE.

[173] J. Yang, Y. Shi, and Z. Qi, “DFR: Deep Feature Reconstruction for
      Unsupervised Anomaly Segmentation,” Dec. 2020, number: arXiv:2012.07122
      arXiv:2012.07122 [cs]. [Online]. Available: http://arxiv.org/abs/2012.07122
      (Accessed 2022-05-26).

[174] “Build software better, together.” [Online]. Available: https://github.com
      (Accessed 2022-06-06).

[175] “PyTorch.” [Online]. Available: https://www.pytorch.org (Accessed 2022-06-
      06).

[176] K. Roth, L. Pemula, J. Zepeda, B. Schölkopf, T. Brox, and P. Gehler, “Towards
      total recall in industrial anomaly detection,” in Proceedings of the IEEE/CVF
      Conference on Computer Vision and Pattern Recognition, 2022, pp. 14 318–
      14 328.

[177] N.-C. Ristea, N. Madan, R. T. Ionescu, K. Nasrollahi, F. S. Khan, T. B.
      Moeslund, and M. Shah, “Self-supervised predictive convolutional attentive
      block for anomaly detection,” in Proceedings of the IEEE/CVF Conference on
      Computer Vision and Pattern Recognition, 2022, pp. 13 576–13 586.

[178] M. Rudolph, T. Wehrbein, B. Rosenhahn, and B. Wandt, “Fully convolutional
      cross-scale-flows for image-based defect detection,” in Proceedings of the
      IEEE/CVF Winter Conference on Applications of Computer Vision, 2022, pp.
      1088–1097.

[179] H. Deng and X. Li, “Anomaly Detection via Reverse Distillation from One-
      Class Embedding,” in Proceedings of the IEEE/CVF Conference on Computer
      Vision and Pattern Recognition, 2022, pp. 9737–9746.

[180] D. Gudovskiy, S. Ishizaka, and K. Kozuka, “Cflow-ad: Real-time unsupervised
      anomaly detection with localization via conditional normalizing flows,” in
      Proceedings of the IEEE/CVF Winter Conference on Applications of Computer
      Vision, 2022, pp. 98–107.

[181] V. Zavrtanik, M. Kristan, and D. Skočaj, “Draem-a discriminatively trained
      reconstruction embedding for surface anomaly detection,” in Proceedings of

108
                                                                         Bibliography


      the IEEE/CVF International Conference on Computer Vision, 2021, pp. 8330–
      8339.

[182] T. Defard, A. Setkov, A. Loesch, and R. Audigier, “Padim: a patch distribution
      modeling framework for anomaly detection and localization,” in International
      Conference on Pattern Recognition.      Springer, 2021, pp. 475–489.

[183] Y. Zheng, X. Wang, R. Deng, T. Bao, R. Zhao, and L. Wu, “Focus your
      distribution: Coarse-to-fine non-contrastive learning for anomaly detection and
      localization,” in 2022 IEEE International Conference on Multimedia and Expo
      (ICME). IEEE, 2022, pp. 1–6.

[184] D. V. Carvalho, E. M. Pereira, and J. S. Cardoso, “Machine Learning
      Interpretability: A Survey on Methods and Metrics,” Electronics, vol. 8, no. 8, p.
      832, Jul. 2019. [Online]. Available: https://www.mdpi.com/2079-9292/8/8/832
      (Accessed 2022-10-25).

[185] A. Barredo Arrieta, N. Díaz-Rodríguez, J. Del Ser, A. Bennetot,
      S. Tabik, A. Barbado, S. Garcia, S. Gil-Lopez, D. Molina, R. Benjamins,
      R. Chatila, and F. Herrera, “Explainable Artificial Intelligence (XAI):
      Concepts, taxonomies, opportunities and challenges toward responsible AI,”
      Information Fusion, vol. 58, pp. 82–115, Jun. 2020. [Online]. Available:
      https://linkinghub.elsevier.com/retrieve/pii/S1566253519308103 (Accessed
      2022-10-24).

[186] D. Gunning, M. Stefik, J. Choi, T. Miller, S. Stumpf, and G.-Z. Yang,
      “XAI—Explainable artificial intelligence,” Science robotics, vol. 4, no. 37,
      p. eaay7120, 2019, publisher: American Association for the Advancement of
      Science.

[187] A. Adadi and M. Berrada, “Peeking Inside the Black-Box: A Survey on
      Explainable Artificial Intelligence (XAI),” IEEE Access, vol. 6, pp. 52 138–
      52 160, 2018. [Online]. Available:       https://ieeexplore.ieee.org/document/
      8466590/ (Accessed 2022-10-24).

[188] F. K. Dosilovic, M. Brcic, and N. Hlupic, “Explainable artificial
      intelligence:     A survey,” in 2018 41st International Convention on
      Information and Communication Technology, Electronics and Microelectronics
      (MIPRO).        Opatija: IEEE, May 2018, pp. 0210–0215. [Online]. Available:
      https://ieeexplore.ieee.org/document/8400040/ (Accessed 2022-10-24).

                                                                                   109
B IBLIOGRAPHY


[189] R. Ibrahim and M. O. Shafiq, “Explainable Convolutional Neural Networks:
      A Taxonomy, Review, and Future Directions,” ACM Computing Surveys,
      p. 3563691, Sep. 2022. [Online]. Available: https://dl.acm.org/doi/10.1145/
      3563691 (Accessed 2022-10-24).

[190] S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R. Müller, and W. Samek,
      “On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-
      Wise Relevance Propagation,” PLOS ONE, vol. 10, no. 7, p. e0130140, Jul.
      2015. [Online]. Available: https://dx.plos.org/10.1371/journal.pone.0130140
      (Accessed 2022-10-25).

[191] Q. Zhang, Y. N. Wu, and S.-C. Zhu, “Interpretable Convolutional Neural
      Networks,” in 2018 IEEE/CVF Conference on Computer Vision and Pattern
      Recognition.    Salt Lake City, UT: IEEE, Jun. 2018, pp. 8827–8836.
      [Online]. Available: https://ieeexplore.ieee.org/document/8579018/ (Accessed
      2022-10-25).

[192] X. Shi, F. Xing, K. Xu, P. Chen, Y. Liang, Z. Lu, and Z. Guo, “Loss-Based
      Attention for Interpreting Image-Level Prediction of Convolutional Neural
      Networks,” IEEE Transactions on Image Processing, vol. 30, pp. 1662–1675,
      2021. [Online]. Available: https://ieeexplore.ieee.org/document/9311772/
      (Accessed 2022-10-25).

[193] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov, R. Zemel, and
      Y. Bengio, “Show, attend and tell: Neural image caption generation with visual
      attention,” in International conference on machine learning. PMLR, 2015, pp.
      2048–2057.

[194] G. E. Hinton, S. Sabour, and N. Frosst, “Matrix capsules with EM routing,” in
      International conference on learning representations, 2018.

[195] Q. Zhang, Y. Yang, H. Ma, and Y. N. Wu, “Interpreting cnns via decision trees,”
      in Proceedings of the IEEE/CVF conference on computer vision and pattern
      recognition, 2019, pp. 6261–6270.

[196] Q. Zhang, R. Cao, F. Shi, Y. N. Wu, and S.-C. Zhu, “Interpreting
      CNN Knowledge via an Explanatory Graph,” Proceedings of the AAAI
      Conference on Artificial Intelligence, vol. 32, no. 1, Apr. 2018. [Online].
      Available: https://ojs.aaai.org/index.php/AAAI/article/view/11819 (Accessed
      2022-10-25).

110
                                                                        Bibliography


[197] M. Tamajka, W. Benesova, and M. Kompanek, “Transforming Convolutional
      Neural Network to an Interpretable Classifier,” in 2019 International
      Conference on Systems, Signals and Image Processing (IWSSIP).           Osijek,
      Croatia:    IEEE, Jun. 2019, pp. 255–259. [Online]. Available:            https:
      //ieeexplore.ieee.org/document/8787211/ (Accessed 2022-10-25).

[198] B. Zhou, Y. Sun, D. Bau, and A. Torralba, “Interpretable Basis
      Decomposition for Visual Explanation,” in Computer Vision – ECCV
      2018, V. Ferrari, M. Hebert, C. Sminchisescu, and Y. Weiss, Eds.
      Cham: Springer International Publishing, 2018, vol. 11212, pp. 122–138,
      series Title: Lecture Notes in Computer Science. [Online]. Available:
      https://link.springer.com/10.1007/978-3-030-01237-3_8 (Accessed 2022-10-
      25).

[199] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba,
      “Learning Deep Features for Discriminative Localization,” in 2016 IEEE
      Conference on Computer Vision and Pattern Recognition (CVPR).               Las
      Vegas, NV, USA: IEEE, Jun. 2016, pp. 2921–2929. [Online]. Available:
      http://ieeexplore.ieee.org/document/7780688/ (Accessed 2022-10-27).

[200] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra,
      “Grad-CAM: Visual Explanations From Deep Networks via Gradient-Based
      Localization,” in Proceedings of the IEEE International Conference on
      Computer Vision (ICCV), Oct. 2017.

[201] J. H. Sejr and A. Schneider-Kamp, “Explainable outlier detection: What, for
      Whom and Why?” Machine Learning with Applications, vol. 6, p. 100172,
      Dec. 2021. [Online]. Available: https://linkinghub.elsevier.com/retrieve/pii/
      S2666827021000864 (Accessed 2022-10-21).

[202] V. Yepmo, G. Smits, and O. Pivert, “Anomaly explanation: A review,” Data &
      Knowledge Engineering, vol. 137, p. 101946, Jan. 2022. [Online]. Available:
      https://linkinghub.elsevier.com/retrieve/pii/S0169023X21000720 (Accessed
      2022-10-21).

[203] E. Panjei, L. Gruenwald, E. Leal, C. Nguyen, and S. Silvia, “A survey on outlier
      explanations,” The VLDB Journal, vol. 31, no. 5, pp. 977–1008, Sep. 2022.
      [Online]. Available: https://link.springer.com/10.1007/s00778-021-00721-1
      (Accessed 2022-10-21).

                                                                                  111
B IBLIOGRAPHY


[204] Z. Li, Y. Zhu, and M. van Leeuwen, “A Survey on Explainable
      Anomaly Detection,” Oct. 2022, arXiv:2210.06959 [cs]. [Online]. Available:
      http://arxiv.org/abs/2210.06959 (Accessed 2022-10-21).

[205] P. Liznerski, L. Ruff, R. A. Vandermeulen, B. J. Franks, M. Kloft, and
      K.-R. Müller, “Explainable Deep One-Class Classification,” Mar. 2021,
      arXiv:2007.01760 [cs, stat]. [Online]. Available: http://arxiv.org/abs/2007.
      01760 (Accessed 2022-10-03).

[206] R. Langone, A. Cuzzocrea, and N. Skantzos, “Interpretable Anomaly
      Prediction: Predicting anomalous behavior in industry 4.0 settings via
      regularized logistic regression tools,” Data & Knowledge Engineering, vol.
      130, p. 101850, Nov. 2020. [Online]. Available: https://linkinghub.elsevier.
      com/retrieve/pii/S0169023X1830644X (Accessed 2022-10-26).

[207] O. Serradilla, E. Zugasti, J. Rodriguez, and U. Zurutuza, “Deep learning
      models for predictive maintenance: a survey, comparison, challenges and
      prospects,” Applied Intelligence, vol. 52, no. 10, pp. 10 934–10 964, Aug. 2022.
      [Online]. Available: https://link.springer.com/10.1007/s10489-021-03004-y
      (Accessed 2022-10-26).

[208] O. Serradilla, E. Zugasti, C. Cernuda, A. Aranburu, J. R. de Okariz, and
      U. Zurutuza, “Interpreting Remaining Useful Life estimations combining
      Explainable Artificial Intelligence and domain knowledge in industrial
      machinery,” in 2020 IEEE International Conference on Fuzzy Systems
      (FUZZ-IEEE).      Glasgow, United Kingdom: IEEE, Jul. 2020, pp. 1–8.
      [Online]. Available: https://ieeexplore.ieee.org/document/9177537/ (Accessed
      2022-10-26).

[209] C. Hu and Y. Wang, “An Efficient Convolutional Neural Network Model
      Based on Object-Level Attention Mechanism for Casting Defect Detection
      on Radiography Images,” IEEE Transactions on Industrial Electronics,
      vol. 67, no. 12, pp. 10 922–10 930, Dec. 2020. [Online]. Available:
      https://ieeexplore.ieee.org/document/8948332/ (Accessed 2022-10-26).

[210] T.-Y. Lin, A. RoyChowdhury, and S. Maji, “Bilinear Convolutional Neural
      Networks for Fine-Grained Visual Recognition,” IEEE Transactions on Pattern
      Analysis and Machine Intelligence, vol. 40, no. 6, pp. 1309–1322, Jun. 2018.
      [Online]. Available: https://ieeexplore.ieee.org/document/7968351/ (Accessed
      2022-10-27).

112
                                                                       Bibliography


[211] M. Lee, J. Jeon, and H. Lee, “Explainable AI for domain experts: a post
      Hoc analysis of deep learning for defect classification of TFT–LCD panels,”
      Journal of Intelligent Manufacturing, vol. 33, no. 6, pp. 1747–1759, Aug. 2022.
      [Online]. Available: https://link.springer.com/10.1007/s10845-021-01758-3
      (Accessed 2022-10-24).

[212] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller, “Striving for
      Simplicity: The All Convolutional Net,” Apr. 2015, arXiv:1412.6806 [cs].
      [Online]. Available: http://arxiv.org/abs/1412.6806 (Accessed 2022-10-27).

[213] G. Pang, C. Ding, C. Shen, and A. v. d. Hengel, “Explainable Deep
      Few-shot Anomaly Detection with Deviation Networks,” Aug. 2021,
      arXiv:2108.00462 [cs]. [Online]. Available: http://arxiv.org/abs/2108.00462
      (Accessed 2022-10-24).

[214] “What Is Empirical Research? Definition, Types & Samples,” Sep. 2022.
      [Online]. Available: https://research.com/research/what-is-empirical-research
      (Accessed 2022-10-13).

[215] F. Martinez-Plumed, L. Contreras-Ochando, C. Ferri, J. Hernandez-
      Orallo, M. Kull, N. Lachiche, M. J. Ramirez-Quintana, and P. Flach,
      “CRISP-DM Twenty Years Later:            From Data Mining Processes to
      Data Science Trajectories,” IEEE Transactions on Knowledge and Data
      Engineering, vol. 33, no. 8, pp. 3048–3061, Aug. 2021. [Online]. Available:
      https://ieeexplore.ieee.org/document/8943998/ (Accessed 2022-10-10).

[216] “Danijel Skočaj,” section: people. [Online]. Available: https://www.vicos.si/
      people/danijel_skocaj/ (Accessed 2022-11-09).

[217] “Matej Kristan,” section: people. [Online]. Available: https://www.vicos.si/
      people/matej_kristan/ (Accessed 2022-11-09).

[218] “Zeynep Akata.” [Online]. Available: https://www.eml-unitue.de/people/
      zeynep-akata (Accessed 2022-11-09).

[219] Y. Gong, H. Shao, J. Luo, and Z. Li, “A deep transfer learning
      model for inclusion defect detection of aeronautics composite materials,”
      Composite Structures, vol. 252, p. 112681, Nov. 2020. [Online]. Available:
      https://linkinghub.elsevier.com/retrieve/pii/S0263822320326076 (Accessed
      2022-07-11).

                                                                                113
B IBLIOGRAPHY


[220] S. Dong, X. Sun, S. Xie, and M. Wang, “Automatic defect identification
      technology of digital image of pipeline weld,” Natural Gas Industry
      B, vol. 6, no. 4, pp. 399–403, Aug. 2019. [Online]. Available:
      https://linkinghub.elsevier.com/retrieve/pii/S2352854019300749 (Accessed
      2022-07-11).

[221] F. Duan, S. Yin, P. Song, W. Zhang, C. Zhu, and H. Yokoi, “Automatic Welding
      Defect Detection of X-Ray Images by Using Cascade AdaBoost With Penalty
      Term,” IEEE Access, vol. 7, pp. 125 929–125 938, 2019, conference Name:
      IEEE Access.

[222] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
      A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei, “ImageNet
      Large Scale Visual Recognition Challenge,” International Journal of Computer
      Vision, vol. 115, no. 3, pp. 211–252, Dec. 2015. [Online]. Available:
      http://link.springer.com/10.1007/s11263-015-0816-y (Accessed 2022-07-13).

[223] “casting       product   image      data     for     quality     inspection.”
      [Online].   Available:       https://www.kaggle.com/datasets/ravirajsinh45/
      real-life-industrial-dataset-of-casting-product (Accessed 2022-11-03).

[224] M. M. Breunig, H.-P. Kriegel, R. T. Ng, and J. Sander, “LOF: identifying
      density-based local outliers,” in Proceedings of the 2000 ACM SIGMOD
      international conference on Management of data, 2000, pp. 93–104.




114
